{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import xlrd\n",
    "import re\n",
    "import openpyxl\n",
    "import random\n",
    "\n",
    "# custom file that maps state names to abbreviations\n",
    "from abbreviation_conversion import abbrev_to_us_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up prescription data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing the find_year function: 2013\n",
      "testing the find_month function: 12\n"
     ]
    }
   ],
   "source": [
    "def find_year(TRANSACTION_DATE):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        TRANSACTION_DATE (str): date in format MMDDYYYY\n",
    "\n",
    "    Returns:\n",
    "        int: year\n",
    "    \"\"\"\n",
    "    TRANSACTION_DATE = str(TRANSACTION_DATE)\n",
    "    \n",
    "    return int(TRANSACTION_DATE[-4:])\n",
    "\n",
    "# quick test \n",
    "print(f\"testing the find_year function: {find_year(12202013)}\")\n",
    "\n",
    "\n",
    "def find_month(TRANSACTION_DATE):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        TRANSACTION_DATE (str): date in format MMDDYYYY\n",
    "\n",
    "    Returns:\n",
    "        int: month\n",
    "    \"\"\"\n",
    "    TRANSACTION_DATE = str(TRANSACTION_DATE)\n",
    "\n",
    "    if len(TRANSACTION_DATE) == 8:\n",
    "        return int(TRANSACTION_DATE[:2])\n",
    "    else:\n",
    "        return int(TRANSACTION_DATE[:1])\n",
    "    \n",
    "\n",
    "# quick test \n",
    "print(f\"testing the find_month function: {find_month(12202013)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load in the data, we need to truncate the amount of columns we use as well as the states\n",
    "cols_to_keep = [\"BUYER_STATE\", \"BUYER_ZIP\", \"BUYER_COUNTY\", \"DRUG_CODE\", \"DRUG_NAME\", \"QUANTITY\", \"TRANSACTION_DATE\"]\n",
    "\n",
    "# we know we need Florida, Texas, and Washington\n",
    "states = [\"FL\", \"TX\", \"WA\"]\n",
    "# since we are normalizing based on population, I think we should pick states that are regionally close to our target states\n",
    "# we can change this later as a group, but I have these selected below:\n",
    "\n",
    "# Florida comparison states\n",
    "fl_states = [\"PA\", \"MI\", \"NC\"]\n",
    "\n",
    "# Texas comparison states\n",
    "tx_states = [\"IL\", \"MA\", \"MI\"]\n",
    "\n",
    "# Washington comparison states\n",
    "wa_states = [\"NC\", \"CO\", \"MD\"]\n",
    "\n",
    "# create list of all states to use\n",
    "variable_states = []\n",
    "variable_states.extend(fl_states)\n",
    "variable_states.extend(tx_states)\n",
    "variable_states.extend(wa_states)\n",
    "\n",
    "# append variable states to our original list\n",
    "states.extend(variable_states)\n",
    "\n",
    "\n",
    "# create separate list of only florida and washington states for prescription data\n",
    "prescription_states = [state for state in states if state not in [\"IL\", \"MA\", \"MI\", \"TX\"]]\n",
    "\n",
    "# NC is appearing twice as it's a comparison state for both target states\n",
    "# making this a set will remove the duplicate\n",
    "prescription_states = list(set(prescription_states))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in individual state prescription data\n",
    "\n",
    "The Washington Post article associated with our data states that data from 2013 and 2014 was only recently added. Resultingly, we found that it was missing from the large dataset of all states. However, upon further digging, we found that these years were present **on an individual state level**, so we will load these in and concatenate them with our larger dataframe above.\n",
    "\n",
    "Without chunking, the below takes 30 seconds for each file to load in. With chunking, this is reduced to about 4 seconds per record, so please make sure to leave this in its current format.\n",
    "\n",
    "\n",
    "\n",
    "current issues:\n",
    "- way more records for 2013 and 2014\n",
    "- non WA states not being read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing record 0 of filename 00_source_data/state_prescriptions\\arcos-co-statewide-itemized.csv\n",
      "Finished processing record 1 of filename 00_source_data/state_prescriptions\\arcos-fl-statewide-itemized.csv\n",
      "Finished processing record 2 of filename 00_source_data/state_prescriptions\\arcos-md-statewide-itemized.csv\n",
      "Finished processing record 3 of filename 00_source_data/state_prescriptions\\arcos-mi-statewide-itemized.csv\n",
      "Finished processing record 4 of filename 00_source_data/state_prescriptions\\arcos-nc-statewide-itemized.csv\n",
      "Finished processing record 5 of filename 00_source_data/state_prescriptions\\arcos-pa-statewide-itemized.csv\n",
      "Finished processing record 6 of filename 00_source_data/state_prescriptions\\arcos-wa-statewide-itemized.csv\n"
     ]
    }
   ],
   "source": [
    "# set FINAL cols we want\n",
    "cols_to_keep = [\"BUYER_STATE\", \"BUYER_ZIP\", \"BUYER_COUNTY\", \"DRUG_CODE\", \"DRUG_NAME\", \"TRANSACTION_DATE\", \"MME_Conversion_Factor\", \"dos_str\", \"DOSAGE_UNIT\", \"CALC_BASE_WT_IN_GM\"]\n",
    "\n",
    "# set additional columns we need to calculate MME\n",
    "chunk_cols = [\"dos_str\", \"DOSAGE_UNIT\", \"MME_Conversion_Factor\",]\n",
    "\n",
    "\n",
    "\n",
    "path = r'00_source_data/state_prescriptions' # point to correct folder\n",
    "filenames = glob.glob(path + \"/*.csv\") # select all text files in folder\n",
    "\n",
    "assert len(filenames) == 7, \"There should be 7 files in the folder - check that we don't have a missing state\"\n",
    "\n",
    "df_prescriptions = pd.DataFrame() # empty df - will store data from all txt files\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for f, count in zip(filenames, range(len(filenames))):\n",
    "\n",
    "    it = pd.read_csv(f, chunksize=1_000_000, usecols = cols_to_keep) # may have to change chunksize depending on your computer's memory\n",
    "    \n",
    "    temp_df = pd.DataFrame()\n",
    "\n",
    "    for chunk in it:\n",
    "\n",
    "        # ensure dtypes for faster calculation below\n",
    "        float_cols = [\"MME_Conversion_Factor\", \"dos_str\", \"DOSAGE_UNIT\", \"CALC_BASE_WT_IN_GM\"]\n",
    "        chunk[float_cols] = chunk[float_cols].astype(\"float64\")\n",
    "\n",
    "        # calculate MME\n",
    "        chunk[\"MME\"] = chunk[\"dos_str\"] * chunk[\"MME_Conversion_Factor\"] * chunk[\"DOSAGE_UNIT\"]\n",
    "\n",
    "        chunk = chunk[cols_to_keep]\n",
    "        # ensure we're working in the correct date range\n",
    "        #filtered_chunk = chunk[chunk[\"year\"] > 2002]\n",
    "        #filtered_chunk = filtered_chunk[filtered_chunk[\"year\"] < 2016]\n",
    "\n",
    "        # extract year out of date column\n",
    "        chunk[\"year\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: int(find_year(x)))\n",
    "\n",
    "        # calculate int cols\n",
    "        \n",
    "        int_cols = [\"BUYER_ZIP\", \"year\"]\n",
    "        chunk[int_cols] = chunk[int_cols].astype(\"int64\")\n",
    "\n",
    "        temp_df = pd.concat([temp_df, chunk])\n",
    "\n",
    "    print(f\"Finished processing record {count} of filename {f}\")\n",
    "    df_prescriptions = pd.concat([df_prescriptions, temp_df], axis=0, ignore_index=True)\n",
    "\n",
    "df_prescriptions.drop(columns={\"TRANSACTION_DATE\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prescriptions[\"MME\"] = df_prescriptions[\"CALC_BASE_WT_IN_GM\"] * df_prescriptions[\"MME_Conversion_Factor\"]\n",
    "\n",
    "assert len(df_prescriptions[df_prescriptions[\"MME\"].isnull()]) == 0, \"There should be no missing MME calculations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null counties & no shipments in counties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null counties & no shipments in counties\n",
    "\n",
    "# check for null counties\n",
    "#df_prescriptions[df_prescriptions[\"BUYER_COUNTY\"].isnull()]\n",
    "# uncommenting the above line shows us only one missing county - zip code 34635\n",
    "# this is bellair beach, so we'll fill this value in and add an assert to ensure no more missing counties\n",
    "\n",
    "# replace buyer county where buyer zip is equal to 34635 with bellair beach\n",
    "df_prescriptions.loc[df_prescriptions[\"BUYER_ZIP\"] == 34635, \"BUYER_COUNTY\"] = \"Pinellas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_prescriptions[df_prescriptions[\"BUYER_COUNTY\"].isnull()]) == 0, \"There should be no missing counties\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### group by state, county, and year\n",
    "\n",
    "Turns unit of observation into one MME calculation per county-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BUYER_STATE</th>\n",
       "      <th>BUYER_ZIP</th>\n",
       "      <th>BUYER_COUNTY</th>\n",
       "      <th>DRUG_CODE</th>\n",
       "      <th>DRUG_NAME</th>\n",
       "      <th>MME_Conversion_Factor</th>\n",
       "      <th>dos_str</th>\n",
       "      <th>DOSAGE_UNIT</th>\n",
       "      <th>CALC_BASE_WT_IN_GM</th>\n",
       "      <th>year</th>\n",
       "      <th>MME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CO</td>\n",
       "      <td>81005</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.45405</td>\n",
       "      <td>2009</td>\n",
       "      <td>0.45405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CO</td>\n",
       "      <td>80907</td>\n",
       "      <td>EL PASO</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.90810</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.90810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CO</td>\n",
       "      <td>80907</td>\n",
       "      <td>EL PASO</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.60540</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.60540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CO</td>\n",
       "      <td>80214</td>\n",
       "      <td>JEFFERSON</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.60540</td>\n",
       "      <td>2007</td>\n",
       "      <td>0.60540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CO</td>\n",
       "      <td>80111</td>\n",
       "      <td>ARAPAHOE</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>1.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>64.54800</td>\n",
       "      <td>2014</td>\n",
       "      <td>96.82200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56338047</th>\n",
       "      <td>WA</td>\n",
       "      <td>99206</td>\n",
       "      <td>SPOKANE</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>9.08100</td>\n",
       "      <td>2012</td>\n",
       "      <td>9.08100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56338048</th>\n",
       "      <td>WA</td>\n",
       "      <td>98908</td>\n",
       "      <td>YAKIMA</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>3.02700</td>\n",
       "      <td>2008</td>\n",
       "      <td>3.02700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56338049</th>\n",
       "      <td>WA</td>\n",
       "      <td>98908</td>\n",
       "      <td>YAKIMA</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>3.02700</td>\n",
       "      <td>2009</td>\n",
       "      <td>3.02700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56338050</th>\n",
       "      <td>WA</td>\n",
       "      <td>98052</td>\n",
       "      <td>KING</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1.81620</td>\n",
       "      <td>2010</td>\n",
       "      <td>1.81620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56338051</th>\n",
       "      <td>WA</td>\n",
       "      <td>98331</td>\n",
       "      <td>CLALLAM</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>1.5</td>\n",
       "      <td>20.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>14.34400</td>\n",
       "      <td>2009</td>\n",
       "      <td>21.51600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56338052 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         BUYER_STATE  BUYER_ZIP BUYER_COUNTY  DRUG_CODE    DRUG_NAME  \\\n",
       "0                 CO      81005       PUEBLO       9193  HYDROCODONE   \n",
       "1                 CO      80907      EL PASO       9193  HYDROCODONE   \n",
       "2                 CO      80907      EL PASO       9193  HYDROCODONE   \n",
       "3                 CO      80214    JEFFERSON       9193  HYDROCODONE   \n",
       "4                 CO      80111     ARAPAHOE       9143    OXYCODONE   \n",
       "...              ...        ...          ...        ...          ...   \n",
       "56338047          WA      99206      SPOKANE       9193  HYDROCODONE   \n",
       "56338048          WA      98908       YAKIMA       9193  HYDROCODONE   \n",
       "56338049          WA      98908       YAKIMA       9193  HYDROCODONE   \n",
       "56338050          WA      98052         KING       9193  HYDROCODONE   \n",
       "56338051          WA      98331      CLALLAM       9143    OXYCODONE   \n",
       "\n",
       "          MME_Conversion_Factor  dos_str  DOSAGE_UNIT  CALC_BASE_WT_IN_GM  \\\n",
       "0                           1.0      5.0        150.0             0.45405   \n",
       "1                           1.0      7.5        200.0             0.90810   \n",
       "2                           1.0      5.0        200.0             0.60540   \n",
       "3                           1.0      5.0        200.0             0.60540   \n",
       "4                           1.5     15.0       4800.0            64.54800   \n",
       "...                         ...      ...          ...                 ...   \n",
       "56338047                    1.0     10.0       1500.0             9.08100   \n",
       "56338048                    1.0     10.0        500.0             3.02700   \n",
       "56338049                    1.0     10.0        500.0             3.02700   \n",
       "56338050                    1.0      5.0        600.0             1.81620   \n",
       "56338051                    1.5     20.0        800.0            14.34400   \n",
       "\n",
       "          year       MME  \n",
       "0         2009   0.45405  \n",
       "1         2006   0.90810  \n",
       "2         2006   0.60540  \n",
       "3         2007   0.60540  \n",
       "4         2014  96.82200  \n",
       "...        ...       ...  \n",
       "56338047  2012   9.08100  \n",
       "56338048  2008   3.02700  \n",
       "56338049  2009   3.02700  \n",
       "56338050  2010   1.81620  \n",
       "56338051  2009  21.51600  \n",
       "\n",
       "[56338052 rows x 11 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prescriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby to get one row per county year\n",
    "# keeping zip for now, may not be necessary\n",
    "\n",
    "df_prescriptions = df_prescriptions.groupby([\"BUYER_STATE\", \"BUYER_ZIP\", \"BUYER_COUNTY\", \"year\"])[\"MME\"].sum().reset_index()\n",
    "\n",
    "\n",
    "#df_prescriptions = df_grouped.copy() # temporary - will remove this later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problems\n",
    "- read huge arcos file, never unzipped (read as compressed zip file)\n",
    "? limited cols taking in, filtered for specific states he wanted\n",
    " - couple of transofmrs - pulled year col, std morpine calculation\n",
    "? saved memory! -> last thing for each chunk, add state abbrev for county name\n",
    "then grouped chunk, chunk size fairly small (maybe he means agg to year now?)\n",
    "    go through x chunks, groupbys here he means\n",
    "    getting to pt where more manageable on memory + easier for other team members\n",
    "\n",
    "\n",
    "other thing:\n",
    "if you leave zipped, read chunks and do transforms on chunks, it'll get done in 12min (about)\n",
    "\n",
    "\n",
    "**NOT CURRENTLY USING THE BELOW CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# set FINAL cols we want\\ncols_to_keep = [\"BUYER_STATE\", \"BUYER_ZIP\", \"BUYER_COUNTY\", \"DRUG_CODE\", \"DRUG_NAME\", \"TRANSACTION_DATE\", \"MME_Conversion_Factor\", \"dos_str\", \"DOSAGE_UNIT\"]\\n\\n# set additional columns we need to calculate MME\\nchunk_cols = [\"dos_str\", \"DOSAGE_UNIT\", \"MME_Conversion_Factor\"]\\n\\n\\ndf_prescriptions = pd.DataFrame() # empty df - will store data from all txt files\\n\\n# need 1 row per county month**** could aggergate it here also?\\nit = pd.read_csv(\"00_source_data/arcos_all_washpost.tsv.gz\", chunksize=1_000_000, sep=\\'\\t\\', usecols=cols_to_keep, low_memory=False) # may have to change chunksize depending on your computer\\'s memory\\ncols_to_keep.extend([\"year\", \"MME\"])\\n\\ntemp_df = pd.DataFrame()\\ncounter = 0\\nfor chunk in it:\\n\\n    # first, filter to only the states that we want\\n    # good first step as this will elminate unecessary calculations on rows we don\\'t need\\n    chunk = chunk[chunk[\"BUYER_STATE\"].isin(prescription_states)]\\n\\n\\n    \\n    # extract year out of date column\\n\\n    #chunk[\\'DATE\\'] = pd.to_datetime(chunk[\\'TRANSACTION_DATE\\'], format=\\'%m%d%Y\\')\\n\\n    # pull out only the year from the date field\\n    #chunk[\\'year\\']= chunk[\\'DATE\\'].dt.year\\n\\n    #chunk[\"year\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: x[-4:])\\n    chunk[\"year\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: int(find_year(x)))\\n\\n    #chunk = chunk[chunk[\"year\"] > 2012]\\n\\n\\n    # ensure dtypes for faster calculation below\\n    float_cols = [\"MME_Conversion_Factor\", \"dos_str\", \"DOSAGE_UNIT\"]\\n    chunk[float_cols] = chunk[float_cols].astype(\"float64\")\\n\\n\\n\\n    # calculate MME\\n    chunk[\"MME\"] = chunk[\"dos_str\"] * chunk[\"MME_Conversion_Factor\"] * chunk[\"DOSAGE_UNIT\"]\\n\\n    chunk = chunk[cols_to_keep]\\n\\n    # ensure we\\'re working in the correct date range\\n    # could map a dict with states + desired years to make this faster later\\n    #chunk = chunk[chunk[\"year\"] > 2012]\\n    #filtered_chunk = filtered_chunk[filtered_chunk[\"year\"] < 2016]\\n\\n\\n    # calculate int cols\\n    \\n    int_cols = [\"BUYER_ZIP\", \"year\"]\\n    chunk[int_cols] = chunk[int_cols].astype(\"int64\")\\n\\n    df_prescriptions = pd.concat([df_prescriptions, chunk])\\n\\n    print(f\"chunk {counter} processed\")\\n    counter+=1\\n    # if counter == 10:\\n    #     break\\n\\n\\ndf_prescriptions.drop(columns={\"TRANSACTION_DATE\"}, inplace=True)\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# set FINAL cols we want\n",
    "cols_to_keep = [\"BUYER_STATE\", \"BUYER_ZIP\", \"BUYER_COUNTY\", \"DRUG_CODE\", \"DRUG_NAME\", \"TRANSACTION_DATE\", \"MME_Conversion_Factor\", \"dos_str\", \"DOSAGE_UNIT\"]\n",
    "\n",
    "# set additional columns we need to calculate MME\n",
    "chunk_cols = [\"dos_str\", \"DOSAGE_UNIT\", \"MME_Conversion_Factor\"]\n",
    "\n",
    "\n",
    "df_prescriptions = pd.DataFrame() # empty df - will store data from all txt files\n",
    "\n",
    "# need 1 row per county month**** could aggergate it here also?\n",
    "it = pd.read_csv(\"00_source_data/arcos_all_washpost.tsv.gz\", chunksize=1_000_000, sep='\\t', usecols=cols_to_keep, low_memory=False) # may have to change chunksize depending on your computer's memory\n",
    "cols_to_keep.extend([\"year\", \"MME\"])\n",
    "\n",
    "temp_df = pd.DataFrame()\n",
    "counter = 0\n",
    "for chunk in it:\n",
    "\n",
    "    # first, filter to only the states that we want\n",
    "    # good first step as this will elminate unecessary calculations on rows we don't need\n",
    "    chunk = chunk[chunk[\"BUYER_STATE\"].isin(prescription_states)]\n",
    "\n",
    "\n",
    "    \n",
    "    # extract year out of date column\n",
    "\n",
    "    #chunk['DATE'] = pd.to_datetime(chunk['TRANSACTION_DATE'], format='%m%d%Y')\n",
    "\n",
    "    # pull out only the year from the date field\n",
    "    #chunk['year']= chunk['DATE'].dt.year\n",
    "\n",
    "    #chunk[\"year\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: x[-4:])\n",
    "    chunk[\"year\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: int(find_year(x)))\n",
    "\n",
    "    #chunk = chunk[chunk[\"year\"] > 2012]\n",
    "\n",
    "\n",
    "    # ensure dtypes for faster calculation below\n",
    "    float_cols = [\"MME_Conversion_Factor\", \"dos_str\", \"DOSAGE_UNIT\"]\n",
    "    chunk[float_cols] = chunk[float_cols].astype(\"float64\")\n",
    "\n",
    "\n",
    "\n",
    "    # calculate MME\n",
    "    chunk[\"MME\"] = chunk[\"dos_str\"] * chunk[\"MME_Conversion_Factor\"] * chunk[\"DOSAGE_UNIT\"]\n",
    "\n",
    "    chunk = chunk[cols_to_keep]\n",
    "\n",
    "    # ensure we're working in the correct date range\n",
    "    # could map a dict with states + desired years to make this faster later\n",
    "    #chunk = chunk[chunk[\"year\"] > 2012]\n",
    "    #filtered_chunk = filtered_chunk[filtered_chunk[\"year\"] < 2016]\n",
    "\n",
    "\n",
    "    # calculate int cols\n",
    "    \n",
    "    int_cols = [\"BUYER_ZIP\", \"year\"]\n",
    "    chunk[int_cols] = chunk[int_cols].astype(\"int64\")\n",
    "\n",
    "    df_prescriptions = pd.concat([df_prescriptions, chunk])\n",
    "\n",
    "    print(f\"chunk {counter} processed\")\n",
    "    counter+=1\n",
    "    # if counter == 10:\n",
    "    #     break\n",
    "\n",
    "\n",
    "df_prescriptions.drop(columns={\"TRANSACTION_DATE\"}, inplace=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up cause of death data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'00_source_data/cause_of_death' # point to correct folder\n",
    "filenames = glob.glob(path + \"/*.txt\") # select all text files in folder\n",
    "\n",
    "df = pd.DataFrame() # empty df - will store data from all txt files\n",
    "\n",
    "for f in filenames:\n",
    "    temp = pd.read_csv(f, index_col=None, header=0, sep='\\t')\n",
    "    # we're getting some extraneous notes at the bottom - let's just drop based on county as these will only be null for these useless notes columns\n",
    "    temp.dropna(subset={'County'}, inplace=True)\n",
    "    \n",
    "    df = pd.concat([df, temp], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to separate county and state\n",
    "\n",
    "def abtract_state(county):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        county (str): county name\n",
    "\n",
    "    Returns:\n",
    "        str: state\n",
    "    \"\"\"\n",
    "    return county.split(\", \")[1]\n",
    "\n",
    "\n",
    "\n",
    "def abstract_county(county):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        county (str): county name\n",
    "\n",
    "    Returns:\n",
    "        str: county\n",
    "    \"\"\"\n",
    "    return county.split(\", \")[0]\n",
    "\n",
    "# apply functions to our df\n",
    "df[\"State\"] = df.apply(lambda x: abtract_state(x[\"County\"]), axis=1)\n",
    "df[\"County\"] = df.apply(lambda x: abstract_county(x[\"County\"]), axis=1)\n",
    "\n",
    "# do not need notes column, let's just drop it here\n",
    "df.drop(columns={\"Notes\"}, inplace=True)\n",
    "\n",
    "df_cause_of_death = df.copy() # keep a copy of this df for later filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's filter our dataframe to be only the states we want\n",
    "df_cause_of_death = df_cause_of_death[df_cause_of_death[\"State\"].isin(states)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding in County Population data\n",
    "\n",
    "[Census county pop. data, 2000-2010](https://www.census.gov/data/tables/time-series/demo/popest/intercensal-2000-2010-counties.html)<br>\n",
    "[Census county pop. data, 2010-2019](https://www.census.gov/data/datasets/time-series/demo/popest/2010s-counties-total.html)<br>\n",
    "For both, just select the appropriate states on the webpage. We will clean and merge as needed in this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guide to cleaning - 2000s data\n",
    "\n",
    "The way the 2000s excel files are formatted, we can clean the data in the following way\n",
    "\n",
    "- load in with header=3\n",
    "- drop null on any of the populations\n",
    "    - notes at the bottom will be removed\n",
    "- drop unnamed 1, 12, and 13\n",
    "    - these contain redundant data about populations from specific dates\n",
    "    - Unnamed 12 is 2010s pop - will be redundant as our next dataset has this as well. Using the newer data\n",
    "- drop first row\n",
    "    - state as a whole\n",
    "- rename Unnamed: 0 to county\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams County</td>\n",
       "      <td>350888.0</td>\n",
       "      <td>359816.0</td>\n",
       "      <td>370753.0</td>\n",
       "      <td>377464.0</td>\n",
       "      <td>384809.0</td>\n",
       "      <td>395146.0</td>\n",
       "      <td>406575.0</td>\n",
       "      <td>415746.0</td>\n",
       "      <td>424913.0</td>\n",
       "      <td>435700.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alamosa County</td>\n",
       "      <td>14954.0</td>\n",
       "      <td>14956.0</td>\n",
       "      <td>15114.0</td>\n",
       "      <td>15067.0</td>\n",
       "      <td>15217.0</td>\n",
       "      <td>15236.0</td>\n",
       "      <td>15196.0</td>\n",
       "      <td>15180.0</td>\n",
       "      <td>15300.0</td>\n",
       "      <td>15289.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arapahoe County</td>\n",
       "      <td>491482.0</td>\n",
       "      <td>502393.0</td>\n",
       "      <td>508936.0</td>\n",
       "      <td>513690.0</td>\n",
       "      <td>518971.0</td>\n",
       "      <td>524466.0</td>\n",
       "      <td>531619.0</td>\n",
       "      <td>542039.0</td>\n",
       "      <td>552461.0</td>\n",
       "      <td>563161.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Archuleta County</td>\n",
       "      <td>10020.0</td>\n",
       "      <td>10454.0</td>\n",
       "      <td>10885.0</td>\n",
       "      <td>11089.0</td>\n",
       "      <td>11266.0</td>\n",
       "      <td>11496.0</td>\n",
       "      <td>11937.0</td>\n",
       "      <td>12262.0</td>\n",
       "      <td>12250.0</td>\n",
       "      <td>12169.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baca County</td>\n",
       "      <td>4501.0</td>\n",
       "      <td>4471.0</td>\n",
       "      <td>4336.0</td>\n",
       "      <td>4117.0</td>\n",
       "      <td>4064.0</td>\n",
       "      <td>3997.0</td>\n",
       "      <td>3933.0</td>\n",
       "      <td>3866.0</td>\n",
       "      <td>3806.0</td>\n",
       "      <td>3767.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             County      2000      2001      2002      2003      2004  \\\n",
       "0      Adams County  350888.0  359816.0  370753.0  377464.0  384809.0   \n",
       "1    Alamosa County   14954.0   14956.0   15114.0   15067.0   15217.0   \n",
       "2   Arapahoe County  491482.0  502393.0  508936.0  513690.0  518971.0   \n",
       "3  Archuleta County   10020.0   10454.0   10885.0   11089.0   11266.0   \n",
       "4       Baca County    4501.0    4471.0    4336.0    4117.0    4064.0   \n",
       "\n",
       "       2005      2006      2007      2008      2009 State  \n",
       "0  395146.0  406575.0  415746.0  424913.0  435700.0    CO  \n",
       "1   15236.0   15196.0   15180.0   15300.0   15289.0    CO  \n",
       "2  524466.0  531619.0  542039.0  552461.0  563161.0    CO  \n",
       "3   11496.0   11937.0   12262.0   12250.0   12169.0    CO  \n",
       "4    3997.0    3933.0    3866.0    3806.0    3767.0    CO  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init emmpty df for our population data\n",
    "pops00 = pd.DataFrame()\n",
    "\n",
    "# end goal - add every excel file in 00_source_data/county_pop/2000s to pops00\n",
    "\n",
    "path = r\"00_source_data/county_pop/2000s/\" # point to correct folder\n",
    "filenames = glob.glob(path + \"*.xls\")\n",
    "\n",
    "for f in filenames:\n",
    "\n",
    "    # read in current file with header = 3\n",
    "    temp = pd.read_excel(f, header = 3)\n",
    "\n",
    "    # regex to pull out state from filename\n",
    "    r = re.search(\"(2000s)(.)(\\w+)\", f)[3]\n",
    "    temp[\"State\"] = r[:2].upper()\n",
    "    \n",
    "    # drop null on any of the years\n",
    "    temp.dropna(subset=[2000], inplace=True)\n",
    "\n",
    "    #drop useless columns\n",
    "    temp.drop(columns={\"Unnamed: 1\", \"Unnamed: 12\", \"Unnamed: 13\"}, inplace=True)\n",
    "\n",
    "    # drop first row\n",
    "    temp = temp.iloc[1:, :]\n",
    "\n",
    "    # rename some cols\n",
    "    temp.rename(columns={\"Unnamed: 0\": \"County\"}, inplace=True)\n",
    "\n",
    "    # remove period at beginning of each county\n",
    "    temp[\"County\"] = temp[\"County\"].apply(lambda x: x[1:])\n",
    "\n",
    "    pops00 = pd.concat([pops00, temp], axis=0, ignore_index=True)\n",
    "\n",
    "# quick peek at the data\n",
    "pops00.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guide to cleaning - 2010s data\n",
    "\n",
    "The way the 2010s excel files are formatted, we can clean the data in the following way\n",
    "\n",
    "- load in with header=3\n",
    "- drop null on any of the populations\n",
    "    - notes at the bottom will be removed\n",
    "- drop census, estimates base\n",
    "- drop first row\n",
    "    - state as a whole\n",
    "- rename Unnamed: 0 to county\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams County</td>\n",
       "      <td>443691.0</td>\n",
       "      <td>452201.0</td>\n",
       "      <td>460558.0</td>\n",
       "      <td>469978.0</td>\n",
       "      <td>479946.0</td>\n",
       "      <td>490443.0</td>\n",
       "      <td>497734.0</td>\n",
       "      <td>503590.0</td>\n",
       "      <td>511354.0</td>\n",
       "      <td>517421.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alamosa County</td>\n",
       "      <td>15515.0</td>\n",
       "      <td>15709.0</td>\n",
       "      <td>15680.0</td>\n",
       "      <td>15787.0</td>\n",
       "      <td>15803.0</td>\n",
       "      <td>15894.0</td>\n",
       "      <td>16053.0</td>\n",
       "      <td>16108.0</td>\n",
       "      <td>16248.0</td>\n",
       "      <td>16233.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arapahoe County</td>\n",
       "      <td>574747.0</td>\n",
       "      <td>585968.0</td>\n",
       "      <td>596500.0</td>\n",
       "      <td>608467.0</td>\n",
       "      <td>619034.0</td>\n",
       "      <td>630984.0</td>\n",
       "      <td>638950.0</td>\n",
       "      <td>644478.0</td>\n",
       "      <td>651797.0</td>\n",
       "      <td>656590.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Archuleta County</td>\n",
       "      <td>12046.0</td>\n",
       "      <td>12021.0</td>\n",
       "      <td>12132.0</td>\n",
       "      <td>12216.0</td>\n",
       "      <td>12231.0</td>\n",
       "      <td>12387.0</td>\n",
       "      <td>12825.0</td>\n",
       "      <td>13295.0</td>\n",
       "      <td>13730.0</td>\n",
       "      <td>14029.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baca County</td>\n",
       "      <td>3807.0</td>\n",
       "      <td>3778.0</td>\n",
       "      <td>3722.0</td>\n",
       "      <td>3656.0</td>\n",
       "      <td>3587.0</td>\n",
       "      <td>3555.0</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>3554.0</td>\n",
       "      <td>3584.0</td>\n",
       "      <td>3581.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             County      2010      2011      2012      2013      2014  \\\n",
       "0      Adams County  443691.0  452201.0  460558.0  469978.0  479946.0   \n",
       "1    Alamosa County   15515.0   15709.0   15680.0   15787.0   15803.0   \n",
       "2   Arapahoe County  574747.0  585968.0  596500.0  608467.0  619034.0   \n",
       "3  Archuleta County   12046.0   12021.0   12132.0   12216.0   12231.0   \n",
       "4       Baca County    3807.0    3778.0    3722.0    3656.0    3587.0   \n",
       "\n",
       "       2015      2016      2017      2018      2019 State  \n",
       "0  490443.0  497734.0  503590.0  511354.0  517421.0    CO  \n",
       "1   15894.0   16053.0   16108.0   16248.0   16233.0    CO  \n",
       "2  630984.0  638950.0  644478.0  651797.0  656590.0    CO  \n",
       "3   12387.0   12825.0   13295.0   13730.0   14029.0    CO  \n",
       "4    3555.0    3530.0    3554.0    3584.0    3581.0    CO  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pops10 = pd.DataFrame()\n",
    "\n",
    "# add every excel file in 00_source_data/county_pop/2000s to pops00\n",
    "\n",
    "path = r\"00_source_data/county_pop/2010s\" # point to correct folder\n",
    "filenames = glob.glob(path + \"/*.xlsx\")\n",
    "\n",
    "for f in filenames:\n",
    "\n",
    "    # read in current file with header = 3\n",
    "    temp = pd.read_excel(f, header = 3)\n",
    "\n",
    "    # regex to pull out state from filename\n",
    "    r = re.search(\"(2010s)(.)(\\w+)\", f)[3]\n",
    "    temp[\"State\"] = r[:2].upper()\n",
    "    \n",
    "    # drop null on any of the years\n",
    "    temp.dropna(subset=[2010], inplace=True)\n",
    "\n",
    "    #drop useless columns\n",
    "    temp.drop(columns={\"Census\", \"Estimates Base\"}, inplace=True)\n",
    "\n",
    "    # drop first row\n",
    "    temp = temp.iloc[1:, :]\n",
    "\n",
    "    # rename some cols\n",
    "    temp.rename(columns={\"Unnamed: 0\": \"County\"}, inplace=True)\n",
    "\n",
    "    # remove period at beginning of each county\n",
    "    temp[\"County\"] = temp[\"County\"].apply(lambda x: x[1:])\n",
    "\n",
    "    # strip state from county\n",
    "    temp[\"County\"] = temp[\"County\"].apply(lambda x: x.split(\", \")[0])\n",
    "\n",
    "    pops10 = pd.concat([pops10, temp], axis=0, ignore_index=True)\n",
    "\n",
    "# quick peek at the data\n",
    "pops10.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt both dfs to get tidy format\n",
    "pops00 = pops00.melt([\"County\", \"State\"])\n",
    "pops10 = pops10.melt([\"County\", \"State\"])\n",
    "\n",
    "# rename columns accordingly\n",
    "pops00.rename(columns={\"variable\": \"Year\", \"value\": \"Population\"}, inplace=True)\n",
    "pops10.rename(columns={\"variable\": \"Year\", \"value\": \"Population\"}, inplace=True)\n",
    "\n",
    "# concatenate the two dfs to get all our population data in one place\n",
    "pops = pd.concat([pops00, pops10], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we have the same number of counties between datasets\n",
    "assert len(pops00[\"County\"].unique()) == len(pops10[\"County\"].unique())\n",
    "\n",
    "# check that we have the same number of counties every year\n",
    "# first, create a df with the number of counties per year\n",
    "pops_county_check = pops.groupby([\"State\", \"Year\"])[\"County\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>State</th>\n",
       "      <th>county_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>CO</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>FL</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>IL</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>MA</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>MD</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year State  county_count\n",
       "0  2000    CO            64\n",
       "1  2000    FL            67\n",
       "2  2000    IL           102\n",
       "3  2000    MA            14\n",
       "4  2000    MD            24"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group the sum of counties by year and state - will help us check if number of counties changes over the years\n",
    "grouped_states = pops_county_check.groupby([\"Year\", \"State\"])[\"County\"].sum().reset_index().rename(columns={\"County\": \"county_count\"})\n",
    "\n",
    "# here's what this looks like\n",
    "# we get a dataframe of states and years, with the number of counties in each state in each year\n",
    "grouped_states.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the above query, we should be able to assert that the number of counties per year is the same\n",
    "# below statement should always equal zero\n",
    "\n",
    "assert (grouped_states.duplicated(subset=[\"Year\", \"State\"]).sum() == 0)\n",
    "#assert (grouped_states10.duplicated(subset=[\"Year\", \"State\"]).sum() == 0)\n",
    "\n",
    "\n",
    "# ensure no duplicate values\n",
    "assert pops.duplicated().sum() == 0\n",
    "\n",
    "# loop to check that every state has the same number of counties every year\n",
    "for state in states:\n",
    "    assert (pops[pops[\"State\"] == state].Year.value_counts().nunique() == 1), f\"error on {state}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trying to integrate fip numbers for a better merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in fips data from external source\n",
    "fips = pd.read_csv(\"https://github.com/ChuckConnell/articles/raw/master/fips2county.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AL']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to get key from value in our abbreviation dictionary\n",
    "# will help us have consistent formatting across dataframes for merging purposes\n",
    "def get_keys_from_value(d, val):\n",
    "    return [k for k, v in d.items() if v == val]\n",
    "\n",
    "\n",
    "keys = get_keys_from_value(abbrev_to_us_state, 'Alabama')\n",
    "keys # quick peek to make sure it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the above to entire fips dataframe\n",
    "fips[\"state_abbrev\"] = fips[\"StateName\"].apply(lambda x: get_keys_from_value(abbrev_to_us_state, x)[0])\n",
    "\n",
    "# filter fips to appropriate states, now that it's in the correct format\n",
    "fips = fips[fips[\"state_abbrev\"].isin(states)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further cleaning of values before merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get rid of the word county in pop df\n",
    "def remove_county(x):\n",
    "\n",
    "    if \"County\" in x:\n",
    "        return x[:-7]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "pops[\"county_test\"] = pops[\"County\"].apply(lambda x: remove_county(x))\n",
    "\n",
    "\n",
    "# fix dona ana and la salle parish\n",
    "pops[\"county_test\"] = pops[\"county_test\"].apply(lambda x: x.replace(\"Doña Ana\", \"Dona Ana\"))\n",
    "fips[\"CountyName\"] = fips[\"CountyName\"].apply(lambda x: x.replace(\"DoÃ±a Ana\", \"Dona Ana\"))\n",
    "\n",
    "\n",
    "#pops[\"county_test\"] = pops[\"county_test\"].apply(lambda x: x.replace(\"La Salle Parish\", \"La Salle\"))\n",
    "\n",
    "\n",
    "# rename county_test where state is texas and county is la salle to La Salle (TX)\n",
    "pops.loc[(pops[\"State\"] == \"TX\") & (pops[\"county_test\"] == \"La Salle\"), \"county_test\"] = \"La Salle County\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change La Salle county name in fips to La Salle County\n",
    "fips.loc[fips[\"CountyName\"] == \"La Salle\", \"CountyName\"] = \"La Salle County\"\n",
    "fips.loc[fips[\"CountyName\"] == \"LaSalle Parish\", \"CountyName\"] = \"La Salle Parish\"\n",
    "pops.loc[pops[\"county_test\"] == \"LaSalle Parish\", \"county_test\"] = \"La Salle Parish\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final merge for population dataset & fip number dataset\n",
    "pops_copy = pops.merge(fips[[\"state_abbrev\", \"CountyFIPS\", \"StateFIPS\", \"CountyName\"]], left_on=[\"county_test\", \"State\"], right_on=[\"CountyName\", \"state_abbrev\"], how=\"outer\", indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should never end up with anything left out of merge\n",
    "assert len(pops_copy[pops_copy[\"_merge\"] != \"both\"]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add fip numbers to df_prescriptions\n",
    "\n",
    "# create copies of both dfs so we have a checkpoint to access our old dfs\n",
    "prescriptions_copy = df_prescriptions.copy()\n",
    "fips_copy = fips.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix nulls in prescriptions at this point\n",
    "\n",
    "prescriptions_copy.dropna(subset=[\"BUYER_COUNTY\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make buyer_county all lowercase\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.lower())\n",
    "\n",
    "# do the same for fips\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove county and parish from fips_copy\n",
    "\n",
    "def remove_parish(x):\n",
    "\n",
    "    if \"parish\" in x:\n",
    "        return x[:-7]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "# prescription dataset has similar format - match fips to this format\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: remove_county(x))\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: remove_parish(x))\n",
    "\n",
    "def expand_saint(x):\n",
    "\n",
    "    if \"st.\" in x:\n",
    "        return x.replace(\"st.\", \"saint\")\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# fix various other inconsistencies\n",
    "# left only values first\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: expand_saint(x))\n",
    "\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"desoto\", \"de soto\"))\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"desoto\", \"de soto\"))\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"st john the baptist\", \"saint john the baptist\"))\n",
    "\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"dekalb\", \"de kalb\"))\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"dekalb\", \"de kalb\"))\n",
    "\n",
    "# fix right only values\n",
    "\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"desoto\", \"de soto\"))\n",
    "\n",
    "\n",
    "\n",
    "# function to remove apostrophes from county names\n",
    "def remove_apostrophe(x):\n",
    "    \n",
    "    if \"'\" in x:\n",
    "        return x.replace(\"'\", \"\")\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "# apply to fips\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: remove_apostrophe(x))\n",
    "\n",
    "# replace lasalle with la salle in fips copy\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"lasalle\", \"la salle\"))\n",
    "\n",
    "# replace dewitt with de witt in prescriptions copy\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"dewitt\", \"de witt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "prescriptions_fips = prescriptions_copy.merge(fips_copy, left_on=[\"BUYER_COUNTY\", \"BUYER_STATE\"], right_on=[\"CountyName\", \"state_abbrev\"], how=\"outer\", indicator=True)\n",
    "\n",
    "# capitalize year and month columns\n",
    "prescriptions_fips.rename(columns={\"year\": \"Year\", \"month\": \"Month\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling missing prescription shipment values\n",
    "\n",
    "Since we have plenty of values joined with right_only indicator status, we know that some counties in our FIPS dataset is not merging correctly to our prescriptions dataset. There are 378 rows where this occurs. After extensive data cleaning and checking of counties, we believe these values should be filled in with zero, as we can assume no prescriptions were shipped to these counties in their given years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BUYER_STATE</th>\n",
       "      <th>BUYER_ZIP</th>\n",
       "      <th>BUYER_COUNTY</th>\n",
       "      <th>Year</th>\n",
       "      <th>MME</th>\n",
       "      <th>StateFIPS</th>\n",
       "      <th>CountyFIPS_3</th>\n",
       "      <th>CountyName</th>\n",
       "      <th>StateName</th>\n",
       "      <th>CountyFIPS</th>\n",
       "      <th>StateAbbr</th>\n",
       "      <th>STATE_COUNTY</th>\n",
       "      <th>state_abbrev</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31023</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>costilla</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8023</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | COSTILLA</td>\n",
       "      <td>CO</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31024</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>dolores</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8033</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | DOLORES</td>\n",
       "      <td>CO</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31025</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>57</td>\n",
       "      <td>jackson</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8057</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | JACKSON</td>\n",
       "      <td>CO</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31026</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>adams</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>17001</td>\n",
       "      <td>IL</td>\n",
       "      <td>IL | ADAMS</td>\n",
       "      <td>IL</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31027</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>alexander</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>17003</td>\n",
       "      <td>IL</td>\n",
       "      <td>IL | ALEXANDER</td>\n",
       "      <td>IL</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31394</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>499</td>\n",
       "      <td>wood</td>\n",
       "      <td>Texas</td>\n",
       "      <td>48499</td>\n",
       "      <td>TX</td>\n",
       "      <td>TX | WOOD</td>\n",
       "      <td>TX</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31395</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>501</td>\n",
       "      <td>yoakum</td>\n",
       "      <td>Texas</td>\n",
       "      <td>48501</td>\n",
       "      <td>TX</td>\n",
       "      <td>TX | YOAKUM</td>\n",
       "      <td>TX</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31396</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>503</td>\n",
       "      <td>young</td>\n",
       "      <td>Texas</td>\n",
       "      <td>48503</td>\n",
       "      <td>TX</td>\n",
       "      <td>TX | YOUNG</td>\n",
       "      <td>TX</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31397</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>505</td>\n",
       "      <td>zapata</td>\n",
       "      <td>Texas</td>\n",
       "      <td>48505</td>\n",
       "      <td>TX</td>\n",
       "      <td>TX | ZAPATA</td>\n",
       "      <td>TX</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31398</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>507</td>\n",
       "      <td>zavala</td>\n",
       "      <td>Texas</td>\n",
       "      <td>48507</td>\n",
       "      <td>TX</td>\n",
       "      <td>TX | ZAVALA</td>\n",
       "      <td>TX</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>376 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      BUYER_STATE  BUYER_ZIP BUYER_COUNTY  Year  MME  StateFIPS  CountyFIPS_3  \\\n",
       "31023         NaN        NaN          NaN   NaN  NaN          8            23   \n",
       "31024         NaN        NaN          NaN   NaN  NaN          8            33   \n",
       "31025         NaN        NaN          NaN   NaN  NaN          8            57   \n",
       "31026         NaN        NaN          NaN   NaN  NaN         17             1   \n",
       "31027         NaN        NaN          NaN   NaN  NaN         17             3   \n",
       "...           ...        ...          ...   ...  ...        ...           ...   \n",
       "31394         NaN        NaN          NaN   NaN  NaN         48           499   \n",
       "31395         NaN        NaN          NaN   NaN  NaN         48           501   \n",
       "31396         NaN        NaN          NaN   NaN  NaN         48           503   \n",
       "31397         NaN        NaN          NaN   NaN  NaN         48           505   \n",
       "31398         NaN        NaN          NaN   NaN  NaN         48           507   \n",
       "\n",
       "      CountyName StateName  CountyFIPS StateAbbr    STATE_COUNTY state_abbrev  \\\n",
       "31023   costilla  Colorado        8023        CO   CO | COSTILLA           CO   \n",
       "31024    dolores  Colorado        8033        CO    CO | DOLORES           CO   \n",
       "31025    jackson  Colorado        8057        CO    CO | JACKSON           CO   \n",
       "31026      adams  Illinois       17001        IL      IL | ADAMS           IL   \n",
       "31027  alexander  Illinois       17003        IL  IL | ALEXANDER           IL   \n",
       "...          ...       ...         ...       ...             ...          ...   \n",
       "31394       wood     Texas       48499        TX       TX | WOOD           TX   \n",
       "31395     yoakum     Texas       48501        TX     TX | YOAKUM           TX   \n",
       "31396      young     Texas       48503        TX      TX | YOUNG           TX   \n",
       "31397     zapata     Texas       48505        TX     TX | ZAPATA           TX   \n",
       "31398     zavala     Texas       48507        TX     TX | ZAVALA           TX   \n",
       "\n",
       "           _merge  \n",
       "31023  right_only  \n",
       "31024  right_only  \n",
       "31025  right_only  \n",
       "31026  right_only  \n",
       "31027  right_only  \n",
       "...           ...  \n",
       "31394  right_only  \n",
       "31395  right_only  \n",
       "31396  right_only  \n",
       "31397  right_only  \n",
       "31398  right_only  \n",
       "\n",
       "[376 rows x 14 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prescriptions_fips[prescriptions_fips[\"_merge\"] != \"both\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = prescriptions_fips[prescriptions_fips[\"Year\"].isnull()]\n",
    "\n",
    "missing_counties = nans.CountyName\n",
    "missing_states=nans.StateAbbr\n",
    "missing_fips = nans.CountyFIPS\n",
    "missing_state_fips = nans.StateFIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counties = [item for item in missing_counties for i in range(9)]\n",
    "missing_states = [item for item in missing_states for i in range(9)]\n",
    "missing_fips = [item for item in missing_fips for i in range(9)]\n",
    "missing_state_fips = [item for item in missing_state_fips for i in range(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(np.arange(2006, 2015, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_for_df = []\n",
    "\n",
    "for year in range(int(3384/9)):\n",
    "    for subyear in years:\n",
    "        years_for_df.append(subyear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = pd.DataFrame()\n",
    "\n",
    "missing_df[\"BUYER_COUNTY\"] = missing_counties\n",
    "missing_df[\"BUYER_STATE\"] = missing_states\n",
    "missing_df[\"CountyFIPS\"] = missing_fips\n",
    "missing_df[\"StateFIPS\"] = missing_state_fips\n",
    "missing_df[\"Year\"] = years_for_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2006    376\n",
       "2007    376\n",
       "2008    376\n",
       "2009    376\n",
       "2010    376\n",
       "2011    376\n",
       "2012    376\n",
       "2013    376\n",
       "2014    376\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_df.Year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing df should now have all our missing counties for every year\\\n",
    "# can drop nulls on this now since we are appending missing_df later anyways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "prescriptions_fips.dropna(subset=[\"BUYER_COUNTY\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BUYER_STATE</th>\n",
       "      <th>BUYER_ZIP</th>\n",
       "      <th>BUYER_COUNTY</th>\n",
       "      <th>Year</th>\n",
       "      <th>MME</th>\n",
       "      <th>StateFIPS</th>\n",
       "      <th>CountyFIPS_3</th>\n",
       "      <th>CountyName</th>\n",
       "      <th>StateName</th>\n",
       "      <th>CountyFIPS</th>\n",
       "      <th>StateAbbr</th>\n",
       "      <th>STATE_COUNTY</th>\n",
       "      <th>state_abbrev</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CO</td>\n",
       "      <td>80002.0</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>8060.375877</td>\n",
       "      <td>8</td>\n",
       "      <td>59</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8059</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | JEFFERSON</td>\n",
       "      <td>CO</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CO</td>\n",
       "      <td>80002.0</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>11845.177512</td>\n",
       "      <td>8</td>\n",
       "      <td>59</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8059</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | JEFFERSON</td>\n",
       "      <td>CO</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CO</td>\n",
       "      <td>80002.0</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>11938.070956</td>\n",
       "      <td>8</td>\n",
       "      <td>59</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8059</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | JEFFERSON</td>\n",
       "      <td>CO</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CO</td>\n",
       "      <td>80002.0</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>13742.309018</td>\n",
       "      <td>8</td>\n",
       "      <td>59</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8059</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | JEFFERSON</td>\n",
       "      <td>CO</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CO</td>\n",
       "      <td>80002.0</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>13043.676747</td>\n",
       "      <td>8</td>\n",
       "      <td>59</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8059</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | JEFFERSON</td>\n",
       "      <td>CO</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31018</th>\n",
       "      <td>WA</td>\n",
       "      <td>99403.0</td>\n",
       "      <td>asotin</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>13635.555484</td>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "      <td>asotin</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53003</td>\n",
       "      <td>WA</td>\n",
       "      <td>WA | ASOTIN</td>\n",
       "      <td>WA</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31019</th>\n",
       "      <td>WA</td>\n",
       "      <td>99403.0</td>\n",
       "      <td>asotin</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>15274.972710</td>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "      <td>asotin</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53003</td>\n",
       "      <td>WA</td>\n",
       "      <td>WA | ASOTIN</td>\n",
       "      <td>WA</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31020</th>\n",
       "      <td>WA</td>\n",
       "      <td>99403.0</td>\n",
       "      <td>asotin</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>15922.687523</td>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "      <td>asotin</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53003</td>\n",
       "      <td>WA</td>\n",
       "      <td>WA | ASOTIN</td>\n",
       "      <td>WA</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31021</th>\n",
       "      <td>WA</td>\n",
       "      <td>99403.0</td>\n",
       "      <td>asotin</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>17184.793767</td>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "      <td>asotin</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53003</td>\n",
       "      <td>WA</td>\n",
       "      <td>WA | ASOTIN</td>\n",
       "      <td>WA</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31022</th>\n",
       "      <td>WA</td>\n",
       "      <td>99403.0</td>\n",
       "      <td>asotin</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>16922.194147</td>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "      <td>asotin</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53003</td>\n",
       "      <td>WA</td>\n",
       "      <td>WA | ASOTIN</td>\n",
       "      <td>WA</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31023 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      BUYER_STATE  BUYER_ZIP BUYER_COUNTY    Year           MME  StateFIPS  \\\n",
       "0              CO    80002.0    jefferson  2006.0   8060.375877          8   \n",
       "1              CO    80002.0    jefferson  2007.0  11845.177512          8   \n",
       "2              CO    80002.0    jefferson  2008.0  11938.070956          8   \n",
       "3              CO    80002.0    jefferson  2009.0  13742.309018          8   \n",
       "4              CO    80002.0    jefferson  2010.0  13043.676747          8   \n",
       "...           ...        ...          ...     ...           ...        ...   \n",
       "31018          WA    99403.0       asotin  2010.0  13635.555484         53   \n",
       "31019          WA    99403.0       asotin  2011.0  15274.972710         53   \n",
       "31020          WA    99403.0       asotin  2012.0  15922.687523         53   \n",
       "31021          WA    99403.0       asotin  2013.0  17184.793767         53   \n",
       "31022          WA    99403.0       asotin  2014.0  16922.194147         53   \n",
       "\n",
       "       CountyFIPS_3 CountyName   StateName  CountyFIPS StateAbbr  \\\n",
       "0                59  jefferson    Colorado        8059        CO   \n",
       "1                59  jefferson    Colorado        8059        CO   \n",
       "2                59  jefferson    Colorado        8059        CO   \n",
       "3                59  jefferson    Colorado        8059        CO   \n",
       "4                59  jefferson    Colorado        8059        CO   \n",
       "...             ...        ...         ...         ...       ...   \n",
       "31018             3     asotin  Washington       53003        WA   \n",
       "31019             3     asotin  Washington       53003        WA   \n",
       "31020             3     asotin  Washington       53003        WA   \n",
       "31021             3     asotin  Washington       53003        WA   \n",
       "31022             3     asotin  Washington       53003        WA   \n",
       "\n",
       "         STATE_COUNTY state_abbrev _merge  \n",
       "0      CO | JEFFERSON           CO   both  \n",
       "1      CO | JEFFERSON           CO   both  \n",
       "2      CO | JEFFERSON           CO   both  \n",
       "3      CO | JEFFERSON           CO   both  \n",
       "4      CO | JEFFERSON           CO   both  \n",
       "...               ...          ...    ...  \n",
       "31018     WA | ASOTIN           WA   both  \n",
       "31019     WA | ASOTIN           WA   both  \n",
       "31020     WA | ASOTIN           WA   both  \n",
       "31021     WA | ASOTIN           WA   both  \n",
       "31022     WA | ASOTIN           WA   both  \n",
       "\n",
       "[31023 rows x 14 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prescriptions_fips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if buyer county is null, replace it with countyname\n",
    "#prescriptions_fips.loc[prescriptions_fips[\"BUYER_COUNTY\"].isnull(), \"BUYER_COUNTY\"] = prescriptions_fips.loc[prescriptions_fips[\"BUYER_COUNTY\"].isnull(), \"CountyName\"]\n",
    "\n",
    "# do the same with state\n",
    "#prescriptions_fips.loc[prescriptions_fips[\"BUYER_STATE\"].isnull(), \"BUYER_STATE\"] = prescriptions_fips.loc[prescriptions_fips[\"BUYER_STATE\"].isnull(), \"state_abbrev\"]\n",
    "\n",
    "# fill in null mme with 0\n",
    "#prescriptions_fips[\"MME\"] = prescriptions_fips[\"MME\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2007.0    3503\n",
       "2006.0    3498\n",
       "2010.0    3452\n",
       "2009.0    3449\n",
       "2011.0    3448\n",
       "2012.0    3448\n",
       "2008.0    3428\n",
       "2013.0    3423\n",
       "2014.0    3407\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prescriptions_fips.Year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of state and unique counties\n",
    "prescription_states = prescriptions_fips[\"BUYER_STATE\"].unique()\n",
    "#county_list = prescriptions_fips[\"BUYER_COUNTY\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n",
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_6584/3929940819.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  prescriptions_fips = prescriptions_fips.append(\n"
     ]
    }
   ],
   "source": [
    "for state in prescription_states:\n",
    "    county_list = prescriptions_fips[prescriptions_fips[\"BUYER_STATE\"] == state][\"BUYER_COUNTY\"].unique()\n",
    "    for county in county_list:\n",
    "        for year in range(2006, 2015):\n",
    "            if (year not in prescriptions_fips[(prescriptions_fips[\"BUYER_COUNTY\"] == county) & (prescriptions_fips[\"BUYER_STATE\"] == state)].values\n",
    "            ):\n",
    "                prescriptions_fips = prescriptions_fips.append(\n",
    "                    {\n",
    "                        \"BUYER_STATE\": state,\n",
    "                        \"BUYER_COUNTY\": county,\n",
    "                        \"Year\": year,\n",
    "                        \"MME\": 0\n",
    "                    },\n",
    "                    ignore_index=True,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding fips to our cause of death data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copies of both dfs\n",
    "\n",
    "cause_of_death_copy = df_cause_of_death.copy()\n",
    "fips_copy = fips.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove county once again\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: remove_county(x))\n",
    "\n",
    "\n",
    "# clean some other miscellaneous values up\n",
    "\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"LaSalle Parish\", \"La Salle Parish\"))\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"DeBaca\", \"De Baca\"))\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"La Salle\", \"La Salle County\"))\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"La Salle County Parish\", \"La Salle Parish\"))\n",
    "\n",
    "\n",
    "\n",
    "# expand mckean to mc kean in fips_copy\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"McKean\", \"Mc Kean\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_of_death_fips = cause_of_death_copy.merge(fips_copy, left_on=[\"County\", \"State\"], right_on=[\"CountyName\", \"state_abbrev\"], how=\"outer\", indicator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not all counties joining back to cause of death dataset\n",
    "\n",
    "If the number of people in a given category (eg. one county/year/cause of death category) is less than 10, those records do not appear in this data. There is also a technicality in the number of total deaths vs. drug deaths (which we are interested in).\n",
    "\n",
    "The example we are given is that if a county has 20 deaths unrelated to drugs and alcohol, and only 7 related to alcohol, only the former figure will be reported. In the next notebook (pick_states.ipynb), we will filter by cause of death. In this notebook, since we still have all causes of death, we will impute for every missing value.\n",
    "\n",
    "To impute this data, we will fill in missing values with **a random integer from 0 to 9**. We thought of drawing from a normal distribution, but this implies negative values could be attained. We could take their absolute values to negate this effect, but then we are no longer drawing from a *true* normal distribution, so we chose to pick random values in our range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace null value with a random integer from 0 to 10 with a normal distribution\n",
    "def value_imputer(x):\n",
    "    if pd.isnull(x):\n",
    "        return random.randint(0, 9)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "cause_of_death_fips[\"Deaths\"] = cause_of_death_fips[\"Deaths\"].apply(lambda x: value_imputer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    2\n",
       "7    1\n",
       "5    1\n",
       "6    1\n",
       "3    1\n",
       "Name: Deaths, dtype: int64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick look at our new imputed data\n",
    "cause_of_death_fips[cause_of_death_fips[\"_merge\"] != \"both\"].Deaths.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Population to final DataFrames\n",
    "\n",
    "For pop_fips, cause_of_death_fips, and prescription_fips. Steps needed:\n",
    "\n",
    "- Create unique ID from county FIPS and state FIPS\n",
    "- Merge population dataset based on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_of_death_fips = cause_of_death_fips[cause_of_death_fips[\"_merge\"] == \"both\"]\n",
    "#pops_copy = pops_copy[cause_of_death_fips[\"_merge\"] == \"both\"]\n",
    "#prescriptions_fips = prescriptions_fips[prescriptions_fips[\"_merge\"] == \"both\"]\n",
    "\n",
    "\n",
    "# drop merge columns\n",
    "cause_of_death_fips.drop(columns=[\"_merge\"], inplace=True)\n",
    "prescriptions_fips.drop(columns=[\"_merge\"], inplace=True)\n",
    "pops_copy.drop(columns=[\"_merge\",], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique FIP from county and state fips\n",
    "\n",
    "cause_of_death_fips[\"FIP_unique\"] = cause_of_death_fips[\"CountyFIPS\"].apply(lambda x: str(x)) + cause_of_death_fips[\"StateFIPS\"].apply(lambda x: str(x))\n",
    "prescriptions_fips[\"FIP_unique\"] = prescriptions_fips[\"CountyFIPS\"].apply(lambda x: str(x)) + prescriptions_fips[\"StateFIPS\"].apply(lambda x: str(x))\n",
    "pops_copy[\"FIP_unique\"] = pops_copy[\"CountyFIPS\"].apply(lambda x: str(x)) + pops_copy[\"StateFIPS\"].apply(lambda x: str(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add some sort of assert here. not sure what it should be yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final prescriptions dataset with populations\n",
    "# can safely left join here, because we only need records in the prescriptions dataset\n",
    "prescriptions = prescriptions_fips.merge(pops_copy, on=[\"FIP_unique\", \"Year\"], how=\"left\", indicator=True)\n",
    "\n",
    "#assert (prescriptions[\"_merge\"] == \"both\").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2006.0    327\n",
       "2007.0    325\n",
       "2008.0    324\n",
       "2009.0    321\n",
       "2010.0    317\n",
       "2011.0    315\n",
       "2012.0    311\n",
       "2013.0    309\n",
       "2014.0    305\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STILL HAVE WASHINGTON YEARS HERE\n",
    "prescriptions[prescriptions[\"BUYER_STATE\"] == \"WA\"].Year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one more assert to check length\n",
    "assert len(prescriptions) == len(prescriptions_fips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some useless columns\n",
    "prescriptions.drop(columns=[\"_merge\", \"CountyName_y\", \"StateFIPS_y\", \"CountyFIPS_y\",\"state_abbrev_y\", \"County\", \"CountyFIPS_3\"], inplace=True)\n",
    "\n",
    "# rename x columns\n",
    "prescriptions.rename(columns={\"CountyName_x\": \"CountyName\", \"StateFIPS_x\": \"StateFIPS\", \"CountyFIPS_x\": \"CountyFIPS\", \"state_abbrev_x\": \"state_abbrev\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final cause of death dataset with populations\n",
    "# can safely left join here, because we only need records in the cause of death dataset\n",
    "cause_of_death = cause_of_death_fips.merge(pops_copy, on=[\"FIP_unique\", \"Year\"], how=\"left\", indicator=True)\n",
    "\n",
    "assert cause_of_death_fips.Deaths.isnull().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some useless columns\n",
    "cause_of_death.drop(columns=[\"_merge\", \"CountyName_y\", \"StateFIPS_y\", \"CountyFIPS_y\",\"state_abbrev_y\", \"County_y\", \"CountyFIPS_3\", \"State_y\"], inplace=True)\n",
    "\n",
    "# rename x columns\n",
    "cause_of_death.rename(columns={\"County_x\": \"County\", \"Year_x\": \"Year\", \"State_x\": \"State\", \"StateFIPS_x\": \"StateFIPS\", \"CountyFIPS_x\": \"CountyFIPS\", \"state_abbrev_x\": \"state_abbrev\", \"CountyName_x\": \"CountyName\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6584/3788078211.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_cause_of_death\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause_of_death\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_prescriptions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprescriptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# asserts to make sure we didn't lose any records from our original datasets\n",
    "\n",
    "assert len(df_cause_of_death) == len(cause_of_death)\n",
    "assert len(df_prescriptions) == len(prescriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export main, unjoined datasets in case we need them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9376/111128345.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcause_of_death\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"20_intermediate_files/cause_of_death_clean.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprescriptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"20_intermediate_files/arcos_all_washpost_clean.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3718\u001b[0m         )\n\u001b[0;32m   3719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3720\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3721\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3722\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         )\n\u001b[1;32m-> 1189\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m             )\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[0mix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format_native_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m         libwriters.write_csv_rows(\n\u001b[0m\u001b[0;32m    316\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m             \u001b[0mix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cause_of_death.to_csv(\"20_intermediate_files/cause_of_death_clean.csv\", index=False)\n",
    "prescriptions.to_csv(\"20_intermediate_files/arcos_all_washpost_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final 3 datasets\n",
    "\n",
    "We should have: (UNSURE IF WE SHOULD EXTEND DATE RANGES, CURRENTLY 3 YEARS BEFORE AND AFTER POLICY IMPLEMENTATION)\n",
    "\n",
    "- Florida and Georgia 2007 - 2013\n",
    "- Texas and Oklahoma 2004 - 2010\n",
    "- Washington and Oregon 2009 - 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug overdose - broken down by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Florida and Georgia\n",
    "\n",
    "prescriptions_fl = prescriptions.copy()\n",
    "prescriptions_wa = prescriptions.copy()\n",
    "\n",
    "prescriptions_fl = prescriptions_fl[(prescriptions_fl[\"BUYER_STATE\"] == \"FL\") | (prescriptions_fl[\"BUYER_STATE\"].isin(fl_states))]\n",
    "prescriptions_wa = prescriptions_wa[(prescriptions_wa[\"BUYER_STATE\"] == \"WA\") | (prescriptions_wa[\"BUYER_STATE\"]).isin(wa_states)]\n",
    "\n",
    "\n",
    "\n",
    "# filter appropriate years\n",
    "fl_start = 2007\n",
    "fl_end = 2013\n",
    "\n",
    "# tx will only be used for overdose deaths\n",
    "tx_start = 2004\n",
    "tx_end = 2010\n",
    "\n",
    "wa_start = 2009\n",
    "wa_end = 2015\n",
    "\n",
    "\n",
    "prescriptions_fl = prescriptions_fl[(prescriptions_fl[\"Year\"] >= fl_start) & (prescriptions_fl[\"Year\"] <= fl_end)]\n",
    "prescriptions_wa = prescriptions_wa[(prescriptions_wa[\"Year\"] >= wa_start) & (prescriptions_wa[\"Year\"] <= wa_end)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cause of death - broken down by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_fl = cause_of_death.copy()\n",
    "deaths_tx = cause_of_death.copy()\n",
    "deaths_wa = cause_of_death.copy()\n",
    "\n",
    "deaths_fl = deaths_fl[(deaths_fl[\"StateName\"] == \"Florida\") | (deaths_fl[\"State\"].isin(fl_states))]\n",
    "deaths_tx = deaths_tx[(deaths_tx[\"StateName\"] == \"Texas\") | (deaths_tx[\"State\"].isin(tx_states))]\n",
    "deaths_wa = deaths_wa[(deaths_wa[\"StateName\"] == \"Washington\") | (deaths_wa[\"State\"].isin(wa_states))]\n",
    "\n",
    "deaths_fl = deaths_fl[(deaths_fl[\"Year\"] >= fl_start) & (deaths_fl[\"Year\"] <= fl_end)]\n",
    "deaths_tx = deaths_tx[(deaths_tx[\"Year\"] >= tx_start) & (deaths_tx[\"Year\"] <= tx_end)]  \n",
    "deaths_wa = deaths_wa[(deaths_wa[\"Year\"] >= wa_start) & (deaths_wa[\"Year\"] <= wa_end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2013.0    2681681\n",
       "2011.0    2656850\n",
       "2012.0    2654508\n",
       "2010.0    2562310\n",
       "2014.0    2433600\n",
       "2009.0    2419344\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prescriptions_wa.Year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_52032/2020115255.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mprescriptions_fl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfl_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfl_end\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mprescriptions_wa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwa_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwa_end\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mdeaths_fl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfl_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfl_end\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# final assert to check years\n",
    "\n",
    "assert prescriptions_fl.Year.unique().tolist() == list(range(fl_start, fl_end + 1))\n",
    "assert prescriptions_wa.Year.unique().tolist() == list(range(wa_start, wa_end + 1))\n",
    "\n",
    "assert deaths_fl.Year.unique().tolist() == list(range(fl_start, fl_end + 1))\n",
    "assert deaths_tx.Year.unique().tolist() == list(range(tx_start, tx_end + 1))\n",
    "assert deaths_wa.Year.unique().tolist() == list(range(wa_start, wa_end + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export all to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prescriptions_fl.to_csv(\"20_intermediate_files/prescriptions_fl.csv\", index=False)\n",
    "#prescriptions_wa.to_csv(\"20_intermediate_files/prescriptions_wa.csv\", index=False)\n",
    "\n",
    "deaths_fl.to_csv(\"20_intermediate_files/deaths_fl.csv\", index=False)\n",
    "deaths_tx.to_csv(\"20_intermediate_files/deaths_tx.csv\", index=False)\n",
    "deaths_wa.to_csv(\"20_intermediate_files/deaths_wa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all as parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prescriptions_fl.to_parquet('20_intermediate_files/prescriptions_fl.parquet', engine='fastparquet', row_group_offsets=10_000_000)\n",
    "prescriptions_wa.to_parquet('20_intermediate_files/prescriptions_wa.parquet', engine='fastparquet', row_group_offsets=10_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for the group\n",
    "\n",
    "- may need to filter out a couple more columns - haven't done this yet as I don't want to accidentally delete something we need\n",
    "- overdose data is only broken down by year unless i messed something up - overdose analysis will have to be less granular"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49aa7209ac0e1a36a89cb04290394fd089cb5ce56cb44c9d4652c0180c6152a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
