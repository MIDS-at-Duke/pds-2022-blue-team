{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import xlrd\n",
    "import re\n",
    "import openpyxl\n",
    "import random\n",
    "\n",
    "# custom file that maps state names to abbreviations\n",
    "from abbreviation_conversion import abbrev_to_us_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up prescription data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing the find_year function: 2013\n",
      "testing the find_month function: 12\n"
     ]
    }
   ],
   "source": [
    "def find_year(TRANSACTION_DATE):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        TRANSACTION_DATE (str): date in format MMDDYYYY\n",
    "\n",
    "    Returns:\n",
    "        int: year\n",
    "    \"\"\"\n",
    "    TRANSACTION_DATE = str(TRANSACTION_DATE)\n",
    "    \n",
    "    return int(TRANSACTION_DATE[-4:])\n",
    "\n",
    "# quick test \n",
    "print(f\"testing the find_year function: {find_year(12202013)}\")\n",
    "\n",
    "\n",
    "def find_month(TRANSACTION_DATE):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        TRANSACTION_DATE (str): date in format MMDDYYYY\n",
    "\n",
    "    Returns:\n",
    "        int: month\n",
    "    \"\"\"\n",
    "    TRANSACTION_DATE = str(TRANSACTION_DATE)\n",
    "\n",
    "    if len(TRANSACTION_DATE) == 8:\n",
    "        return int(TRANSACTION_DATE[:2])\n",
    "    else:\n",
    "        return int(TRANSACTION_DATE[:1])\n",
    "    \n",
    "\n",
    "# quick test \n",
    "print(f\"testing the find_month function: {find_month(12202013)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load in the data, we need to truncate the amount of columns we use as well as the states\n",
    "cols_to_keep = [\"BUYER_STATE\", \"BUYER_ZIP\", \"BUYER_COUNTY\", \"DRUG_CODE\", \"DRUG_NAME\", \"QUANTITY\", \"TRANSACTION_DATE\"]\n",
    "\n",
    "# we know we need Florida, Texas, and Washington\n",
    "states = [\"FL\", \"TX\", \"WA\"]\n",
    "# since we are normalizing based on population, I think we should pick states that are regionally close to our target states\n",
    "# we can change this later as a group, but I have these selected below:\n",
    "\n",
    "# Florida comparison states\n",
    "fl_states = [\"PA\", \"MI\", \"NC\"]\n",
    "\n",
    "# Texas comparison states\n",
    "tx_states = [\"IL\", \"MA\", \"MI\"]\n",
    "\n",
    "# Washington comparison states\n",
    "wa_states = [\"NC\", \"CO\", \"MD\"]\n",
    "\n",
    "# create list of all states to use\n",
    "variable_states = []\n",
    "variable_states.extend(fl_states)\n",
    "variable_states.extend(tx_states)\n",
    "variable_states.extend(wa_states)\n",
    "\n",
    "# append variable states to our original list\n",
    "states.extend(variable_states)\n",
    "\n",
    "\n",
    "# create separate list of only florida and washington states for prescription data\n",
    "prescription_states = [state for state in states if state not in [\"IL\", \"MA\", \"MI\", \"TX\"]]\n",
    "\n",
    "# NC is appearing twice as it's a comparison state for both target states\n",
    "# making this a set will remove the duplicate\n",
    "prescription_states = list(set(prescription_states))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in individual state prescription data\n",
    "\n",
    "The Washington Post article associated with our data states that data from 2013 and 2014 was only recently added. Resultingly, we found that it was missing from the large dataset of all states. However, upon further digging, we found that these years were present **on an individual state level**, so we will load these in and concatenate them with our larger dataframe above.\n",
    "\n",
    "Without chunking, the below takes 30 seconds for each file to load in. With chunking, this is reduced to about 4 seconds per record, so please make sure to leave this in its current format.\n",
    "\n",
    "\n",
    "\n",
    "current issues:\n",
    "- way more records for 2013 and 2014\n",
    "- non WA states not being read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing record 0 of filename 00_source_data/state_prescriptions\\arcos-co-statewide-itemized.csv\n",
      "Finished processing record 1 of filename 00_source_data/state_prescriptions\\arcos-fl-statewide-itemized.csv\n",
      "Finished processing record 2 of filename 00_source_data/state_prescriptions\\arcos-md-statewide-itemized.csv\n",
      "Finished processing record 3 of filename 00_source_data/state_prescriptions\\arcos-mi-statewide-itemized.csv\n",
      "Finished processing record 4 of filename 00_source_data/state_prescriptions\\arcos-nc-statewide-itemized.csv\n",
      "Finished processing record 5 of filename 00_source_data/state_prescriptions\\arcos-pa-statewide-itemized.csv\n",
      "Finished processing record 6 of filename 00_source_data/state_prescriptions\\arcos-wa-statewide-itemized.csv\n"
     ]
    }
   ],
   "source": [
    "# set FINAL cols we want\n",
    "cols_to_keep = [\"BUYER_STATE\", \"BUYER_ZIP\", \"BUYER_COUNTY\", \"DRUG_CODE\", \"DRUG_NAME\", \"TRANSACTION_DATE\", \"MME_Conversion_Factor\", \"dos_str\", \"DOSAGE_UNIT\", \"CALC_BASE_WT_IN_GM\"]\n",
    "\n",
    "# set additional columns we need to calculate MME\n",
    "chunk_cols = [\"dos_str\", \"DOSAGE_UNIT\", \"MME_Conversion_Factor\",]\n",
    "\n",
    "\n",
    "\n",
    "path = r'00_source_data/state_prescriptions' # point to correct folder\n",
    "filenames = glob.glob(path + \"/*.csv\") # select all text files in folder\n",
    "\n",
    "assert len(filenames) == 7, \"There should be 7 files in the folder - check that we don't have a missing state\"\n",
    "\n",
    "df = pd.DataFrame() # empty df - will store data from all txt files\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for f, count in zip(filenames, range(len(filenames))):\n",
    "\n",
    "    it = pd.read_csv(f, chunksize=1_000_000, usecols = cols_to_keep) # may have to change chunksize depending on your computer's memory\n",
    "    \n",
    "    temp_df = pd.DataFrame()\n",
    "\n",
    "    for chunk in it:\n",
    "\n",
    "        # ensure dtypes for faster calculation below\n",
    "        float_cols = [\"MME_Conversion_Factor\", \"dos_str\", \"DOSAGE_UNIT\", \"CALC_BASE_WT_IN_GM\"]\n",
    "        chunk[float_cols] = chunk[float_cols].astype(\"float64\")\n",
    "\n",
    "        # calculate MME\n",
    "        chunk[\"MME\"] = chunk[\"dos_str\"] * chunk[\"MME_Conversion_Factor\"] * chunk[\"DOSAGE_UNIT\"]\n",
    "\n",
    "        chunk = chunk[cols_to_keep]\n",
    "        # ensure we're working in the correct date range\n",
    "        #filtered_chunk = chunk[chunk[\"year\"] > 2002]\n",
    "        #filtered_chunk = filtered_chunk[filtered_chunk[\"year\"] < 2016]\n",
    "\n",
    "        # extract year out of date column\n",
    "        chunk[\"year\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: int(find_year(x)))\n",
    "\n",
    "        # calculate int cols\n",
    "        \n",
    "        int_cols = [\"BUYER_ZIP\", \"year\"]\n",
    "        chunk[int_cols] = chunk[int_cols].astype(\"int64\")\n",
    "\n",
    "        temp_df = pd.concat([temp_df, chunk])\n",
    "\n",
    "    print(f\"Finished processing record {count} of filename {f}\")\n",
    "    df = pd.concat([df_prescriptions, temp_df], axis=0, ignore_index=True)\n",
    "\n",
    "df.drop(columns={\"TRANSACTION_DATE\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"MME\"] = df[\"CALC_BASE_WT_IN_GM\"] * df[\"MME_Conversion_Factor\"]\n",
    "\n",
    "assert len(df[df[\"MME\"].isnull()]) == 0, \"There should be no missing MME calculations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null counties & no shipments in counties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null counties & no shipments in counties\n",
    "\n",
    "# check for null counties\n",
    "#df_prescriptions[df_prescriptions[\"BUYER_COUNTY\"].isnull()]\n",
    "# uncommenting the above line shows us only one missing county - zip code 34635\n",
    "# this is bellair beach, so we'll fill this value in and add an assert to ensure no more missing counties\n",
    "\n",
    "# replace buyer county where buyer zip is equal to 34635 with bellair beach\n",
    "df.loc[df[\"BUYER_ZIP\"] == 34635, \"BUYER_COUNTY\"] = \"Pinellas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df[df[\"BUYER_COUNTY\"].isnull()]) == 0, \"There should be no missing counties\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### group by state, county, and year\n",
    "\n",
    "Turns unit of observation into one MME calculation per county-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BUYER_STATE</th>\n",
       "      <th>BUYER_ZIP</th>\n",
       "      <th>BUYER_COUNTY</th>\n",
       "      <th>DRUG_CODE</th>\n",
       "      <th>DRUG_NAME</th>\n",
       "      <th>MME_Conversion_Factor</th>\n",
       "      <th>dos_str</th>\n",
       "      <th>DOSAGE_UNIT</th>\n",
       "      <th>CALC_BASE_WT_IN_GM</th>\n",
       "      <th>year</th>\n",
       "      <th>MME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CO</td>\n",
       "      <td>81005</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.454050</td>\n",
       "      <td>2009</td>\n",
       "      <td>0.454050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>CO</td>\n",
       "      <td>81004</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2.270250</td>\n",
       "      <td>2006</td>\n",
       "      <td>2.270250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>CO</td>\n",
       "      <td>81004</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.605400</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.605400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>CO</td>\n",
       "      <td>81004</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.513500</td>\n",
       "      <td>2006</td>\n",
       "      <td>1.513500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>CO</td>\n",
       "      <td>81004</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>1.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.672375</td>\n",
       "      <td>2006</td>\n",
       "      <td>1.008563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3818207</th>\n",
       "      <td>CO</td>\n",
       "      <td>81005</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.908100</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.908100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3818208</th>\n",
       "      <td>CO</td>\n",
       "      <td>81005</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>3.027000</td>\n",
       "      <td>2014</td>\n",
       "      <td>3.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3818209</th>\n",
       "      <td>CO</td>\n",
       "      <td>81005</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2.270250</td>\n",
       "      <td>2014</td>\n",
       "      <td>2.270250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3818210</th>\n",
       "      <td>CO</td>\n",
       "      <td>81005</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.513500</td>\n",
       "      <td>2014</td>\n",
       "      <td>1.513500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3818247</th>\n",
       "      <td>CO</td>\n",
       "      <td>81003</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>1.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>3.227400</td>\n",
       "      <td>2013</td>\n",
       "      <td>4.841100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211443 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        BUYER_STATE  BUYER_ZIP BUYER_COUNTY  DRUG_CODE    DRUG_NAME  \\\n",
       "0                CO      81005       PUEBLO       9193  HYDROCODONE   \n",
       "324              CO      81004       PUEBLO       9193  HYDROCODONE   \n",
       "325              CO      81004       PUEBLO       9193  HYDROCODONE   \n",
       "326              CO      81004       PUEBLO       9193  HYDROCODONE   \n",
       "327              CO      81004       PUEBLO       9143    OXYCODONE   \n",
       "...             ...        ...          ...        ...          ...   \n",
       "3818207          CO      81005       PUEBLO       9193  HYDROCODONE   \n",
       "3818208          CO      81005       PUEBLO       9193  HYDROCODONE   \n",
       "3818209          CO      81005       PUEBLO       9193  HYDROCODONE   \n",
       "3818210          CO      81005       PUEBLO       9193  HYDROCODONE   \n",
       "3818247          CO      81003       PUEBLO       9143    OXYCODONE   \n",
       "\n",
       "         MME_Conversion_Factor  dos_str  DOSAGE_UNIT  CALC_BASE_WT_IN_GM  \\\n",
       "0                          1.0      5.0        150.0            0.454050   \n",
       "324                        1.0      7.5        500.0            2.270250   \n",
       "325                        1.0     10.0        100.0            0.605400   \n",
       "326                        1.0      5.0        500.0            1.513500   \n",
       "327                        1.5      7.5        100.0            0.672375   \n",
       "...                        ...      ...          ...                 ...   \n",
       "3818207                    1.0      5.0        300.0            0.908100   \n",
       "3818208                    1.0      5.0       1000.0            3.027000   \n",
       "3818209                    1.0      7.5        500.0            2.270250   \n",
       "3818210                    1.0      5.0        500.0            1.513500   \n",
       "3818247                    1.5     10.0        360.0            3.227400   \n",
       "\n",
       "         year       MME  \n",
       "0        2009  0.454050  \n",
       "324      2006  2.270250  \n",
       "325      2006  0.605400  \n",
       "326      2006  1.513500  \n",
       "327      2006  1.008563  \n",
       "...       ...       ...  \n",
       "3818207  2013  0.908100  \n",
       "3818208  2014  3.027000  \n",
       "3818209  2014  2.270250  \n",
       "3818210  2014  1.513500  \n",
       "3818247  2013  4.841100  \n",
       "\n",
       "[211443 rows x 11 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"BUYER_COUNTY\"] == \"PUEBLO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby to get one row per county year\n",
    "# keeping zip for now, may not be necessary\n",
    "\n",
    "df_prescriptions = df.groupby([\"BUYER_STATE\", \"BUYER_COUNTY\", \"year\"])[\"MME\"].sum().reset_index()\n",
    "\n",
    "\n",
    "#df_prescriptions = df_grouped.copy() # temporary - will remove this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BUYER_STATE</th>\n",
       "      <th>BUYER_COUNTY</th>\n",
       "      <th>year</th>\n",
       "      <th>MME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>CO</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>2006</td>\n",
       "      <td>66467.019279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>CO</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>2007</td>\n",
       "      <td>80805.048150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>CO</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>2008</td>\n",
       "      <td>98193.220958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>CO</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>2009</td>\n",
       "      <td>115942.791973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>CO</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>2010</td>\n",
       "      <td>141464.937486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>CO</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>2011</td>\n",
       "      <td>180468.041823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>CO</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>2012</td>\n",
       "      <td>204180.305891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>CO</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>2013</td>\n",
       "      <td>146468.052559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>CO</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>2014</td>\n",
       "      <td>121860.909022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    BUYER_STATE BUYER_COUNTY  year            MME\n",
       "423          CO       PUEBLO  2006   66467.019279\n",
       "424          CO       PUEBLO  2007   80805.048150\n",
       "425          CO       PUEBLO  2008   98193.220958\n",
       "426          CO       PUEBLO  2009  115942.791973\n",
       "427          CO       PUEBLO  2010  141464.937486\n",
       "428          CO       PUEBLO  2011  180468.041823\n",
       "429          CO       PUEBLO  2012  204180.305891\n",
       "430          CO       PUEBLO  2013  146468.052559\n",
       "431          CO       PUEBLO  2014  121860.909022"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prescriptions[df_prescriptions[\"BUYER_COUNTY\"] == \"PUEBLO\"].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problems\n",
    "- read huge arcos file, never unzipped (read as compressed zip file)\n",
    "? limited cols taking in, filtered for specific states he wanted\n",
    " - couple of transofmrs - pulled year col, std morpine calculation\n",
    "? saved memory! -> last thing for each chunk, add state abbrev for county name\n",
    "then grouped chunk, chunk size fairly small (maybe he means agg to year now?)\n",
    "    go through x chunks, groupbys here he means\n",
    "    getting to pt where more manageable on memory + easier for other team members\n",
    "\n",
    "\n",
    "other thing:\n",
    "if you leave zipped, read chunks and do transforms on chunks, it'll get done in 12min (about)\n",
    "\n",
    "\n",
    "**NOT CURRENTLY USING THE BELOW CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# set FINAL cols we want\\ncols_to_keep = [\"BUYER_STATE\", \"BUYER_ZIP\", \"BUYER_COUNTY\", \"DRUG_CODE\", \"DRUG_NAME\", \"TRANSACTION_DATE\", \"MME_Conversion_Factor\", \"dos_str\", \"DOSAGE_UNIT\"]\\n\\n# set additional columns we need to calculate MME\\nchunk_cols = [\"dos_str\", \"DOSAGE_UNIT\", \"MME_Conversion_Factor\"]\\n\\n\\ndf_prescriptions = pd.DataFrame() # empty df - will store data from all txt files\\n\\n# need 1 row per county month**** could aggergate it here also?\\nit = pd.read_csv(\"00_source_data/arcos_all_washpost.tsv.gz\", chunksize=1_000_000, sep=\\'\\t\\', usecols=cols_to_keep, low_memory=False) # may have to change chunksize depending on your computer\\'s memory\\ncols_to_keep.extend([\"year\", \"MME\"])\\n\\ntemp_df = pd.DataFrame()\\ncounter = 0\\nfor chunk in it:\\n\\n    # first, filter to only the states that we want\\n    # good first step as this will elminate unecessary calculations on rows we don\\'t need\\n    chunk = chunk[chunk[\"BUYER_STATE\"].isin(prescription_states)]\\n\\n\\n    \\n    # extract year out of date column\\n\\n    #chunk[\\'DATE\\'] = pd.to_datetime(chunk[\\'TRANSACTION_DATE\\'], format=\\'%m%d%Y\\')\\n\\n    # pull out only the year from the date field\\n    #chunk[\\'year\\']= chunk[\\'DATE\\'].dt.year\\n\\n    #chunk[\"year\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: x[-4:])\\n    chunk[\"year\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: int(find_year(x)))\\n\\n    #chunk = chunk[chunk[\"year\"] > 2012]\\n\\n\\n    # ensure dtypes for faster calculation below\\n    float_cols = [\"MME_Conversion_Factor\", \"dos_str\", \"DOSAGE_UNIT\"]\\n    chunk[float_cols] = chunk[float_cols].astype(\"float64\")\\n\\n\\n\\n    # calculate MME\\n    chunk[\"MME\"] = chunk[\"dos_str\"] * chunk[\"MME_Conversion_Factor\"] * chunk[\"DOSAGE_UNIT\"]\\n\\n    chunk = chunk[cols_to_keep]\\n\\n    # ensure we\\'re working in the correct date range\\n    # could map a dict with states + desired years to make this faster later\\n    #chunk = chunk[chunk[\"year\"] > 2012]\\n    #filtered_chunk = filtered_chunk[filtered_chunk[\"year\"] < 2016]\\n\\n\\n    # calculate int cols\\n    \\n    int_cols = [\"BUYER_ZIP\", \"year\"]\\n    chunk[int_cols] = chunk[int_cols].astype(\"int64\")\\n\\n    df_prescriptions = pd.concat([df_prescriptions, chunk])\\n\\n    print(f\"chunk {counter} processed\")\\n    counter+=1\\n    # if counter == 10:\\n    #     break\\n\\n\\ndf_prescriptions.drop(columns={\"TRANSACTION_DATE\"}, inplace=True)\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# set FINAL cols we want\n",
    "cols_to_keep = [\"BUYER_STATE\", \"BUYER_ZIP\", \"BUYER_COUNTY\", \"DRUG_CODE\", \"DRUG_NAME\", \"TRANSACTION_DATE\", \"MME_Conversion_Factor\", \"dos_str\", \"DOSAGE_UNIT\"]\n",
    "\n",
    "# set additional columns we need to calculate MME\n",
    "chunk_cols = [\"dos_str\", \"DOSAGE_UNIT\", \"MME_Conversion_Factor\"]\n",
    "\n",
    "\n",
    "df_prescriptions = pd.DataFrame() # empty df - will store data from all txt files\n",
    "\n",
    "# need 1 row per county month**** could aggergate it here also?\n",
    "it = pd.read_csv(\"00_source_data/arcos_all_washpost.tsv.gz\", chunksize=1_000_000, sep='\\t', usecols=cols_to_keep, low_memory=False) # may have to change chunksize depending on your computer's memory\n",
    "cols_to_keep.extend([\"year\", \"MME\"])\n",
    "\n",
    "temp_df = pd.DataFrame()\n",
    "counter = 0\n",
    "for chunk in it:\n",
    "\n",
    "    # first, filter to only the states that we want\n",
    "    # good first step as this will elminate unecessary calculations on rows we don't need\n",
    "    chunk = chunk[chunk[\"BUYER_STATE\"].isin(prescription_states)]\n",
    "\n",
    "\n",
    "    \n",
    "    # extract year out of date column\n",
    "\n",
    "    #chunk['DATE'] = pd.to_datetime(chunk['TRANSACTION_DATE'], format='%m%d%Y')\n",
    "\n",
    "    # pull out only the year from the date field\n",
    "    #chunk['year']= chunk['DATE'].dt.year\n",
    "\n",
    "    #chunk[\"year\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: x[-4:])\n",
    "    chunk[\"year\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: int(find_year(x)))\n",
    "\n",
    "    #chunk = chunk[chunk[\"year\"] > 2012]\n",
    "\n",
    "\n",
    "    # ensure dtypes for faster calculation below\n",
    "    float_cols = [\"MME_Conversion_Factor\", \"dos_str\", \"DOSAGE_UNIT\"]\n",
    "    chunk[float_cols] = chunk[float_cols].astype(\"float64\")\n",
    "\n",
    "\n",
    "\n",
    "    # calculate MME\n",
    "    chunk[\"MME\"] = chunk[\"dos_str\"] * chunk[\"MME_Conversion_Factor\"] * chunk[\"DOSAGE_UNIT\"]\n",
    "\n",
    "    chunk = chunk[cols_to_keep]\n",
    "\n",
    "    # ensure we're working in the correct date range\n",
    "    # could map a dict with states + desired years to make this faster later\n",
    "    #chunk = chunk[chunk[\"year\"] > 2012]\n",
    "    #filtered_chunk = filtered_chunk[filtered_chunk[\"year\"] < 2016]\n",
    "\n",
    "\n",
    "    # calculate int cols\n",
    "    \n",
    "    int_cols = [\"BUYER_ZIP\", \"year\"]\n",
    "    chunk[int_cols] = chunk[int_cols].astype(\"int64\")\n",
    "\n",
    "    df_prescriptions = pd.concat([df_prescriptions, chunk])\n",
    "\n",
    "    print(f\"chunk {counter} processed\")\n",
    "    counter+=1\n",
    "    # if counter == 10:\n",
    "    #     break\n",
    "\n",
    "\n",
    "df_prescriptions.drop(columns={\"TRANSACTION_DATE\"}, inplace=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up cause of death data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'00_source_data/cause_of_death' # point to correct folder\n",
    "filenames = glob.glob(path + \"/*.txt\") # select all text files in folder\n",
    "\n",
    "df = pd.DataFrame() # empty df - will store data from all txt files\n",
    "\n",
    "for f in filenames:\n",
    "    temp = pd.read_csv(f, index_col=None, header=0, sep='\\t')\n",
    "    # we're getting some extraneous notes at the bottom - let's just drop based on county as these will only be null for these useless notes columns\n",
    "    temp.dropna(subset={'County'}, inplace=True)\n",
    "    \n",
    "    df = pd.concat([df, temp], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to separate county and state\n",
    "\n",
    "def abtract_state(county):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        county (str): county name\n",
    "\n",
    "    Returns:\n",
    "        str: state\n",
    "    \"\"\"\n",
    "    return county.split(\", \")[1]\n",
    "\n",
    "\n",
    "\n",
    "def abstract_county(county):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        county (str): county name\n",
    "\n",
    "    Returns:\n",
    "        str: county\n",
    "    \"\"\"\n",
    "    return county.split(\", \")[0]\n",
    "\n",
    "# apply functions to our df\n",
    "df[\"State\"] = df.apply(lambda x: abtract_state(x[\"County\"]), axis=1)\n",
    "df[\"County\"] = df.apply(lambda x: abstract_county(x[\"County\"]), axis=1)\n",
    "\n",
    "# do not need notes column, let's just drop it here\n",
    "df.drop(columns={\"Notes\"}, inplace=True)\n",
    "\n",
    "df_cause_of_death = df.copy() # keep a copy of this df for later filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's filter our dataframe to be only the states we want\n",
    "df_cause_of_death = df_cause_of_death[df_cause_of_death[\"State\"].isin(states)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding in County Population data\n",
    "\n",
    "[Census county pop. data, 2000-2010](https://www.census.gov/data/tables/time-series/demo/popest/intercensal-2000-2010-counties.html)<br>\n",
    "[Census county pop. data, 2010-2019](https://www.census.gov/data/datasets/time-series/demo/popest/2010s-counties-total.html)<br>\n",
    "For both, just select the appropriate states on the webpage. We will clean and merge as needed in this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guide to cleaning - 2000s data\n",
    "\n",
    "The way the 2000s excel files are formatted, we can clean the data in the following way\n",
    "\n",
    "- load in with header=3\n",
    "- drop null on any of the populations\n",
    "    - notes at the bottom will be removed\n",
    "- drop unnamed 1, 12, and 13\n",
    "    - these contain redundant data about populations from specific dates\n",
    "    - Unnamed 12 is 2010s pop - will be redundant as our next dataset has this as well. Using the newer data\n",
    "- drop first row\n",
    "    - state as a whole\n",
    "- rename Unnamed: 0 to county\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams County</td>\n",
       "      <td>350888.0</td>\n",
       "      <td>359816.0</td>\n",
       "      <td>370753.0</td>\n",
       "      <td>377464.0</td>\n",
       "      <td>384809.0</td>\n",
       "      <td>395146.0</td>\n",
       "      <td>406575.0</td>\n",
       "      <td>415746.0</td>\n",
       "      <td>424913.0</td>\n",
       "      <td>435700.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alamosa County</td>\n",
       "      <td>14954.0</td>\n",
       "      <td>14956.0</td>\n",
       "      <td>15114.0</td>\n",
       "      <td>15067.0</td>\n",
       "      <td>15217.0</td>\n",
       "      <td>15236.0</td>\n",
       "      <td>15196.0</td>\n",
       "      <td>15180.0</td>\n",
       "      <td>15300.0</td>\n",
       "      <td>15289.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arapahoe County</td>\n",
       "      <td>491482.0</td>\n",
       "      <td>502393.0</td>\n",
       "      <td>508936.0</td>\n",
       "      <td>513690.0</td>\n",
       "      <td>518971.0</td>\n",
       "      <td>524466.0</td>\n",
       "      <td>531619.0</td>\n",
       "      <td>542039.0</td>\n",
       "      <td>552461.0</td>\n",
       "      <td>563161.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Archuleta County</td>\n",
       "      <td>10020.0</td>\n",
       "      <td>10454.0</td>\n",
       "      <td>10885.0</td>\n",
       "      <td>11089.0</td>\n",
       "      <td>11266.0</td>\n",
       "      <td>11496.0</td>\n",
       "      <td>11937.0</td>\n",
       "      <td>12262.0</td>\n",
       "      <td>12250.0</td>\n",
       "      <td>12169.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baca County</td>\n",
       "      <td>4501.0</td>\n",
       "      <td>4471.0</td>\n",
       "      <td>4336.0</td>\n",
       "      <td>4117.0</td>\n",
       "      <td>4064.0</td>\n",
       "      <td>3997.0</td>\n",
       "      <td>3933.0</td>\n",
       "      <td>3866.0</td>\n",
       "      <td>3806.0</td>\n",
       "      <td>3767.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             County      2000      2001      2002      2003      2004  \\\n",
       "0      Adams County  350888.0  359816.0  370753.0  377464.0  384809.0   \n",
       "1    Alamosa County   14954.0   14956.0   15114.0   15067.0   15217.0   \n",
       "2   Arapahoe County  491482.0  502393.0  508936.0  513690.0  518971.0   \n",
       "3  Archuleta County   10020.0   10454.0   10885.0   11089.0   11266.0   \n",
       "4       Baca County    4501.0    4471.0    4336.0    4117.0    4064.0   \n",
       "\n",
       "       2005      2006      2007      2008      2009 State  \n",
       "0  395146.0  406575.0  415746.0  424913.0  435700.0    CO  \n",
       "1   15236.0   15196.0   15180.0   15300.0   15289.0    CO  \n",
       "2  524466.0  531619.0  542039.0  552461.0  563161.0    CO  \n",
       "3   11496.0   11937.0   12262.0   12250.0   12169.0    CO  \n",
       "4    3997.0    3933.0    3866.0    3806.0    3767.0    CO  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init emmpty df for our population data\n",
    "pops00 = pd.DataFrame()\n",
    "\n",
    "# end goal - add every excel file in 00_source_data/county_pop/2000s to pops00\n",
    "\n",
    "path = r\"00_source_data/county_pop/2000s/\" # point to correct folder\n",
    "filenames = glob.glob(path + \"*.xls\")\n",
    "\n",
    "for f in filenames:\n",
    "\n",
    "    # read in current file with header = 3\n",
    "    temp = pd.read_excel(f, header = 3)\n",
    "\n",
    "    # regex to pull out state from filename\n",
    "    r = re.search(\"(2000s)(.)(\\w+)\", f)[3]\n",
    "    temp[\"State\"] = r[:2].upper()\n",
    "    \n",
    "    # drop null on any of the years\n",
    "    temp.dropna(subset=[2000], inplace=True)\n",
    "\n",
    "    #drop useless columns\n",
    "    temp.drop(columns={\"Unnamed: 1\", \"Unnamed: 12\", \"Unnamed: 13\"}, inplace=True)\n",
    "\n",
    "    # drop first row\n",
    "    temp = temp.iloc[1:, :]\n",
    "\n",
    "    # rename some cols\n",
    "    temp.rename(columns={\"Unnamed: 0\": \"County\"}, inplace=True)\n",
    "\n",
    "    # remove period at beginning of each county\n",
    "    temp[\"County\"] = temp[\"County\"].apply(lambda x: x[1:])\n",
    "\n",
    "    pops00 = pd.concat([pops00, temp], axis=0, ignore_index=True)\n",
    "\n",
    "# quick peek at the data\n",
    "pops00.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guide to cleaning - 2010s data\n",
    "\n",
    "The way the 2010s excel files are formatted, we can clean the data in the following way\n",
    "\n",
    "- load in with header=3\n",
    "- drop null on any of the populations\n",
    "    - notes at the bottom will be removed\n",
    "- drop census, estimates base\n",
    "- drop first row\n",
    "    - state as a whole\n",
    "- rename Unnamed: 0 to county\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams County</td>\n",
       "      <td>443691.0</td>\n",
       "      <td>452201.0</td>\n",
       "      <td>460558.0</td>\n",
       "      <td>469978.0</td>\n",
       "      <td>479946.0</td>\n",
       "      <td>490443.0</td>\n",
       "      <td>497734.0</td>\n",
       "      <td>503590.0</td>\n",
       "      <td>511354.0</td>\n",
       "      <td>517421.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alamosa County</td>\n",
       "      <td>15515.0</td>\n",
       "      <td>15709.0</td>\n",
       "      <td>15680.0</td>\n",
       "      <td>15787.0</td>\n",
       "      <td>15803.0</td>\n",
       "      <td>15894.0</td>\n",
       "      <td>16053.0</td>\n",
       "      <td>16108.0</td>\n",
       "      <td>16248.0</td>\n",
       "      <td>16233.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arapahoe County</td>\n",
       "      <td>574747.0</td>\n",
       "      <td>585968.0</td>\n",
       "      <td>596500.0</td>\n",
       "      <td>608467.0</td>\n",
       "      <td>619034.0</td>\n",
       "      <td>630984.0</td>\n",
       "      <td>638950.0</td>\n",
       "      <td>644478.0</td>\n",
       "      <td>651797.0</td>\n",
       "      <td>656590.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Archuleta County</td>\n",
       "      <td>12046.0</td>\n",
       "      <td>12021.0</td>\n",
       "      <td>12132.0</td>\n",
       "      <td>12216.0</td>\n",
       "      <td>12231.0</td>\n",
       "      <td>12387.0</td>\n",
       "      <td>12825.0</td>\n",
       "      <td>13295.0</td>\n",
       "      <td>13730.0</td>\n",
       "      <td>14029.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baca County</td>\n",
       "      <td>3807.0</td>\n",
       "      <td>3778.0</td>\n",
       "      <td>3722.0</td>\n",
       "      <td>3656.0</td>\n",
       "      <td>3587.0</td>\n",
       "      <td>3555.0</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>3554.0</td>\n",
       "      <td>3584.0</td>\n",
       "      <td>3581.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             County      2010      2011      2012      2013      2014  \\\n",
       "0      Adams County  443691.0  452201.0  460558.0  469978.0  479946.0   \n",
       "1    Alamosa County   15515.0   15709.0   15680.0   15787.0   15803.0   \n",
       "2   Arapahoe County  574747.0  585968.0  596500.0  608467.0  619034.0   \n",
       "3  Archuleta County   12046.0   12021.0   12132.0   12216.0   12231.0   \n",
       "4       Baca County    3807.0    3778.0    3722.0    3656.0    3587.0   \n",
       "\n",
       "       2015      2016      2017      2018      2019 State  \n",
       "0  490443.0  497734.0  503590.0  511354.0  517421.0    CO  \n",
       "1   15894.0   16053.0   16108.0   16248.0   16233.0    CO  \n",
       "2  630984.0  638950.0  644478.0  651797.0  656590.0    CO  \n",
       "3   12387.0   12825.0   13295.0   13730.0   14029.0    CO  \n",
       "4    3555.0    3530.0    3554.0    3584.0    3581.0    CO  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pops10 = pd.DataFrame()\n",
    "\n",
    "# add every excel file in 00_source_data/county_pop/2000s to pops00\n",
    "\n",
    "path = r\"00_source_data/county_pop/2010s\" # point to correct folder\n",
    "filenames = glob.glob(path + \"/*.xlsx\")\n",
    "\n",
    "for f in filenames:\n",
    "\n",
    "    # read in current file with header = 3\n",
    "    temp = pd.read_excel(f, header = 3)\n",
    "\n",
    "    # regex to pull out state from filename\n",
    "    r = re.search(\"(2010s)(.)(\\w+)\", f)[3]\n",
    "    temp[\"State\"] = r[:2].upper()\n",
    "    \n",
    "    # drop null on any of the years\n",
    "    temp.dropna(subset=[2010], inplace=True)\n",
    "\n",
    "    #drop useless columns\n",
    "    temp.drop(columns={\"Census\", \"Estimates Base\"}, inplace=True)\n",
    "\n",
    "    # drop first row\n",
    "    temp = temp.iloc[1:, :]\n",
    "\n",
    "    # rename some cols\n",
    "    temp.rename(columns={\"Unnamed: 0\": \"County\"}, inplace=True)\n",
    "\n",
    "    # remove period at beginning of each county\n",
    "    temp[\"County\"] = temp[\"County\"].apply(lambda x: x[1:])\n",
    "\n",
    "    # strip state from county\n",
    "    temp[\"County\"] = temp[\"County\"].apply(lambda x: x.split(\", \")[0])\n",
    "\n",
    "    pops10 = pd.concat([pops10, temp], axis=0, ignore_index=True)\n",
    "\n",
    "# quick peek at the data\n",
    "pops10.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt both dfs to get tidy format\n",
    "pops00 = pops00.melt([\"County\", \"State\"])\n",
    "pops10 = pops10.melt([\"County\", \"State\"])\n",
    "\n",
    "# rename columns accordingly\n",
    "pops00.rename(columns={\"variable\": \"Year\", \"value\": \"Population\"}, inplace=True)\n",
    "pops10.rename(columns={\"variable\": \"Year\", \"value\": \"Population\"}, inplace=True)\n",
    "\n",
    "# concatenate the two dfs to get all our population data in one place\n",
    "pops = pd.concat([pops00, pops10], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we have the same number of counties between datasets\n",
    "assert len(pops00[\"County\"].unique()) == len(pops10[\"County\"].unique())\n",
    "\n",
    "# check that we have the same number of counties every year\n",
    "# first, create a df with the number of counties per year\n",
    "pops_county_check = pops.groupby([\"State\", \"Year\"])[\"County\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>State</th>\n",
       "      <th>county_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>CO</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>FL</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>IL</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>MA</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>MD</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year State  county_count\n",
       "0  2000    CO            64\n",
       "1  2000    FL            67\n",
       "2  2000    IL           102\n",
       "3  2000    MA            14\n",
       "4  2000    MD            24"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group the sum of counties by year and state - will help us check if number of counties changes over the years\n",
    "grouped_states = pops_county_check.groupby([\"Year\", \"State\"])[\"County\"].sum().reset_index().rename(columns={\"County\": \"county_count\"})\n",
    "\n",
    "# here's what this looks like\n",
    "# we get a dataframe of states and years, with the number of counties in each state in each year\n",
    "grouped_states.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the above query, we should be able to assert that the number of counties per year is the same\n",
    "# below statement should always equal zero\n",
    "\n",
    "assert (grouped_states.duplicated(subset=[\"Year\", \"State\"]).sum() == 0)\n",
    "#assert (grouped_states10.duplicated(subset=[\"Year\", \"State\"]).sum() == 0)\n",
    "\n",
    "\n",
    "# ensure no duplicate values\n",
    "assert pops.duplicated().sum() == 0\n",
    "\n",
    "# loop to check that every state has the same number of counties every year\n",
    "for state in states:\n",
    "    assert (pops[pops[\"State\"] == state].Year.value_counts().nunique() == 1), f\"error on {state}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trying to integrate fip numbers for a better merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in fips data from external source\n",
    "fips = pd.read_csv(\"https://github.com/ChuckConnell/articles/raw/master/fips2county.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AL']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to get key from value in our abbreviation dictionary\n",
    "# will help us have consistent formatting across dataframes for merging purposes\n",
    "def get_keys_from_value(d, val):\n",
    "    return [k for k, v in d.items() if v == val]\n",
    "\n",
    "\n",
    "keys = get_keys_from_value(abbrev_to_us_state, 'Alabama')\n",
    "keys # quick peek to make sure it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the above to entire fips dataframe\n",
    "fips[\"state_abbrev\"] = fips[\"StateName\"].apply(lambda x: get_keys_from_value(abbrev_to_us_state, x)[0])\n",
    "\n",
    "# filter fips to appropriate states, now that it's in the correct format\n",
    "fips = fips[fips[\"state_abbrev\"].isin(states)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further cleaning of values before merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get rid of the word county in pop df\n",
    "def remove_county(x):\n",
    "\n",
    "    if \"County\" in x:\n",
    "        return x[:-7]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "pops[\"county_test\"] = pops[\"County\"].apply(lambda x: remove_county(x))\n",
    "\n",
    "\n",
    "# fix dona ana and la salle parish\n",
    "pops[\"county_test\"] = pops[\"county_test\"].apply(lambda x: x.replace(\"Doña Ana\", \"Dona Ana\"))\n",
    "fips[\"CountyName\"] = fips[\"CountyName\"].apply(lambda x: x.replace(\"DoÃ±a Ana\", \"Dona Ana\"))\n",
    "\n",
    "\n",
    "#pops[\"county_test\"] = pops[\"county_test\"].apply(lambda x: x.replace(\"La Salle Parish\", \"La Salle\"))\n",
    "\n",
    "\n",
    "# rename county_test where state is texas and county is la salle to La Salle (TX)\n",
    "pops.loc[(pops[\"State\"] == \"TX\") & (pops[\"county_test\"] == \"La Salle\"), \"county_test\"] = \"La Salle County\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change La Salle county name in fips to La Salle County\n",
    "fips.loc[fips[\"CountyName\"] == \"La Salle\", \"CountyName\"] = \"La Salle County\"\n",
    "fips.loc[fips[\"CountyName\"] == \"LaSalle Parish\", \"CountyName\"] = \"La Salle Parish\"\n",
    "pops.loc[pops[\"county_test\"] == \"LaSalle Parish\", \"county_test\"] = \"La Salle Parish\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final merge for population dataset & fip number dataset\n",
    "pops_copy = pops.merge(fips[[\"state_abbrev\", \"CountyFIPS\", \"StateFIPS\", \"CountyName\"]], left_on=[\"county_test\", \"State\"], right_on=[\"CountyName\", \"state_abbrev\"], how=\"outer\", indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should never end up with anything left out of merge\n",
    "assert len(pops_copy[pops_copy[\"_merge\"] != \"both\"]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add fip numbers to df_prescriptions\n",
    "\n",
    "# create copies of both dfs so we have a checkpoint to access our old dfs\n",
    "prescriptions_copy = df_prescriptions.copy()\n",
    "fips_copy = fips.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix nulls in prescriptions at this point\n",
    "\n",
    "prescriptions_copy.dropna(subset=[\"BUYER_COUNTY\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make buyer_county all lowercase\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.lower())\n",
    "\n",
    "# do the same for fips\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove county and parish from fips_copy\n",
    "\n",
    "def remove_parish(x):\n",
    "\n",
    "    if \"parish\" in x:\n",
    "        return x[:-7]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "# prescription dataset has similar format - match fips to this format\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: remove_county(x))\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: remove_parish(x))\n",
    "\n",
    "def expand_saint(x):\n",
    "\n",
    "    if \"st.\" in x:\n",
    "        return x.replace(\"st.\", \"saint\")\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# fix various other inconsistencies\n",
    "# left only values first\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: expand_saint(x))\n",
    "\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"desoto\", \"de soto\"))\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"desoto\", \"de soto\"))\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"st john the baptist\", \"saint john the baptist\"))\n",
    "\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"dekalb\", \"de kalb\"))\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"dekalb\", \"de kalb\"))\n",
    "\n",
    "# fix right only values\n",
    "\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"desoto\", \"de soto\"))\n",
    "\n",
    "\n",
    "\n",
    "# function to remove apostrophes from county names\n",
    "def remove_apostrophe(x):\n",
    "    \n",
    "    if \"'\" in x:\n",
    "        return x.replace(\"'\", \"\")\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "# apply to fips\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: remove_apostrophe(x))\n",
    "\n",
    "# replace lasalle with la salle in fips copy\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"lasalle\", \"la salle\"))\n",
    "\n",
    "# replace dewitt with de witt in prescriptions copy\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"dewitt\", \"de witt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "prescriptions_fips = prescriptions_copy.merge(fips_copy, left_on=[\"BUYER_COUNTY\", \"BUYER_STATE\"], right_on=[\"CountyName\", \"state_abbrev\"], how=\"outer\", indicator=True)\n",
    "\n",
    "# capitalize year and month columns\n",
    "prescriptions_fips.rename(columns={\"year\": \"Year\", \"month\": \"Month\"}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling missing prescription shipment values\n",
    "\n",
    "Since we have plenty of values joined with right_only indicator status, we know that some counties in our FIPS dataset is not merging correctly to our prescriptions dataset. There are 376 rows where this occurs. After extensive data cleaning and checking of counties, we believe these values should be filled in with zero, as we can assume no prescriptions were shipped to these counties in their given years.\n",
    "\n",
    "\n",
    "## Analyzing the problem\n",
    "\n",
    "We have to main issues to overcome in terms of missing values\n",
    "\n",
    "1) We have 376 values not joining back to our main dataframe from the FIPS dataframe\n",
    "2) A handful of counties have shipments in some years, and no shipments in other years\n",
    "\n",
    "\n",
    "The solution to 2) will be touched on later in this notebook. As for 1), the solution is as follows:\n",
    "\n",
    "### Solution:\n",
    "\n",
    "We are assuming that the right_only joins (counties present in FIPS data but not prescription shipment data) implies that there were zero shipments for a given county throughout our entire range of years. Therefore, we cannot drop these values as we'd be ignoring counties that do not have any shipments and, therefore, would result in a biased analysis. To solve this issue, we will perform the following:\n",
    "\n",
    "1) create lists for various fields in our dataframe where we see this right_only join\n",
    "2) expand these lists out by a factor of 9 (because we are studying 9 years of interest)\n",
    "    - this is done through a list comprehension that multiplies each value by a range() object, therefore expanding our lists\n",
    "3) adding a vector of zeros for the MME column\n",
    "4) concatenating this data to our prescriptions dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BUYER_STATE</th>\n",
       "      <th>BUYER_COUNTY</th>\n",
       "      <th>Year</th>\n",
       "      <th>MME</th>\n",
       "      <th>StateFIPS</th>\n",
       "      <th>CountyFIPS_3</th>\n",
       "      <th>CountyName</th>\n",
       "      <th>StateName</th>\n",
       "      <th>CountyFIPS</th>\n",
       "      <th>StateAbbr</th>\n",
       "      <th>STATE_COUNTY</th>\n",
       "      <th>state_abbrev</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3918</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>costilla</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8023</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | COSTILLA</td>\n",
       "      <td>CO</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3919</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>dolores</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8033</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | DOLORES</td>\n",
       "      <td>CO</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3920</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>57</td>\n",
       "      <td>jackson</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8057</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | JACKSON</td>\n",
       "      <td>CO</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3921</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>adams</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>17001</td>\n",
       "      <td>IL</td>\n",
       "      <td>IL | ADAMS</td>\n",
       "      <td>IL</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>alexander</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>17003</td>\n",
       "      <td>IL</td>\n",
       "      <td>IL | ALEXANDER</td>\n",
       "      <td>IL</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4289</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>499</td>\n",
       "      <td>wood</td>\n",
       "      <td>Texas</td>\n",
       "      <td>48499</td>\n",
       "      <td>TX</td>\n",
       "      <td>TX | WOOD</td>\n",
       "      <td>TX</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4290</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>501</td>\n",
       "      <td>yoakum</td>\n",
       "      <td>Texas</td>\n",
       "      <td>48501</td>\n",
       "      <td>TX</td>\n",
       "      <td>TX | YOAKUM</td>\n",
       "      <td>TX</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4291</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>503</td>\n",
       "      <td>young</td>\n",
       "      <td>Texas</td>\n",
       "      <td>48503</td>\n",
       "      <td>TX</td>\n",
       "      <td>TX | YOUNG</td>\n",
       "      <td>TX</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4292</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>505</td>\n",
       "      <td>zapata</td>\n",
       "      <td>Texas</td>\n",
       "      <td>48505</td>\n",
       "      <td>TX</td>\n",
       "      <td>TX | ZAPATA</td>\n",
       "      <td>TX</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4293</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>507</td>\n",
       "      <td>zavala</td>\n",
       "      <td>Texas</td>\n",
       "      <td>48507</td>\n",
       "      <td>TX</td>\n",
       "      <td>TX | ZAVALA</td>\n",
       "      <td>TX</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>376 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BUYER_STATE BUYER_COUNTY  Year  MME  StateFIPS  CountyFIPS_3 CountyName  \\\n",
       "3918         NaN          NaN   NaN  NaN          8            23   costilla   \n",
       "3919         NaN          NaN   NaN  NaN          8            33    dolores   \n",
       "3920         NaN          NaN   NaN  NaN          8            57    jackson   \n",
       "3921         NaN          NaN   NaN  NaN         17             1      adams   \n",
       "3922         NaN          NaN   NaN  NaN         17             3  alexander   \n",
       "...          ...          ...   ...  ...        ...           ...        ...   \n",
       "4289         NaN          NaN   NaN  NaN         48           499       wood   \n",
       "4290         NaN          NaN   NaN  NaN         48           501     yoakum   \n",
       "4291         NaN          NaN   NaN  NaN         48           503      young   \n",
       "4292         NaN          NaN   NaN  NaN         48           505     zapata   \n",
       "4293         NaN          NaN   NaN  NaN         48           507     zavala   \n",
       "\n",
       "     StateName  CountyFIPS StateAbbr    STATE_COUNTY state_abbrev      _merge  \n",
       "3918  Colorado        8023        CO   CO | COSTILLA           CO  right_only  \n",
       "3919  Colorado        8033        CO    CO | DOLORES           CO  right_only  \n",
       "3920  Colorado        8057        CO    CO | JACKSON           CO  right_only  \n",
       "3921  Illinois       17001        IL      IL | ADAMS           IL  right_only  \n",
       "3922  Illinois       17003        IL  IL | ALEXANDER           IL  right_only  \n",
       "...        ...         ...       ...             ...          ...         ...  \n",
       "4289     Texas       48499        TX       TX | WOOD           TX  right_only  \n",
       "4290     Texas       48501        TX     TX | YOAKUM           TX  right_only  \n",
       "4291     Texas       48503        TX      TX | YOUNG           TX  right_only  \n",
       "4292     Texas       48505        TX     TX | ZAPATA           TX  right_only  \n",
       "4293     Texas       48507        TX     TX | ZAVALA           TX  right_only  \n",
       "\n",
       "[376 rows x 13 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick look at some of the right only merges\n",
    "prescriptions_fips[prescriptions_fips[\"_merge\"] != \"both\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df of all the nulls as stated above\n",
    "# can take year's null values because this column will have not joined back to the records we're interested in\n",
    "nans = prescriptions_fips[prescriptions_fips[\"Year\"].isnull()]\n",
    "\n",
    "# create lists that pull columns from each of these non-joined records\n",
    "missing_counties = nans.CountyName\n",
    "missing_states=nans.StateAbbr\n",
    "missing_fips = nans.CountyFIPS\n",
    "missing_state_fips = nans.StateFIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand each list out so there are 9 years for each record\n",
    "missing_counties = [item for item in missing_counties for i in range(9)]\n",
    "missing_states = [item for item in missing_states for i in range(9)]\n",
    "missing_fips = [item for item in missing_fips for i in range(9)]\n",
    "missing_state_fips = [item for item in missing_state_fips for i in range(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(missing_counties) == len(missing_states) == len(missing_fips) == len(missing_state_fips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of each of our lists: 3384\n"
     ]
    }
   ],
   "source": [
    "# create an iterable list of years\n",
    "# will be cycled through and added to a new list\n",
    "# this list should match one of each year (2006-2014) for each of our records in our lists above\n",
    "years = list(np.arange(2006, 2015, 1))\n",
    "\n",
    "print(f\"length of each of our lists: {len(missing_counties)}\")\n",
    "\n",
    "# list to be appended to our df\n",
    "years_for_df = []\n",
    "\n",
    "# 3384, as seen in print above, is the number of records we have in our lists\n",
    "# we want to divide by the number of years we want for each record, so we divide by 9\n",
    "# this results in 374 records for each year\n",
    "for year in range(int(3384/9)):\n",
    "    for subyear in years:\n",
    "        years_for_df.append(subyear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create first missing df, append values derived above\n",
    "missing_df = pd.DataFrame()\n",
    "\n",
    "missing_df[\"BUYER_COUNTY\"] = missing_counties\n",
    "missing_df[\"BUYER_STATE\"] = missing_states\n",
    "missing_df[\"CountyFIPS\"] = missing_fips\n",
    "missing_df[\"StateFIPS\"] = missing_state_fips\n",
    "missing_df[\"Year\"] = years_for_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(missing_df.Year.value_counts()) == 9, \"double check that we have 9 years for each record\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing our second issue\n",
    "\n",
    "Now, we want to tackle the problem where some counties have shipments in some years, but not others. As can be seen below, we are missing a few counties in each year. The good news on this front is that there shouldn't be very many records to fix, so we likely do not have to be robust to the possibility of duplicate county names across states (more concrete proof of this is further down in the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2006.0    437\n",
       "2007.0    436\n",
       "2010.0    436\n",
       "2009.0    435\n",
       "2011.0    435\n",
       "2012.0    435\n",
       "2013.0    435\n",
       "2014.0    435\n",
       "2008.0    434\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prescriptions_fips.Year.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving a few missing counties in specific years\n",
    "\n",
    "First, we'll make a list of every county below. We'll copy this list for every year in which we want data (2006-2014). Then, we'll check our prescription dataframe for records in which these counties **do not appear with an associated year**. Given that there are not too many missing values (as per value counts in cell above), we can manually check each value to make sure it is actually missing in our dataframe (and therefore, check that our logic is correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of unique counties\n",
    "counties = prescriptions_fips.BUYER_COUNTY.unique()\n",
    "\n",
    "# duplicate this list for every year we want\n",
    "counties2006 = counties.copy()\n",
    "counties2007 = counties.copy()\n",
    "counties2008 = counties.copy()\n",
    "counties2009 = counties.copy()\n",
    "counties2010 = counties.copy()\n",
    "counties2011 = counties.copy()\n",
    "counties2012 = counties.copy()\n",
    "counties2013 = counties.copy()\n",
    "counties2014 = counties.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a missing list for every year we want\n",
    "# iterate through each county in the list and check if it is in the dataframe for that year\n",
    "# admittedly, this is a suboptimal solution, but given the small amount of missing records it is workable\n",
    "\n",
    "missing06 = []\n",
    "for county in counties2006:\n",
    "    if county not in prescriptions_fips[(prescriptions_fips[\"Year\"] == 2006)][\"BUYER_COUNTY\"].unique():\n",
    "        missing06.append(county)\n",
    "\n",
    "missing07 = []\n",
    "for county in counties2007:\n",
    "    if county not in prescriptions_fips[(prescriptions_fips[\"Year\"] == 2007)][\"BUYER_COUNTY\"].unique():\n",
    "        missing07.append(county)\n",
    "\n",
    "missing08 = []\n",
    "for county in counties2008:\n",
    "    if county not in prescriptions_fips[(prescriptions_fips[\"Year\"] == 2008)][\"BUYER_COUNTY\"].unique():\n",
    "        missing08.append(county)\n",
    "\n",
    "missing09 = []\n",
    "for county in counties2009:\n",
    "    if county not in prescriptions_fips[(prescriptions_fips[\"Year\"] == 2009)][\"BUYER_COUNTY\"].unique():\n",
    "        missing09.append(county)\n",
    "\n",
    "missing10 = []\n",
    "for county in counties2010:\n",
    "    if county not in prescriptions_fips[(prescriptions_fips[\"Year\"] == 2010)][\"BUYER_COUNTY\"].unique():\n",
    "        missing10.append(county)\n",
    "\n",
    "missing11 = []\n",
    "for county in counties2011:\n",
    "    if county not in prescriptions_fips[(prescriptions_fips[\"Year\"] == 2011)][\"BUYER_COUNTY\"].unique():\n",
    "        missing11.append(county)\n",
    "\n",
    "missing12 = []\n",
    "for county in counties2012:\n",
    "    if county not in prescriptions_fips[(prescriptions_fips[\"Year\"] == 2012)][\"BUYER_COUNTY\"].unique():\n",
    "        missing12.append(county)\n",
    "\n",
    "missing13 = []\n",
    "for county in counties2013:\n",
    "    if county not in prescriptions_fips[(prescriptions_fips[\"Year\"] == 2013)][\"BUYER_COUNTY\"].unique():\n",
    "        missing13.append(county)\n",
    "\n",
    "missing14 = []\n",
    "for county in counties2014:\n",
    "    if county not in prescriptions_fips[(prescriptions_fips[\"Year\"] == 2014)][\"BUYER_COUNTY\"].unique():\n",
    "        missing14.append(county)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### double check that there are no duplicate counties across states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BUYER_STATE</th>\n",
       "      <th>BUYER_COUNTY</th>\n",
       "      <th>Year</th>\n",
       "      <th>MME</th>\n",
       "      <th>StateFIPS</th>\n",
       "      <th>CountyFIPS_3</th>\n",
       "      <th>CountyName</th>\n",
       "      <th>StateName</th>\n",
       "      <th>CountyFIPS</th>\n",
       "      <th>StateAbbr</th>\n",
       "      <th>STATE_COUNTY</th>\n",
       "      <th>state_abbrev</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>CO</td>\n",
       "      <td>custer</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>2.724300</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>custer</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8027</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | CUSTER</td>\n",
       "      <td>CO</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>CO</td>\n",
       "      <td>custer</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>3.027000</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>custer</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8027</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | CUSTER</td>\n",
       "      <td>CO</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>CO</td>\n",
       "      <td>custer</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0.605400</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>custer</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8027</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | CUSTER</td>\n",
       "      <td>CO</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>CO</td>\n",
       "      <td>custer</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>1.513500</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>custer</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8027</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | CUSTER</td>\n",
       "      <td>CO</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>CO</td>\n",
       "      <td>custer</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>3.027000</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>custer</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8027</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | CUSTER</td>\n",
       "      <td>CO</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>CO</td>\n",
       "      <td>custer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>3.329700</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>custer</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8027</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | CUSTER</td>\n",
       "      <td>CO</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>CO</td>\n",
       "      <td>custer</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>5.751300</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>custer</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8027</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | CUSTER</td>\n",
       "      <td>CO</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>CO</td>\n",
       "      <td>custer</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>4.017900</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>custer</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8027</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | CUSTER</td>\n",
       "      <td>CO</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>CO</td>\n",
       "      <td>mineral</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.302700</td>\n",
       "      <td>8</td>\n",
       "      <td>79</td>\n",
       "      <td>mineral</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8079</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | MINERAL</td>\n",
       "      <td>CO</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>FL</td>\n",
       "      <td>glades</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>2627.311200</td>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "      <td>glades</td>\n",
       "      <td>Florida</td>\n",
       "      <td>12043</td>\n",
       "      <td>FL</td>\n",
       "      <td>FL | GLADES</td>\n",
       "      <td>FL</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>FL</td>\n",
       "      <td>glades</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>6489.442050</td>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "      <td>glades</td>\n",
       "      <td>Florida</td>\n",
       "      <td>12043</td>\n",
       "      <td>FL</td>\n",
       "      <td>FL | GLADES</td>\n",
       "      <td>FL</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>FL</td>\n",
       "      <td>glades</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>4646.468100</td>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "      <td>glades</td>\n",
       "      <td>Florida</td>\n",
       "      <td>12043</td>\n",
       "      <td>FL</td>\n",
       "      <td>FL | GLADES</td>\n",
       "      <td>FL</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>FL</td>\n",
       "      <td>glades</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>1898.209500</td>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "      <td>glades</td>\n",
       "      <td>Florida</td>\n",
       "      <td>12043</td>\n",
       "      <td>FL</td>\n",
       "      <td>FL | GLADES</td>\n",
       "      <td>FL</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>FL</td>\n",
       "      <td>glades</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>922.494225</td>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "      <td>glades</td>\n",
       "      <td>Florida</td>\n",
       "      <td>12043</td>\n",
       "      <td>FL</td>\n",
       "      <td>FL | GLADES</td>\n",
       "      <td>FL</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>FL</td>\n",
       "      <td>glades</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>924.516600</td>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "      <td>glades</td>\n",
       "      <td>Florida</td>\n",
       "      <td>12043</td>\n",
       "      <td>FL</td>\n",
       "      <td>FL | GLADES</td>\n",
       "      <td>FL</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    BUYER_STATE BUYER_COUNTY    Year          MME  StateFIPS  CountyFIPS_3  \\\n",
       "117          CO       custer  2006.0     2.724300          8            27   \n",
       "118          CO       custer  2007.0     3.027000          8            27   \n",
       "119          CO       custer  2009.0     0.605400          8            27   \n",
       "120          CO       custer  2010.0     1.513500          8            27   \n",
       "121          CO       custer  2011.0     3.027000          8            27   \n",
       "122          CO       custer  2012.0     3.329700          8            27   \n",
       "123          CO       custer  2013.0     5.751300          8            27   \n",
       "124          CO       custer  2014.0     4.017900          8            27   \n",
       "332          CO      mineral  2010.0     0.302700          8            79   \n",
       "706          FL       glades  2009.0  2627.311200         12            43   \n",
       "707          FL       glades  2010.0  6489.442050         12            43   \n",
       "708          FL       glades  2011.0  4646.468100         12            43   \n",
       "709          FL       glades  2012.0  1898.209500         12            43   \n",
       "710          FL       glades  2013.0   922.494225         12            43   \n",
       "711          FL       glades  2014.0   924.516600         12            43   \n",
       "\n",
       "    CountyName StateName  CountyFIPS StateAbbr  STATE_COUNTY state_abbrev  \\\n",
       "117     custer  Colorado        8027        CO   CO | CUSTER           CO   \n",
       "118     custer  Colorado        8027        CO   CO | CUSTER           CO   \n",
       "119     custer  Colorado        8027        CO   CO | CUSTER           CO   \n",
       "120     custer  Colorado        8027        CO   CO | CUSTER           CO   \n",
       "121     custer  Colorado        8027        CO   CO | CUSTER           CO   \n",
       "122     custer  Colorado        8027        CO   CO | CUSTER           CO   \n",
       "123     custer  Colorado        8027        CO   CO | CUSTER           CO   \n",
       "124     custer  Colorado        8027        CO   CO | CUSTER           CO   \n",
       "332    mineral  Colorado        8079        CO  CO | MINERAL           CO   \n",
       "706     glades   Florida       12043        FL   FL | GLADES           FL   \n",
       "707     glades   Florida       12043        FL   FL | GLADES           FL   \n",
       "708     glades   Florida       12043        FL   FL | GLADES           FL   \n",
       "709     glades   Florida       12043        FL   FL | GLADES           FL   \n",
       "710     glades   Florida       12043        FL   FL | GLADES           FL   \n",
       "711     glades   Florida       12043        FL   FL | GLADES           FL   \n",
       "\n",
       "    _merge  \n",
       "117   both  \n",
       "118   both  \n",
       "119   both  \n",
       "120   both  \n",
       "121   both  \n",
       "122   both  \n",
       "123   both  \n",
       "124   both  \n",
       "332   both  \n",
       "706   both  \n",
       "707   both  \n",
       "708   both  \n",
       "709   both  \n",
       "710   both  \n",
       "711   both  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine all missing lists into one\n",
    "missing_total = missing06 + missing07 + missing08 + missing09 + missing10 + missing11 + missing12 + missing13 + missing14\n",
    "\n",
    "# check county values against this list\n",
    "prescriptions_fips[prescriptions_fips[\"BUYER_COUNTY\"].isin(set(missing_total))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is that **none of our missing counties have duplicate names across states**, so we can be sure our method for deriving these values is workable. If there was a Glades County in both Florida and Colorado, for example, the above method would be too ambiguous and we'd need to save the state variable in our missing lists as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of years the same dimension as missing total that has the appropriate year based on length of missing total\n",
    "# in simpler terms, this is a list of 2006s, 2007s, etc. that is the same length of its corresponding missing list\n",
    "missing_yrs06 = [2006 for item in missing06]\n",
    "missing_yrs07 = [2007 for item in missing07]\n",
    "missing_yrs08 = [2008 for item in missing08]\n",
    "missing_yrs09 = [2009 for item in missing09]\n",
    "missing_yrs10 = [2010 for item in missing10]\n",
    "missing_yrs11 = [2011 for item in missing11]\n",
    "missing_yrs12 = [2012 for item in missing12]\n",
    "missing_yrs13 = [2013 for item in missing13]\n",
    "missing_yrs14 = [2014 for item in missing14]\n",
    "\n",
    "# concat into one list\n",
    "missing_yrs = missing_yrs06 + missing_yrs07 + missing_yrs08 + missing_yrs09 + missing_yrs10 + missing_yrs11 + missing_yrs12 + missing_yrs13 + missing_yrs14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(missing_total) == len(missing_yrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create second missing df\n",
    "smaller_missing_df = pd.DataFrame()\n",
    "\n",
    "# add two columns calculated from lists above\n",
    "smaller_missing_df[\"BUYER_COUNTY\"] = missing_total\n",
    "smaller_missing_df[\"Year\"] = missing_yrs\n",
    "\n",
    "# save state of df for checking after merge\n",
    "smaller_df_check = smaller_missing_df.copy()\n",
    "\n",
    "# since we know there are no duplicate counties for these values, we will left join the state back to this df with confidence that the result is sound\n",
    "smaller_missing_df = pd.merge(smaller_missing_df, prescriptions_fips[[\"BUYER_COUNTY\", \"StateFIPS\", \"CountyFIPS\", \"BUYER_STATE\"]], how=\"left\", left_on=\"BUYER_COUNTY\", right_on=\"BUYER_COUNTY\")\n",
    "\n",
    "# drop duplicates to get rid of extra columns that appear as a result of join\n",
    "smaller_missing_df.drop_duplicates(subset={\"BUYER_COUNTY\", \"Year\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(smaller_missing_df) == len(smaller_df_check)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining these all back together\n",
    "\n",
    "First, we need to clean the datasets to make sure we have only the columns we need so they concatenate properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset prescription fips to only what we need\n",
    "prescriptions_fips = prescriptions_fips[[\"BUYER_STATE\", \"BUYER_COUNTY\", \"CountyFIPS\", \"StateFIPS\", \"Year\", \"MME\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorganize missing_df to match prescriptions_fips\n",
    "missing_df = missing_df[[\"BUYER_STATE\", \"BUYER_COUNTY\", \"CountyFIPS\", \"StateFIPS\", \"Year\"]]\n",
    "missing_df[\"MME\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorganize smaller missing df to match prescriptions_fips\n",
    "smaller_missing_df = smaller_missing_df[[\"BUYER_STATE\", \"BUYER_COUNTY\", \"CountyFIPS\", \"StateFIPS\", \"Year\"]]\n",
    "smaller_missing_df[\"MME\"] = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concatenate all three dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "prescriptions = pd.concat([prescriptions_fips, missing_df, smaller_missing_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BUYER_STATE</th>\n",
       "      <th>BUYER_COUNTY</th>\n",
       "      <th>CountyFIPS</th>\n",
       "      <th>StateFIPS</th>\n",
       "      <th>Year</th>\n",
       "      <th>MME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7678</th>\n",
       "      <td>CO</td>\n",
       "      <td>mineral</td>\n",
       "      <td>8079</td>\n",
       "      <td>8</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7680</th>\n",
       "      <td>CO</td>\n",
       "      <td>mineral</td>\n",
       "      <td>8079</td>\n",
       "      <td>8</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7683</th>\n",
       "      <td>CO</td>\n",
       "      <td>mineral</td>\n",
       "      <td>8079</td>\n",
       "      <td>8</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7685</th>\n",
       "      <td>CO</td>\n",
       "      <td>mineral</td>\n",
       "      <td>8079</td>\n",
       "      <td>8</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>CO</td>\n",
       "      <td>mineral</td>\n",
       "      <td>8079</td>\n",
       "      <td>8</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.3027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7686</th>\n",
       "      <td>CO</td>\n",
       "      <td>mineral</td>\n",
       "      <td>8079</td>\n",
       "      <td>8</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7687</th>\n",
       "      <td>CO</td>\n",
       "      <td>mineral</td>\n",
       "      <td>8079</td>\n",
       "      <td>8</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7688</th>\n",
       "      <td>CO</td>\n",
       "      <td>mineral</td>\n",
       "      <td>8079</td>\n",
       "      <td>8</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7689</th>\n",
       "      <td>CO</td>\n",
       "      <td>mineral</td>\n",
       "      <td>8079</td>\n",
       "      <td>8</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     BUYER_STATE BUYER_COUNTY  CountyFIPS  StateFIPS    Year     MME\n",
       "7678          CO      mineral        8079          8  2006.0  0.0000\n",
       "7680          CO      mineral        8079          8  2007.0  0.0000\n",
       "7683          CO      mineral        8079          8  2008.0  0.0000\n",
       "7685          CO      mineral        8079          8  2009.0  0.0000\n",
       "332           CO      mineral        8079          8  2010.0  0.3027\n",
       "7686          CO      mineral        8079          8  2011.0  0.0000\n",
       "7687          CO      mineral        8079          8  2012.0  0.0000\n",
       "7688          CO      mineral        8079          8  2013.0  0.0000\n",
       "7689          CO      mineral        8079          8  2014.0  0.0000"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how one of the values looks now\n",
    "prescriptions[prescriptions[\"BUYER_COUNTY\"] == \"mineral\"].sort_values(\"Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop leftover duplicates, if any\n",
    "prescriptions.drop_duplicates(subset={\"BUYER_STATE\",\"BUYER_COUNTY\", \"Year\"}, keep=\"first\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding fips to our cause of death data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copies of both dfs\n",
    "\n",
    "cause_of_death_copy = df_cause_of_death.copy()\n",
    "fips_copy = fips.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove county once again\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: remove_county(x))\n",
    "\n",
    "\n",
    "# clean some other miscellaneous values up\n",
    "\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"LaSalle Parish\", \"La Salle Parish\"))\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"DeBaca\", \"De Baca\"))\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"La Salle\", \"La Salle County\"))\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"La Salle County Parish\", \"La Salle Parish\"))\n",
    "\n",
    "\n",
    "\n",
    "# expand mckean to mc kean in fips_copy\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"McKean\", \"Mc Kean\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_of_death_fips = cause_of_death_copy.merge(fips_copy, left_on=[\"County\", \"State\"], right_on=[\"CountyName\", \"state_abbrev\"], how=\"outer\", indicator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not all counties joining back to cause of death dataset\n",
    "\n",
    "If the number of people in a given category (eg. one county/year/cause of death category) is less than 10, those records do not appear in this data. There is also a technicality in the number of total deaths vs. drug deaths (which we are interested in).\n",
    "\n",
    "The example we are given is that if a county has 20 deaths unrelated to drugs and alcohol, and only 7 related to alcohol, only the former figure will be reported. In the next notebook (pick_states.ipynb), we will filter by cause of death. In this notebook, since we still have all causes of death, we will impute for every missing value.\n",
    "\n",
    "To impute this data, we will fill in missing values with **a random integer from 0 to 9**. We thought of drawing from a normal distribution, but this implies negative values could be attained. We could take their absolute values to negate this effect, but then we are no longer drawing from a *true* normal distribution, so we chose to pick random values in our range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace null value with a random integer from 0 to 10 with a normal distribution\n",
    "def value_imputer(x):\n",
    "    if pd.isnull(x):\n",
    "        return random.randint(0, 9)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "cause_of_death_fips[\"Deaths\"] = cause_of_death_fips[\"Deaths\"].apply(lambda x: value_imputer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    2\n",
       "0    1\n",
       "8    1\n",
       "7    1\n",
       "9    1\n",
       "Name: Deaths, dtype: int64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick look at our new imputed data\n",
    "cause_of_death_fips[cause_of_death_fips[\"_merge\"] != \"both\"].Deaths.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Population to final DataFrames\n",
    "\n",
    "For pop_fips, cause_of_death_fips, and prescription_fips. Steps needed:\n",
    "\n",
    "- Create unique ID from county FIPS and state FIPS\n",
    "- Merge population dataset based on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_of_death_fips = cause_of_death_fips[cause_of_death_fips[\"_merge\"] == \"both\"]\n",
    "#pops_copy = pops_copy[cause_of_death_fips[\"_merge\"] == \"both\"]\n",
    "#prescriptions_fips = prescriptions_fips[prescriptions_fips[\"_merge\"] == \"both\"]\n",
    "\n",
    "\n",
    "# drop merge columns\n",
    "cause_of_death_fips.drop(columns=[\"_merge\"], inplace=True)\n",
    "prescriptions_fips.drop(columns=[\"_merge\"], inplace=True)\n",
    "pops_copy.drop(columns=[\"_merge\",], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique FIP from county and state fips\n",
    "\n",
    "cause_of_death_fips[\"FIP_unique\"] = cause_of_death_fips[\"CountyFIPS\"].apply(lambda x: str(x)) + cause_of_death_fips[\"StateFIPS\"].apply(lambda x: str(x))\n",
    "prescriptions_fips[\"FIP_unique\"] = prescriptions_fips[\"CountyFIPS\"].apply(lambda x: str(x)) + prescriptions_fips[\"StateFIPS\"].apply(lambda x: str(x))\n",
    "pops_copy[\"FIP_unique\"] = pops_copy[\"CountyFIPS\"].apply(lambda x: str(x)) + pops_copy[\"StateFIPS\"].apply(lambda x: str(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add some sort of assert here. not sure what it should be yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final prescriptions dataset with populations\n",
    "# can safely left join here, because we only need records in the prescriptions dataset\n",
    "prescriptions = prescriptions_fips.merge(pops_copy, on=[\"FIP_unique\", \"Year\"], how=\"left\", indicator=True)\n",
    "\n",
    "#assert (prescriptions[\"_merge\"] == \"both\").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2006.0    327\n",
       "2007.0    325\n",
       "2008.0    324\n",
       "2009.0    321\n",
       "2010.0    317\n",
       "2011.0    315\n",
       "2012.0    311\n",
       "2013.0    309\n",
       "2014.0    305\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STILL HAVE WASHINGTON YEARS HERE\n",
    "prescriptions[prescriptions[\"BUYER_STATE\"] == \"WA\"].Year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one more assert to check length\n",
    "assert len(prescriptions) == len(prescriptions_fips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some useless columns\n",
    "prescriptions.drop(columns=[\"_merge\", \"CountyName_y\", \"StateFIPS_y\", \"CountyFIPS_y\",\"state_abbrev_y\", \"County\", \"CountyFIPS_3\"], inplace=True)\n",
    "\n",
    "# rename x columns\n",
    "prescriptions.rename(columns={\"CountyName_x\": \"CountyName\", \"StateFIPS_x\": \"StateFIPS\", \"CountyFIPS_x\": \"CountyFIPS\", \"state_abbrev_x\": \"state_abbrev\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final cause of death dataset with populations\n",
    "# can safely left join here, because we only need records in the cause of death dataset\n",
    "cause_of_death = cause_of_death_fips.merge(pops_copy, on=[\"FIP_unique\", \"Year\"], how=\"left\", indicator=True)\n",
    "\n",
    "assert cause_of_death_fips.Deaths.isnull().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some useless columns\n",
    "cause_of_death.drop(columns=[\"_merge\", \"CountyName_y\", \"StateFIPS_y\", \"CountyFIPS_y\",\"state_abbrev_y\", \"County_y\", \"CountyFIPS_3\", \"State_y\"], inplace=True)\n",
    "\n",
    "# rename x columns\n",
    "cause_of_death.rename(columns={\"County_x\": \"County\", \"Year_x\": \"Year\", \"State_x\": \"State\", \"StateFIPS_x\": \"StateFIPS\", \"CountyFIPS_x\": \"CountyFIPS\", \"state_abbrev_x\": \"state_abbrev\", \"CountyName_x\": \"CountyName\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6584/3788078211.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_cause_of_death\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause_of_death\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_prescriptions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprescriptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# asserts to make sure we didn't lose any records from our original datasets\n",
    "\n",
    "assert len(df_cause_of_death) == len(cause_of_death)\n",
    "assert len(df_prescriptions) == len(prescriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export main, unjoined datasets in case we need them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9376/111128345.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcause_of_death\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"20_intermediate_files/cause_of_death_clean.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprescriptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"20_intermediate_files/arcos_all_washpost_clean.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3718\u001b[0m         )\n\u001b[0;32m   3719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3720\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3721\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3722\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         )\n\u001b[1;32m-> 1189\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m             )\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[0mix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format_native_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m         libwriters.write_csv_rows(\n\u001b[0m\u001b[0;32m    316\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m             \u001b[0mix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cause_of_death.to_csv(\"20_intermediate_files/cause_of_death_clean.csv\", index=False)\n",
    "prescriptions.to_csv(\"20_intermediate_files/arcos_all_washpost_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final 3 datasets\n",
    "\n",
    "We should have: (UNSURE IF WE SHOULD EXTEND DATE RANGES, CURRENTLY 3 YEARS BEFORE AND AFTER POLICY IMPLEMENTATION)\n",
    "\n",
    "- Florida and Georgia 2007 - 2013\n",
    "- Texas and Oklahoma 2004 - 2010\n",
    "- Washington and Oregon 2009 - 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug overdose - broken down by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Florida and Georgia\n",
    "\n",
    "prescriptions_fl = prescriptions.copy()\n",
    "prescriptions_wa = prescriptions.copy()\n",
    "\n",
    "prescriptions_fl = prescriptions_fl[(prescriptions_fl[\"BUYER_STATE\"] == \"FL\") | (prescriptions_fl[\"BUYER_STATE\"].isin(fl_states))]\n",
    "prescriptions_wa = prescriptions_wa[(prescriptions_wa[\"BUYER_STATE\"] == \"WA\") | (prescriptions_wa[\"BUYER_STATE\"]).isin(wa_states)]\n",
    "\n",
    "\n",
    "\n",
    "# filter appropriate years\n",
    "fl_start = 2007\n",
    "fl_end = 2013\n",
    "\n",
    "# tx will only be used for overdose deaths\n",
    "tx_start = 2004\n",
    "tx_end = 2010\n",
    "\n",
    "wa_start = 2009\n",
    "wa_end = 2015\n",
    "\n",
    "\n",
    "prescriptions_fl = prescriptions_fl[(prescriptions_fl[\"Year\"] >= fl_start) & (prescriptions_fl[\"Year\"] <= fl_end)]\n",
    "prescriptions_wa = prescriptions_wa[(prescriptions_wa[\"Year\"] >= wa_start) & (prescriptions_wa[\"Year\"] <= wa_end)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cause of death - broken down by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_fl = cause_of_death.copy()\n",
    "deaths_tx = cause_of_death.copy()\n",
    "deaths_wa = cause_of_death.copy()\n",
    "\n",
    "deaths_fl = deaths_fl[(deaths_fl[\"StateName\"] == \"Florida\") | (deaths_fl[\"State\"].isin(fl_states))]\n",
    "deaths_tx = deaths_tx[(deaths_tx[\"StateName\"] == \"Texas\") | (deaths_tx[\"State\"].isin(tx_states))]\n",
    "deaths_wa = deaths_wa[(deaths_wa[\"StateName\"] == \"Washington\") | (deaths_wa[\"State\"].isin(wa_states))]\n",
    "\n",
    "deaths_fl = deaths_fl[(deaths_fl[\"Year\"] >= fl_start) & (deaths_fl[\"Year\"] <= fl_end)]\n",
    "deaths_tx = deaths_tx[(deaths_tx[\"Year\"] >= tx_start) & (deaths_tx[\"Year\"] <= tx_end)]  \n",
    "deaths_wa = deaths_wa[(deaths_wa[\"Year\"] >= wa_start) & (deaths_wa[\"Year\"] <= wa_end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2013.0    2681681\n",
       "2011.0    2656850\n",
       "2012.0    2654508\n",
       "2010.0    2562310\n",
       "2014.0    2433600\n",
       "2009.0    2419344\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prescriptions_wa.Year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_52032/2020115255.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mprescriptions_fl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfl_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfl_end\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mprescriptions_wa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwa_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwa_end\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mdeaths_fl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfl_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfl_end\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# final assert to check years\n",
    "\n",
    "assert prescriptions_fl.Year.unique().tolist() == list(range(fl_start, fl_end + 1))\n",
    "assert prescriptions_wa.Year.unique().tolist() == list(range(wa_start, wa_end + 1))\n",
    "\n",
    "assert deaths_fl.Year.unique().tolist() == list(range(fl_start, fl_end + 1))\n",
    "assert deaths_tx.Year.unique().tolist() == list(range(tx_start, tx_end + 1))\n",
    "assert deaths_wa.Year.unique().tolist() == list(range(wa_start, wa_end + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export all to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prescriptions_fl.to_csv(\"20_intermediate_files/prescriptions_fl.csv\", index=False)\n",
    "#prescriptions_wa.to_csv(\"20_intermediate_files/prescriptions_wa.csv\", index=False)\n",
    "\n",
    "deaths_fl.to_csv(\"20_intermediate_files/deaths_fl.csv\", index=False)\n",
    "deaths_tx.to_csv(\"20_intermediate_files/deaths_tx.csv\", index=False)\n",
    "deaths_wa.to_csv(\"20_intermediate_files/deaths_wa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all as parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prescriptions_fl.to_parquet('20_intermediate_files/prescriptions_fl.parquet', engine='fastparquet', row_group_offsets=10_000_000)\n",
    "prescriptions_wa.to_parquet('20_intermediate_files/prescriptions_wa.parquet', engine='fastparquet', row_group_offsets=10_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for the group\n",
    "\n",
    "- may need to filter out a couple more columns - haven't done this yet as I don't want to accidentally delete something we need\n",
    "- overdose data is only broken down by year unless i messed something up - overdose analysis will have to be less granular"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49aa7209ac0e1a36a89cb04290394fd089cb5ce56cb44c9d4652c0180c6152a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
