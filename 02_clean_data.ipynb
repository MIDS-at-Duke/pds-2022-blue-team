{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import xlrd\n",
    "import re\n",
    "import openpyxl\n",
    "import random\n",
    "\n",
    "# custom file that maps state names to abbreviations\n",
    "from abbreviation_conversion import abbrev_to_us_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up prescription data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing the find_year function: 2019\n",
      "testing the find_month function: 12\n"
     ]
    }
   ],
   "source": [
    "def find_year(TRANSACTION_DATE):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        TRANSACTION_DATE (str): date in format MMDDYYYY\n",
    "\n",
    "    Returns:\n",
    "        int: year\n",
    "    \"\"\"\n",
    "    TRANSACTION_DATE = str(TRANSACTION_DATE)\n",
    "    \n",
    "    return int(TRANSACTION_DATE[-4:])\n",
    "\n",
    "# quick test \n",
    "print(f\"testing the find_year function: {find_year(12202019)}\")\n",
    "\n",
    "\n",
    "def find_month(TRANSACTION_DATE):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        TRANSACTION_DATE (str): date in format MMDDYYYY\n",
    "\n",
    "    Returns:\n",
    "        int: month\n",
    "    \"\"\"\n",
    "    TRANSACTION_DATE = str(TRANSACTION_DATE)\n",
    "\n",
    "    if len(TRANSACTION_DATE) == 8:\n",
    "        return int(TRANSACTION_DATE[:2])\n",
    "    else:\n",
    "        return int(TRANSACTION_DATE[:1])\n",
    "    \n",
    "\n",
    "# quick test \n",
    "print(f\"testing the find_month function: {find_month(12202019)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load in the data, we need to truncate the amount of columns we use as well as the states\n",
    "cols_to_keep = [\"REPORTER_DEA_NO\", \"BUYER_STATE\", \"BUYER_ZIP\", \"BUYER_COUNTY\", \"DRUG_CODE\", \"TRANSACTION_CODE\", \"DRUG_NAME\", \"QUANTITY\", \"TRANSACTION_DATE\", \"Product_Name\"]\n",
    "\n",
    "# we know we need Florida, Texas, and Washington\n",
    "states = [\"FL\", \"TX\", \"WA\"]\n",
    "# since we are normalizing based on population, I think we should pick states that are regionally close to our target states\n",
    "# we can change this later as a group, but I have these selected below:\n",
    "\n",
    "# Florida comparison states\n",
    "fl_states = [\"PA\", \"MI\", \"NC\"]\n",
    "\n",
    "# Texas comparison states\n",
    "tx_states = [\"IL\", \"MA\", \"MI\"]\n",
    "\n",
    "# Washington comparison states\n",
    "wa_states = [\"NC\", \"CO\", \"MD\"]\n",
    "\n",
    "# create list of all states to use\n",
    "variable_states = []\n",
    "variable_states.extend(fl_states)\n",
    "variable_states.extend(tx_states)\n",
    "variable_states.extend(wa_states)\n",
    "\n",
    "# append variable states to our original list\n",
    "states.extend(variable_states)\n",
    "\n",
    "\n",
    "# create separate list of only florida and washington states for prescription data\n",
    "prescription_states = [state for state in states if state not in [\"IL\", \"MA\", \"MI\", \"TX\"]]\n",
    "\n",
    "# NC is appearing twice as it's a comparison state for both target states\n",
    "# making this a set will remove the duplicate\n",
    "prescription_states = list(set(prescription_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_52032/218960981.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(filtered_chunk)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REPORTER_DEA_NO</th>\n",
       "      <th>BUYER_STATE</th>\n",
       "      <th>BUYER_ZIP</th>\n",
       "      <th>BUYER_COUNTY</th>\n",
       "      <th>TRANSACTION_CODE</th>\n",
       "      <th>DRUG_CODE</th>\n",
       "      <th>DRUG_NAME</th>\n",
       "      <th>QUANTITY</th>\n",
       "      <th>TRANSACTION_DATE</th>\n",
       "      <th>Product_Name</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PA0021179</td>\n",
       "      <td>CO</td>\n",
       "      <td>81005</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>S</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1122009</td>\n",
       "      <td>HYDROCODONE BITARTRATE &amp; ACETA  5MG/</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4745</th>\n",
       "      <td>PB0034861</td>\n",
       "      <td>FL</td>\n",
       "      <td>33460</td>\n",
       "      <td>PALM BEACH</td>\n",
       "      <td>S</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8182006</td>\n",
       "      <td>OXYCODONE HCL 40MG TABS</td>\n",
       "      <td>2006</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4746</th>\n",
       "      <td>PB0034861</td>\n",
       "      <td>FL</td>\n",
       "      <td>33460</td>\n",
       "      <td>PALM BEACH</td>\n",
       "      <td>S</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11292006</td>\n",
       "      <td>ENDOCET - 10MG OXYCODONE.HCL/325MG A</td>\n",
       "      <td>2006</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>PB0034861</td>\n",
       "      <td>FL</td>\n",
       "      <td>33460</td>\n",
       "      <td>PALM BEACH</td>\n",
       "      <td>S</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2062007</td>\n",
       "      <td>OXYCODONE HCL 80MG TABS</td>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>PB0034861</td>\n",
       "      <td>FL</td>\n",
       "      <td>33460</td>\n",
       "      <td>PALM BEACH</td>\n",
       "      <td>S</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3012007</td>\n",
       "      <td>OXYCODONE HCL 80MG TABS</td>\n",
       "      <td>2007</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494070</th>\n",
       "      <td>RC0231148</td>\n",
       "      <td>FL</td>\n",
       "      <td>32832</td>\n",
       "      <td>ORANGE</td>\n",
       "      <td>S</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7122007</td>\n",
       "      <td>HYDROCODONE BIT./ACET.,10MG &amp; 325MG/</td>\n",
       "      <td>2007</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494206</th>\n",
       "      <td>RC0231148</td>\n",
       "      <td>WA</td>\n",
       "      <td>99328</td>\n",
       "      <td>COLUMBIA</td>\n",
       "      <td>S</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>80.0</td>\n",
       "      <td>10032006</td>\n",
       "      <td>HYDROCODONE BITARTRATE/APAP 10MG/650</td>\n",
       "      <td>2006</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494207</th>\n",
       "      <td>RC0231148</td>\n",
       "      <td>WA</td>\n",
       "      <td>99328</td>\n",
       "      <td>COLUMBIA</td>\n",
       "      <td>S</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12132006</td>\n",
       "      <td>HYDROCODONE BITARTRATE/APAP 5MG/500M</td>\n",
       "      <td>2006</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494208</th>\n",
       "      <td>RC0231148</td>\n",
       "      <td>WA</td>\n",
       "      <td>99328</td>\n",
       "      <td>COLUMBIA</td>\n",
       "      <td>S</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>82.0</td>\n",
       "      <td>3112008</td>\n",
       "      <td>HYDROCODONE BITARTRATE/APAP 10MG/650</td>\n",
       "      <td>2008</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494211</th>\n",
       "      <td>RC0231148</td>\n",
       "      <td>CO</td>\n",
       "      <td>80222</td>\n",
       "      <td>DENVER</td>\n",
       "      <td>S</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9062011</td>\n",
       "      <td>HYDROCODONE BITARTRATE/APAP 5MG/500M</td>\n",
       "      <td>2011</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104968 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       REPORTER_DEA_NO BUYER_STATE  BUYER_ZIP BUYER_COUNTY TRANSACTION_CODE  \\\n",
       "18           PA0021179          CO      81005       PUEBLO                S   \n",
       "4745         PB0034861          FL      33460   PALM BEACH                S   \n",
       "4746         PB0034861          FL      33460   PALM BEACH                S   \n",
       "4747         PB0034861          FL      33460   PALM BEACH                S   \n",
       "4748         PB0034861          FL      33460   PALM BEACH                S   \n",
       "...                ...         ...        ...          ...              ...   \n",
       "494070       RC0231148          FL      32832       ORANGE                S   \n",
       "494206       RC0231148          WA      99328     COLUMBIA                S   \n",
       "494207       RC0231148          WA      99328     COLUMBIA                S   \n",
       "494208       RC0231148          WA      99328     COLUMBIA                S   \n",
       "494211       RC0231148          CO      80222       DENVER                S   \n",
       "\n",
       "        DRUG_CODE    DRUG_NAME  QUANTITY  TRANSACTION_DATE  \\\n",
       "18           9193  HYDROCODONE      10.0           1122009   \n",
       "4745         9143    OXYCODONE       1.0           8182006   \n",
       "4746         9143    OXYCODONE       8.0          11292006   \n",
       "4747         9143    OXYCODONE       3.0           2062007   \n",
       "4748         9143    OXYCODONE       3.0           3012007   \n",
       "...           ...          ...       ...               ...   \n",
       "494070       9193  HYDROCODONE       7.0           7122007   \n",
       "494206       9193  HYDROCODONE      80.0          10032006   \n",
       "494207       9193  HYDROCODONE      20.0          12132006   \n",
       "494208       9193  HYDROCODONE      82.0           3112008   \n",
       "494211       9193  HYDROCODONE       6.0           9062011   \n",
       "\n",
       "                                Product_Name  year  month  \n",
       "18      HYDROCODONE BITARTRATE & ACETA  5MG/  2009      1  \n",
       "4745                 OXYCODONE HCL 40MG TABS  2006      8  \n",
       "4746    ENDOCET - 10MG OXYCODONE.HCL/325MG A  2006     11  \n",
       "4747                 OXYCODONE HCL 80MG TABS  2007      2  \n",
       "4748                 OXYCODONE HCL 80MG TABS  2007      3  \n",
       "...                                      ...   ...    ...  \n",
       "494070  HYDROCODONE BIT./ACET.,10MG & 325MG/  2007      7  \n",
       "494206  HYDROCODONE BITARTRATE/APAP 10MG/650  2006     10  \n",
       "494207  HYDROCODONE BITARTRATE/APAP 5MG/500M  2006     12  \n",
       "494208  HYDROCODONE BITARTRATE/APAP 10MG/650  2008      3  \n",
       "494211  HYDROCODONE BITARTRATE/APAP 5MG/500M  2011      9  \n",
       "\n",
       "[104968 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, load in our data as an iterator so we can load in chunks\n",
    "it = pd.read_csv(\"00_source_data/arcos_all_washpost.tsv\", chunksize=500_000, sep='\\t', usecols = cols_to_keep) # may have to change chunksize depending on your computer's memory\n",
    "\n",
    "# init empty dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for chunk in it:\n",
    "    # extract year out of date column\n",
    "    chunk[\"year\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: find_year(x))\n",
    "    chunk[\"month\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: find_month(x))\n",
    "\n",
    "    # ensure we're working in the correct date range\n",
    "    filtered_chunk = chunk[chunk[\"year\"] > 2002]\n",
    "    filtered_chunk = filtered_chunk[filtered_chunk[\"year\"] < 2016]\n",
    "\n",
    "    # filter out the states we want\n",
    "    filtered_chunk = filtered_chunk[filtered_chunk[\"BUYER_STATE\"].isin(prescription_states)]\n",
    "\n",
    "    df = df.append(filtered_chunk)\n",
    "    break\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in individual state prescription data\n",
    "\n",
    "The Washington Post article associated with our data states that data from 2013 and 2014 was only recently added. Resultingly, we found that it was missing from the large dataset of all states. However, upon further digging, we found that these years were present **on an individual state level**, so we will load these in and concatenate them with our larger dataframe above.\n",
    "\n",
    "Without chunking, the below takes 30 seconds for each file to load in. With chunking, this is reduced to about 4 seconds per record, so please make sure to leave this in its current format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'00_source_data/state_prescriptions' # point to correct folder\n",
    "filenames = glob.glob(path + \"/*.csv\") # select all text files in folder\n",
    "\n",
    "assert len(filenames) == 7, \"There should be 7 files in the folder\"\n",
    "\n",
    "missing_years = pd.DataFrame() # empty df - will store data from all txt files\n",
    "\n",
    "for f in filenames:\n",
    "\n",
    "    it = pd.read_csv(f, chunksize=500_000, usecols = cols_to_keep) # may have to change chunksize depending on your computer's memory\n",
    "    temp_df = pd.DataFrame()\n",
    "\n",
    "    for chunk in it:\n",
    "        # extract year out of date column\n",
    "        chunk[\"year\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: find_year(x))\n",
    "        chunk[\"month\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: find_month(x))\n",
    "\n",
    "        # ensure we're working in the correct date range\n",
    "        filtered_chunk = chunk[chunk[\"year\"] > 2012]\n",
    "        filtered_chunk = filtered_chunk[filtered_chunk[\"year\"] < 2015]\n",
    "\n",
    "        temp_df = pd.concat([temp_df, filtered_chunk])\n",
    "        break\n",
    "    \n",
    "    missing_years = pd.concat([missing_years, temp_df], axis=0, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel like there should be an assert here but I'm not sure what it would be\n",
    "df_prescriptions = pd.concat([df, missing_years], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While doing analysis, we learned that a handful of county values in MA were missing. However, when we looked up the associated zip codes (02401, 02174), we learned that we could fill these values with Plymouth and Middlesex Counties, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prescriptions[\"date_test\"] = df_prescriptions[\"TRANSACTION_DATE\"].apply(lambda x: datetime.strptime(str(x), \"%m%d%Y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prescriptions[\"year_test\"] = df_prescriptions[\"TRANSACTION_DATE\"].apply(lambda x: find_year(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          1122009\n",
       "1          8182006\n",
       "2         11292006\n",
       "3          2062007\n",
       "4          3012007\n",
       "            ...   \n",
       "924654     2092013\n",
       "924655     3132013\n",
       "924656     3022013\n",
       "924657     4032013\n",
       "924658     2182013\n",
       "Name: TRANSACTION_DATE, Length: 924659, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prescriptions[\"TRANSACTION_DATE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2006, 2007, 2012, 2008, 2010, 2011, 2009, 2014, 2013], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prescriptions[df_prescriptions[\"BUYER_STATE\"] == \"WA\"].year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REPORTER_DEA_NO</th>\n",
       "      <th>BUYER_STATE</th>\n",
       "      <th>BUYER_ZIP</th>\n",
       "      <th>BUYER_COUNTY</th>\n",
       "      <th>TRANSACTION_CODE</th>\n",
       "      <th>DRUG_CODE</th>\n",
       "      <th>DRUG_NAME</th>\n",
       "      <th>QUANTITY</th>\n",
       "      <th>TRANSACTION_DATE</th>\n",
       "      <th>Product_Name</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>date_test</th>\n",
       "      <th>year_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [REPORTER_DEA_NO, BUYER_STATE, BUYER_ZIP, BUYER_COUNTY, TRANSACTION_CODE, DRUG_CODE, DRUG_NAME, QUANTITY, TRANSACTION_DATE, Product_Name, year, month, date_test, year_test]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick look at the null values (we checked, and these are all the values for which county is null)\n",
    "df_prescriptions[df_prescriptions[\"BUYER_COUNTY\"].isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace values accordingly\n",
    "df_prescriptions.loc[df_prescriptions[\"BUYER_ZIP\"] == 2401, \"BUYER_COUNTY\"] = \"PLYMOUTH\"\n",
    "df_prescriptions.loc[df_prescriptions[\"BUYER_ZIP\"] == 2174, \"BUYER_COUNTY\"] = \"MIDDLESEX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null values again\n",
    "df_prescriptions[df_prescriptions[\"BUYER_COUNTY\"].isnull()]\n",
    "\n",
    "assert len(df_prescriptions[df_prescriptions[\"BUYER_COUNTY\"].isnull()]) == 0, \"still have missing values for counties - double check code above\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up cause of death data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'00_source_data/cause_of_death' # point to correct folder\n",
    "filenames = glob.glob(path + \"/*.txt\") # select all text files in folder\n",
    "\n",
    "df = pd.DataFrame() # empty df - will store data from all txt files\n",
    "\n",
    "for f in filenames:\n",
    "    temp = pd.read_csv(f, index_col=None, header=0, sep='\\t')\n",
    "    # we're getting some extraneous notes at the bottom - let's just drop based on county as these will only be null for these useless notes columns\n",
    "    temp.dropna(subset={'County'}, inplace=True)\n",
    "    \n",
    "    df = pd.concat([df, temp], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to separate county and state\n",
    "\n",
    "def abtract_state(county):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        county (str): county name\n",
    "\n",
    "    Returns:\n",
    "        str: state\n",
    "    \"\"\"\n",
    "    return county.split(\", \")[1]\n",
    "\n",
    "\n",
    "\n",
    "def abstract_county(county):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        county (str): county name\n",
    "\n",
    "    Returns:\n",
    "        str: county\n",
    "    \"\"\"\n",
    "    return county.split(\", \")[0]\n",
    "\n",
    "# apply functions to our df\n",
    "df[\"State\"] = df.apply(lambda x: abtract_state(x[\"County\"]), axis=1)\n",
    "df[\"County\"] = df.apply(lambda x: abstract_county(x[\"County\"]), axis=1)\n",
    "\n",
    "# do not need notes column, let's just drop it here\n",
    "df.drop(columns={\"Notes\"}, inplace=True)\n",
    "\n",
    "df_cause_of_death = df.copy() # keep a copy of this df for later filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's filter our dataframe to be only the states we want\n",
    "df_cause_of_death = df_cause_of_death[df_cause_of_death[\"State\"].isin(states)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding in County Population data\n",
    "\n",
    "[Census county pop. data, 2000-2010](https://www.census.gov/data/tables/time-series/demo/popest/intercensal-2000-2010-counties.html)<br>\n",
    "[Census county pop. data, 2010-2019](https://www.census.gov/data/datasets/time-series/demo/popest/2010s-counties-total.html)<br>\n",
    "For both, just select the appropriate states on the webpage. We will clean and merge as needed in this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guide to cleaning - 2000s data\n",
    "\n",
    "The way the 2000s excel files are formatted, we can clean the data in the following way\n",
    "\n",
    "- load in with header=3\n",
    "- drop null on any of the populations\n",
    "    - notes at the bottom will be removed\n",
    "- drop unnamed 1, 12, and 13\n",
    "    - these contain redundant data about populations from specific dates\n",
    "    - Unnamed 12 is 2010s pop - will be redundant as our next dataset has this as well. Using the newer data\n",
    "- drop first row\n",
    "    - state as a whole\n",
    "- rename Unnamed: 0 to county\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams County</td>\n",
       "      <td>350888.0</td>\n",
       "      <td>359816.0</td>\n",
       "      <td>370753.0</td>\n",
       "      <td>377464.0</td>\n",
       "      <td>384809.0</td>\n",
       "      <td>395146.0</td>\n",
       "      <td>406575.0</td>\n",
       "      <td>415746.0</td>\n",
       "      <td>424913.0</td>\n",
       "      <td>435700.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alamosa County</td>\n",
       "      <td>14954.0</td>\n",
       "      <td>14956.0</td>\n",
       "      <td>15114.0</td>\n",
       "      <td>15067.0</td>\n",
       "      <td>15217.0</td>\n",
       "      <td>15236.0</td>\n",
       "      <td>15196.0</td>\n",
       "      <td>15180.0</td>\n",
       "      <td>15300.0</td>\n",
       "      <td>15289.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arapahoe County</td>\n",
       "      <td>491482.0</td>\n",
       "      <td>502393.0</td>\n",
       "      <td>508936.0</td>\n",
       "      <td>513690.0</td>\n",
       "      <td>518971.0</td>\n",
       "      <td>524466.0</td>\n",
       "      <td>531619.0</td>\n",
       "      <td>542039.0</td>\n",
       "      <td>552461.0</td>\n",
       "      <td>563161.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Archuleta County</td>\n",
       "      <td>10020.0</td>\n",
       "      <td>10454.0</td>\n",
       "      <td>10885.0</td>\n",
       "      <td>11089.0</td>\n",
       "      <td>11266.0</td>\n",
       "      <td>11496.0</td>\n",
       "      <td>11937.0</td>\n",
       "      <td>12262.0</td>\n",
       "      <td>12250.0</td>\n",
       "      <td>12169.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baca County</td>\n",
       "      <td>4501.0</td>\n",
       "      <td>4471.0</td>\n",
       "      <td>4336.0</td>\n",
       "      <td>4117.0</td>\n",
       "      <td>4064.0</td>\n",
       "      <td>3997.0</td>\n",
       "      <td>3933.0</td>\n",
       "      <td>3866.0</td>\n",
       "      <td>3806.0</td>\n",
       "      <td>3767.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             County      2000      2001      2002      2003      2004  \\\n",
       "0      Adams County  350888.0  359816.0  370753.0  377464.0  384809.0   \n",
       "1    Alamosa County   14954.0   14956.0   15114.0   15067.0   15217.0   \n",
       "2   Arapahoe County  491482.0  502393.0  508936.0  513690.0  518971.0   \n",
       "3  Archuleta County   10020.0   10454.0   10885.0   11089.0   11266.0   \n",
       "4       Baca County    4501.0    4471.0    4336.0    4117.0    4064.0   \n",
       "\n",
       "       2005      2006      2007      2008      2009 State  \n",
       "0  395146.0  406575.0  415746.0  424913.0  435700.0    CO  \n",
       "1   15236.0   15196.0   15180.0   15300.0   15289.0    CO  \n",
       "2  524466.0  531619.0  542039.0  552461.0  563161.0    CO  \n",
       "3   11496.0   11937.0   12262.0   12250.0   12169.0    CO  \n",
       "4    3997.0    3933.0    3866.0    3806.0    3767.0    CO  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init emmpty df for our population data\n",
    "pops00 = pd.DataFrame()\n",
    "\n",
    "# end goal - add every excel file in 00_source_data/county_pop/2000s to pops00\n",
    "\n",
    "path = r\"00_source_data/county_pop/2000s/\" # point to correct folder\n",
    "filenames = glob.glob(path + \"*.xls\")\n",
    "\n",
    "for f in filenames:\n",
    "\n",
    "    # read in current file with header = 3\n",
    "    temp = pd.read_excel(f, header = 3)\n",
    "\n",
    "    # regex to pull out state from filename\n",
    "    r = re.search(\"(2000s)(.)(\\w+)\", f)[3]\n",
    "    temp[\"State\"] = r[:2].upper()\n",
    "    \n",
    "    # drop null on any of the years\n",
    "    temp.dropna(subset=[2000], inplace=True)\n",
    "\n",
    "    #drop useless columns\n",
    "    temp.drop(columns={\"Unnamed: 1\", \"Unnamed: 12\", \"Unnamed: 13\"}, inplace=True)\n",
    "\n",
    "    # drop first row\n",
    "    temp = temp.iloc[1:, :]\n",
    "\n",
    "    # rename some cols\n",
    "    temp.rename(columns={\"Unnamed: 0\": \"County\"}, inplace=True)\n",
    "\n",
    "    # remove period at beginning of each county\n",
    "    temp[\"County\"] = temp[\"County\"].apply(lambda x: x[1:])\n",
    "\n",
    "    pops00 = pd.concat([pops00, temp], axis=0, ignore_index=True)\n",
    "\n",
    "# quick peek at the data\n",
    "pops00.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guide to cleaning - 2010s data\n",
    "\n",
    "The way the 2010s excel files are formatted, we can clean the data in the following way\n",
    "\n",
    "- load in with header=3\n",
    "- drop null on any of the populations\n",
    "    - notes at the bottom will be removed\n",
    "- drop census, estimates base\n",
    "- drop first row\n",
    "    - state as a whole\n",
    "- rename Unnamed: 0 to county\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams County</td>\n",
       "      <td>443691.0</td>\n",
       "      <td>452201.0</td>\n",
       "      <td>460558.0</td>\n",
       "      <td>469978.0</td>\n",
       "      <td>479946.0</td>\n",
       "      <td>490443.0</td>\n",
       "      <td>497734.0</td>\n",
       "      <td>503590.0</td>\n",
       "      <td>511354.0</td>\n",
       "      <td>517421.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alamosa County</td>\n",
       "      <td>15515.0</td>\n",
       "      <td>15709.0</td>\n",
       "      <td>15680.0</td>\n",
       "      <td>15787.0</td>\n",
       "      <td>15803.0</td>\n",
       "      <td>15894.0</td>\n",
       "      <td>16053.0</td>\n",
       "      <td>16108.0</td>\n",
       "      <td>16248.0</td>\n",
       "      <td>16233.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arapahoe County</td>\n",
       "      <td>574747.0</td>\n",
       "      <td>585968.0</td>\n",
       "      <td>596500.0</td>\n",
       "      <td>608467.0</td>\n",
       "      <td>619034.0</td>\n",
       "      <td>630984.0</td>\n",
       "      <td>638950.0</td>\n",
       "      <td>644478.0</td>\n",
       "      <td>651797.0</td>\n",
       "      <td>656590.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Archuleta County</td>\n",
       "      <td>12046.0</td>\n",
       "      <td>12021.0</td>\n",
       "      <td>12132.0</td>\n",
       "      <td>12216.0</td>\n",
       "      <td>12231.0</td>\n",
       "      <td>12387.0</td>\n",
       "      <td>12825.0</td>\n",
       "      <td>13295.0</td>\n",
       "      <td>13730.0</td>\n",
       "      <td>14029.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baca County</td>\n",
       "      <td>3807.0</td>\n",
       "      <td>3778.0</td>\n",
       "      <td>3722.0</td>\n",
       "      <td>3656.0</td>\n",
       "      <td>3587.0</td>\n",
       "      <td>3555.0</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>3554.0</td>\n",
       "      <td>3584.0</td>\n",
       "      <td>3581.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             County      2010      2011      2012      2013      2014  \\\n",
       "0      Adams County  443691.0  452201.0  460558.0  469978.0  479946.0   \n",
       "1    Alamosa County   15515.0   15709.0   15680.0   15787.0   15803.0   \n",
       "2   Arapahoe County  574747.0  585968.0  596500.0  608467.0  619034.0   \n",
       "3  Archuleta County   12046.0   12021.0   12132.0   12216.0   12231.0   \n",
       "4       Baca County    3807.0    3778.0    3722.0    3656.0    3587.0   \n",
       "\n",
       "       2015      2016      2017      2018      2019 State  \n",
       "0  490443.0  497734.0  503590.0  511354.0  517421.0    CO  \n",
       "1   15894.0   16053.0   16108.0   16248.0   16233.0    CO  \n",
       "2  630984.0  638950.0  644478.0  651797.0  656590.0    CO  \n",
       "3   12387.0   12825.0   13295.0   13730.0   14029.0    CO  \n",
       "4    3555.0    3530.0    3554.0    3584.0    3581.0    CO  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pops10 = pd.DataFrame()\n",
    "\n",
    "# add every excel file in 00_source_data/county_pop/2000s to pops00\n",
    "\n",
    "path = r\"00_source_data/county_pop/2010s\" # point to correct folder\n",
    "filenames = glob.glob(path + \"/*.xlsx\")\n",
    "\n",
    "for f in filenames:\n",
    "\n",
    "    # read in current file with header = 3\n",
    "    temp = pd.read_excel(f, header = 3)\n",
    "\n",
    "    # regex to pull out state from filename\n",
    "    r = re.search(\"(2010s)(.)(\\w+)\", f)[3]\n",
    "    temp[\"State\"] = r[:2].upper()\n",
    "    \n",
    "    # drop null on any of the years\n",
    "    temp.dropna(subset=[2010], inplace=True)\n",
    "\n",
    "    #drop useless columns\n",
    "    temp.drop(columns={\"Census\", \"Estimates Base\"}, inplace=True)\n",
    "\n",
    "    # drop first row\n",
    "    temp = temp.iloc[1:, :]\n",
    "\n",
    "    # rename some cols\n",
    "    temp.rename(columns={\"Unnamed: 0\": \"County\"}, inplace=True)\n",
    "\n",
    "    # remove period at beginning of each county\n",
    "    temp[\"County\"] = temp[\"County\"].apply(lambda x: x[1:])\n",
    "\n",
    "    # strip state from county\n",
    "    temp[\"County\"] = temp[\"County\"].apply(lambda x: x.split(\", \")[0])\n",
    "\n",
    "    pops10 = pd.concat([pops10, temp], axis=0, ignore_index=True)\n",
    "\n",
    "# quick peek at the data\n",
    "pops10.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt both dfs to get tidy format\n",
    "pops00 = pops00.melt([\"County\", \"State\"])\n",
    "pops10 = pops10.melt([\"County\", \"State\"])\n",
    "\n",
    "# rename columns accordingly\n",
    "pops00.rename(columns={\"variable\": \"Year\", \"value\": \"Population\"}, inplace=True)\n",
    "pops10.rename(columns={\"variable\": \"Year\", \"value\": \"Population\"}, inplace=True)\n",
    "\n",
    "# concatenate the two dfs to get all our population data in one place\n",
    "pops = pd.concat([pops00, pops10], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we have the same number of counties between datasets\n",
    "assert len(pops00[\"County\"].unique()) == len(pops10[\"County\"].unique())\n",
    "\n",
    "# check that we have the same number of counties every year\n",
    "# first, create a df with the number of counties per year\n",
    "pops_county_check = pops.groupby([\"State\", \"Year\"])[\"County\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>State</th>\n",
       "      <th>county_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>CO</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>FL</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>IL</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>MA</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>MD</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year State  county_count\n",
       "0  2000    CO            64\n",
       "1  2000    FL            67\n",
       "2  2000    IL           102\n",
       "3  2000    MA            14\n",
       "4  2000    MD            24"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group the sum of counties by year and state - will help us check if number of counties changes over the years\n",
    "grouped_states = pops_county_check.groupby([\"Year\", \"State\"])[\"County\"].sum().reset_index().rename(columns={\"County\": \"county_count\"})\n",
    "\n",
    "# here's what this looks like\n",
    "# we get a dataframe of states and years, with the number of counties in each state in each year\n",
    "grouped_states.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the above query, we should be able to assert that the number of counties per year is the same\n",
    "# below statement should always equal zero\n",
    "\n",
    "assert (grouped_states.duplicated(subset=[\"Year\", \"State\"]).sum() == 0)\n",
    "#assert (grouped_states10.duplicated(subset=[\"Year\", \"State\"]).sum() == 0)\n",
    "\n",
    "\n",
    "# ensure no duplicate values\n",
    "assert pops.duplicated().sum() == 0\n",
    "\n",
    "# loop to check that every state has the same number of counties every year\n",
    "for state in states:\n",
    "    assert (pops[pops[\"State\"] == state].Year.value_counts().nunique() == 1), f\"error on {state}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trying to integrate fip numbers for a better merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in fips data from external source\n",
    "fips = pd.read_csv(\"https://github.com/ChuckConnell/articles/raw/master/fips2county.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AL']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to get key from value in our abbreviation dictionary\n",
    "# will help us have consistent formatting across dataframes for merging purposes\n",
    "def get_keys_from_value(d, val):\n",
    "    return [k for k, v in d.items() if v == val]\n",
    "\n",
    "\n",
    "keys = get_keys_from_value(abbrev_to_us_state, 'Alabama')\n",
    "keys # quick peek to make sure it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the above to entire fips dataframe\n",
    "fips[\"state_abbrev\"] = fips[\"StateName\"].apply(lambda x: get_keys_from_value(abbrev_to_us_state, x)[0])\n",
    "\n",
    "# filter fips to appropriate states, now that it's in the correct format\n",
    "fips = fips[fips[\"state_abbrev\"].isin(states)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further cleaning of values before merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get rid of the word county in pop df\n",
    "def remove_county(x):\n",
    "\n",
    "    if \"County\" in x:\n",
    "        return x[:-7]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "pops[\"county_test\"] = pops[\"County\"].apply(lambda x: remove_county(x))\n",
    "\n",
    "\n",
    "# fix dona ana and la salle parish\n",
    "pops[\"county_test\"] = pops[\"county_test\"].apply(lambda x: x.replace(\"Doña Ana\", \"Dona Ana\"))\n",
    "fips[\"CountyName\"] = fips[\"CountyName\"].apply(lambda x: x.replace(\"DoÃ±a Ana\", \"Dona Ana\"))\n",
    "\n",
    "\n",
    "#pops[\"county_test\"] = pops[\"county_test\"].apply(lambda x: x.replace(\"La Salle Parish\", \"La Salle\"))\n",
    "\n",
    "\n",
    "# rename county_test where state is texas and county is la salle to La Salle (TX)\n",
    "pops.loc[(pops[\"State\"] == \"TX\") & (pops[\"county_test\"] == \"La Salle\"), \"county_test\"] = \"La Salle County\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change La Salle county name in fips to La Salle County\n",
    "fips.loc[fips[\"CountyName\"] == \"La Salle\", \"CountyName\"] = \"La Salle County\"\n",
    "fips.loc[fips[\"CountyName\"] == \"LaSalle Parish\", \"CountyName\"] = \"La Salle Parish\"\n",
    "pops.loc[pops[\"county_test\"] == \"LaSalle Parish\", \"county_test\"] = \"La Salle Parish\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final merge for population dataset & fip number dataset\n",
    "pops_copy = pops.merge(fips[[\"state_abbrev\", \"CountyFIPS\", \"StateFIPS\", \"CountyName\"]], left_on=[\"county_test\", \"State\"], right_on=[\"CountyName\", \"state_abbrev\"], how=\"outer\", indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should never end up with anything left out of merge\n",
    "assert len(pops_copy[pops_copy[\"_merge\"] != \"both\"]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add fip numbers to df_prescriptions\n",
    "\n",
    "# create copies of both dfs so we have a checkpoint to access our old dfs\n",
    "prescriptions_copy = df_prescriptions.copy()\n",
    "fips_copy = fips.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make buyer_county all lowercase\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.lower())\n",
    "\n",
    "# do the same for fips\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove county and parish from fips_copy\n",
    "\n",
    "def remove_parish(x):\n",
    "\n",
    "    if \"parish\" in x:\n",
    "        return x[:-7]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "# prescription dataset has similar format - match fips to this format\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: remove_county(x))\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: remove_parish(x))\n",
    "\n",
    "def expand_saint(x):\n",
    "\n",
    "    if \"st.\" in x:\n",
    "        return x.replace(\"st.\", \"saint\")\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# fix various other inconsistencies\n",
    "# left only values first\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: expand_saint(x))\n",
    "\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"desoto\", \"de soto\"))\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"desoto\", \"de soto\"))\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"st john the baptist\", \"saint john the baptist\"))\n",
    "\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"dekalb\", \"de kalb\"))\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"dekalb\", \"de kalb\"))\n",
    "\n",
    "# fix right only values\n",
    "\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"desoto\", \"de soto\"))\n",
    "\n",
    "\n",
    "\n",
    "# function to remove apostrophes from county names\n",
    "def remove_apostrophe(x):\n",
    "    \n",
    "    if \"'\" in x:\n",
    "        return x.replace(\"'\", \"\")\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "# apply to fips\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: remove_apostrophe(x))\n",
    "\n",
    "# replace lasalle with la salle in fips copy\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"lasalle\", \"la salle\"))\n",
    "\n",
    "# replace dewitt with de witt in prescriptions copy\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"dewitt\", \"de witt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prescriptions_fips = prescriptions_copy.merge(fips_copy, left_on=[\"BUYER_COUNTY\", \"BUYER_STATE\"], right_on=[\"CountyName\", \"state_abbrev\"], how=\"outer\", indicator=True)\n",
    "\n",
    "# capitalize year and month columns\n",
    "prescriptions_fips.rename(columns={\"year\": \"Year\", \"month\": \"Month\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2013.0    56182\n",
       "2014.0    54377\n",
       "2011.0     1543\n",
       "2012.0     1261\n",
       "2010.0     1012\n",
       "2008.0      912\n",
       "2009.0      911\n",
       "2007.0      895\n",
       "2006.0      791\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STILL HAVE ALL WASHINGTON YEARS HERE\n",
    "prescriptions_fips[prescriptions_fips[\"BUYER_STATE\"] == \"WA\"].Year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing missing values\n",
    "\n",
    "Since we have plenty of values joined with right_only indicator status, we know that some counties in our FIPS dataset is not merging correctly to our prescriptions dataset. Let's take one example of missing data - San Juan County in Washington. When exploring the Washington Post's website on prescription data and selecting for this county individually, we can see that the data does, in fact, exist here. However, we see an exceptionally low rate of pills prescribed (32 pills per person per year, in this case). Upon looking at some other examples, we can see that the counties joining with right_only below are likely missing from the Washington Post data due to having such small numbers.\n",
    "\n",
    "Since any given missing county does not have data available, we need to find a way to impute these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prescriptions_fips.Year.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KING            27359\n",
       "SNOHOMISH       13079\n",
       "PIERCE          11853\n",
       "SPOKANE         11069\n",
       "CLARK            8521\n",
       "YAKIMA           5826\n",
       "BENTON           4536\n",
       "THURSTON         3825\n",
       "KITSAP           3553\n",
       "COWLITZ          3049\n",
       "WHATCOM          2868\n",
       "SKAGIT           2536\n",
       "CLALLAM          1723\n",
       "WALLA WALLA      1701\n",
       "CHELAN           1502\n",
       "GRAYS HARBOR     1355\n",
       "GRANT            1333\n",
       "FRANKLIN         1247\n",
       "LEWIS            1242\n",
       "STEVENS          1060\n",
       "MASON            1054\n",
       "ISLAND            964\n",
       "ASOTIN            931\n",
       "KITTITAS          839\n",
       "WHITMAN           827\n",
       "OKANOGAN          742\n",
       "DOUGLAS           553\n",
       "PEND OREILLE      464\n",
       "PACIFIC           379\n",
       "JEFFERSON         353\n",
       "SAN JUAN          291\n",
       "LINCOLN           274\n",
       "ADAMS             267\n",
       "COLUMBIA          234\n",
       "KLICKITAT         157\n",
       "SKAMANIA          126\n",
       "FERRY             104\n",
       "WAHKIAKUM          46\n",
       "GARFIELD           42\n",
       "Name: BUYER_COUNTY, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at an example of our missing data\n",
    "wa = df_prescriptions[df_prescriptions[\"BUYER_STATE\"] == 'WA']\n",
    "\n",
    "wa[\"BUYER_COUNTY\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert prescriptions_fips[prescriptions_fips[\"_merge\"] == \"left_only\"].shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not all counties joining back to prescription dataset\n",
    "\n",
    "This could be okay, but I want to do a quick check that there are just not records for these counties. To do this, I'll take a small sample of counties in our FIPS dataset that did NOT merge properly to the prescriptions dataset, and search each one manually in the prescription dataset. I will search various different ways the counties could be transcribed, as well as google the county to ensure there are no secondary names for the same county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['REPORTER_DEA_NO', 'BUYER_STATE', 'BUYER_ZIP', 'BUYER_COUNTY',\n",
       "       'TRANSACTION_CODE', 'DRUG_CODE', 'DRUG_NAME', 'QUANTITY',\n",
       "       'TRANSACTION_DATE', 'Product_Name', 'Year', 'Month', 'date_test',\n",
       "       'year_test', 'StateFIPS', 'CountyFIPS_3', 'CountyName', 'StateName',\n",
       "       'CountyFIPS', 'StateAbbr', 'STATE_COUNTY', 'state_abbrev', '_merge'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prescriptions_fips[prescriptions_fips[\"_merge\"] != \"both\"].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need a new solution to the above - should impute the values somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prescriptions_fips = prescriptions_fips[prescriptions_fips[\"_merge\"] != \"right_only\"]\n",
    "\n",
    "# may have to add explanation for this higher up\n",
    "# re add assertion once value imputer is done\n",
    "#assert len(prescriptions_copy) == len(prescriptions_fips)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding fips to our cause of death data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copies of both dfs\n",
    "\n",
    "cause_of_death_copy = df_cause_of_death.copy()\n",
    "fips_copy = fips.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove county once again\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: remove_county(x))\n",
    "\n",
    "\n",
    "# clean some other miscellaneous values up\n",
    "\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"LaSalle Parish\", \"La Salle Parish\"))\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"DeBaca\", \"De Baca\"))\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"La Salle\", \"La Salle County\"))\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"La Salle County Parish\", \"La Salle Parish\"))\n",
    "\n",
    "\n",
    "\n",
    "# expand mckean to mc kean in fips_copy\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"McKean\", \"Mc Kean\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_of_death_fips = cause_of_death_copy.merge(fips_copy, left_on=[\"County\", \"State\"], right_on=[\"CountyName\", \"state_abbrev\"], how=\"outer\", indicator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not all counties joining back to cause of death dataset\n",
    "\n",
    "If the number of people in a given category (eg. one county/year/cause of death category) is less than 10, those records do not appear in this data. There is also a technicality in the number of total deaths vs. drug deaths (which we are interested in).\n",
    "\n",
    "The example we are given is that if a county has 20 deaths unrelated to drugs and alcohol, and only 7 related to alcohol, only the former figure will be reported. In the next notebook (pick_states.ipynb), we will filter by cause of death. In this notebook, since we still have all causes of death, we will impute for every missing value.\n",
    "\n",
    "To impute this data, we will fill in missing values with **a random integer from 0 to 9**. We thought of drawing from a normal distribution, but this implies negative values could be attained. We could take their absolute values to negate this effect, but then we are no longer drawing from a *true* normal distribution, so we chose to pick random values in our range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace null value with a random integer from 0 to 10 with a normal distribution\n",
    "def value_imputer(x):\n",
    "    if pd.isnull(x):\n",
    "        return random.randint(0, 9)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "cause_of_death_fips[\"Deaths\"] = cause_of_death_fips[\"Deaths\"].apply(lambda x: value_imputer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    1\n",
       "4    1\n",
       "7    1\n",
       "1    1\n",
       "3    1\n",
       "9    1\n",
       "Name: Deaths, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick look at our new imputed data\n",
    "cause_of_death_fips[cause_of_death_fips[\"_merge\"] != \"both\"].Deaths.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Population to final DataFrames\n",
    "\n",
    "For pop_fips, cause_of_death_fips, and prescription_fips. Steps needed:\n",
    "\n",
    "- Create unique ID from county FIPS and state FIPS\n",
    "- Merge population dataset based on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_of_death_fips = cause_of_death_fips[cause_of_death_fips[\"_merge\"] == \"both\"]\n",
    "#pops_copy = pops_copy[cause_of_death_fips[\"_merge\"] == \"both\"]\n",
    "prescriptions_fips = prescriptions_fips[prescriptions_fips[\"_merge\"] == \"both\"]\n",
    "\n",
    "\n",
    "# drop merge columns\n",
    "cause_of_death_fips.drop(columns=[\"_merge\"], inplace=True)\n",
    "prescriptions_fips.drop(columns=[\"_merge\"], inplace=True)\n",
    "pops_copy.drop(columns=[\"_merge\",], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique FIP from county and state fips\n",
    "\n",
    "cause_of_death_fips[\"FIP_unique\"] = cause_of_death_fips[\"CountyFIPS\"].apply(lambda x: str(x)) + cause_of_death_fips[\"StateFIPS\"].apply(lambda x: str(x))\n",
    "prescriptions_fips[\"FIP_unique\"] = prescriptions_fips[\"CountyFIPS\"].apply(lambda x: str(x)) + prescriptions_fips[\"StateFIPS\"].apply(lambda x: str(x))\n",
    "pops_copy[\"FIP_unique\"] = pops_copy[\"CountyFIPS\"].apply(lambda x: str(x)) + pops_copy[\"StateFIPS\"].apply(lambda x: str(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add some sort of assert here. not sure what it should be yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final prescriptions dataset with populations\n",
    "# can safely left join here, because we only need records in the prescriptions dataset\n",
    "prescriptions = prescriptions_fips.merge(pops_copy, on=[\"FIP_unique\", \"Year\"], how=\"left\", indicator=True)\n",
    "\n",
    "assert (prescriptions[\"_merge\"] == \"both\").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2013.0    56182\n",
       "2014.0    54377\n",
       "2011.0     1543\n",
       "2012.0     1261\n",
       "2010.0     1012\n",
       "2008.0      912\n",
       "2009.0      911\n",
       "2007.0      895\n",
       "2006.0      791\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STILL HAVE WASHINGTON YEARS HERE\n",
    "prescriptions[prescriptions[\"BUYER_STATE\"] == \"WA\"].Year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one more assert to check length\n",
    "assert len(prescriptions) == len(prescriptions_fips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some useless columns\n",
    "prescriptions.drop(columns=[\"_merge\", \"CountyName_y\", \"StateFIPS_y\", \"CountyFIPS_y\",\"state_abbrev_y\", \"County\", \"CountyFIPS_3\"], inplace=True)\n",
    "\n",
    "# rename x columns\n",
    "prescriptions.rename(columns={\"CountyName_x\": \"CountyName\", \"StateFIPS_x\": \"StateFIPS\", \"CountyFIPS_x\": \"CountyFIPS\", \"state_abbrev_x\": \"state_abbrev\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final cause of death dataset with populations\n",
    "# can safely left join here, because we only need records in the cause of death dataset\n",
    "cause_of_death = cause_of_death_fips.merge(pops_copy, on=[\"FIP_unique\", \"Year\"], how=\"left\", indicator=True)\n",
    "\n",
    "assert cause_of_death_fips.Deaths.isnull().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some useless columns\n",
    "cause_of_death.drop(columns=[\"_merge\", \"CountyName_y\", \"StateFIPS_y\", \"CountyFIPS_y\",\"state_abbrev_y\", \"County_y\", \"CountyFIPS_3\", \"State_y\"], inplace=True)\n",
    "\n",
    "# rename x columns\n",
    "cause_of_death.rename(columns={\"County_x\": \"County\", \"Year_x\": \"Year\", \"State_x\": \"State\", \"StateFIPS_x\": \"StateFIPS\", \"CountyFIPS_x\": \"CountyFIPS\", \"state_abbrev_x\": \"state_abbrev\", \"CountyName_x\": \"CountyName\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asserts to make sure we didn't lose any records from our original datasets\n",
    "\n",
    "assert len(df_cause_of_death) == len(cause_of_death)\n",
    "assert len(df_prescriptions) == len(prescriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export main, unjoined datasets in case we need them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_of_death.to_csv(\"20_intermediate_files/cause_of_death_clean.csv\", index=False)\n",
    "prescriptions.to_csv(\"20_intermediate_files/arcos_all_washpost_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final 3 datasets\n",
    "\n",
    "We should have: (UNSURE IF WE SHOULD EXTEND DATE RANGES, CURRENTLY 3 YEARS BEFORE AND AFTER POLICY IMPLEMENTATION)\n",
    "\n",
    "- Florida and Georgia 2007 - 2013\n",
    "- Texas and Oklahoma 2004 - 2010\n",
    "- Washington and Oregon 2009 - 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug overdose - broken down by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Florida and Georgia\n",
    "\n",
    "prescriptions_fl = prescriptions.copy()\n",
    "prescriptions_wa = prescriptions.copy()\n",
    "\n",
    "prescriptions_fl = prescriptions_fl[(prescriptions_fl[\"BUYER_STATE\"] == \"FL\") | (prescriptions_fl[\"BUYER_STATE\"].isin(fl_states))]\n",
    "prescriptions_wa = prescriptions_wa[(prescriptions_wa[\"BUYER_STATE\"] == \"WA\") | (prescriptions_wa[\"BUYER_STATE\"]).isin(wa_states)]\n",
    "\n",
    "\n",
    "\n",
    "# filter appropriate years\n",
    "fl_start = 2007\n",
    "fl_end = 2013\n",
    "\n",
    "# tx will only be used for overdose deaths\n",
    "tx_start = 2004\n",
    "tx_end = 2010\n",
    "\n",
    "wa_start = 2009\n",
    "wa_end = 2015\n",
    "\n",
    "\n",
    "prescriptions_fl = prescriptions_fl[(prescriptions_fl[\"Year\"] >= fl_start) & (prescriptions_fl[\"Year\"] <= fl_end)]\n",
    "prescriptions_wa = prescriptions_wa[(prescriptions_wa[\"Year\"] >= wa_start) & (prescriptions_wa[\"Year\"] <= wa_end)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prescriptions_fl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_52032/1747030193.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprescriptions_fl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'prescriptions_fl' is not defined"
     ]
    }
   ],
   "source": [
    "prescriptions_fl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cause of death - broken down by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_fl = cause_of_death.copy()\n",
    "deaths_tx = cause_of_death.copy()\n",
    "deaths_wa = cause_of_death.copy()\n",
    "\n",
    "deaths_fl = deaths_fl[(deaths_fl[\"StateName\"] == \"Florida\") | (deaths_fl[\"State\"].isin(fl_states))]\n",
    "deaths_tx = deaths_tx[(deaths_tx[\"StateName\"] == \"Texas\") | (deaths_tx[\"State\"].isin(tx_states))]\n",
    "deaths_wa = deaths_wa[(deaths_wa[\"StateName\"] == \"Washington\") | (deaths_wa[\"State\"].isin(wa_states))]\n",
    "\n",
    "deaths_fl = deaths_fl[(deaths_fl[\"Year\"] >= fl_start) & (deaths_fl[\"Year\"] <= fl_end)]\n",
    "deaths_tx = deaths_tx[(deaths_tx[\"Year\"] >= tx_start) & (deaths_tx[\"Year\"] <= tx_end)]  \n",
    "deaths_wa = deaths_wa[(deaths_wa[\"Year\"] >= wa_start) & (deaths_wa[\"Year\"] <= wa_end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final assert to check years\n",
    "\n",
    "assert prescriptions_fl.Year.unique().tolist() == list(range(fl_start, fl_end + 1))\n",
    "assert prescriptions_wa.Year.unique().tolist() == list(range(wa_start, wa_end + 1))\n",
    "\n",
    "assert deaths_fl.Year.unique().tolist() == list(range(fl_start, fl_end + 1))\n",
    "assert deaths_tx.Year.unique().tolist() == list(range(tx_start, tx_end + 1))\n",
    "assert deaths_wa.Year.unique().tolist() == list(range(wa_start, wa_end + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export all to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prescriptions_fl.to_csv(\"20_intermediate_files/prescriptions_fl.csv\", index=False)\n",
    "prescriptions_wa.to_csv(\"20_intermediate_files/prescriptions_wa.csv\", index=False)\n",
    "\n",
    "deaths_fl.to_csv(\"20_intermediate_files/deaths_fl.csv\", index=False)\n",
    "deaths_tx.to_csv(\"20_intermediate_files/deaths_tx.csv\", index=False)\n",
    "deaths_wa.to_csv(\"20_intermediate_files/deaths_wa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for the group\n",
    "\n",
    "- may need to filter out a couple more columns - haven't done this yet as I don't want to accidentally delete something we need\n",
    "- overdose data is only broken down by year unless i messed something up - overdose analysis will have to be less granular"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49aa7209ac0e1a36a89cb04290394fd089cb5ce56cb44c9d4652c0180c6152a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
