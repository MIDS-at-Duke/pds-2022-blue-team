{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import xlrd\n",
    "import re\n",
    "import openpyxl\n",
    "import random\n",
    "\n",
    "# custom file that maps state names to abbreviations\n",
    "from abbreviation_conversion import abbrev_to_us_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up prescription data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing the find_year function: 2019\n",
      "testing the find_month function: 12\n"
     ]
    }
   ],
   "source": [
    "def find_year(TRANSACTION_DATE):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        TRANSACTION_DATE (str): date in format MMDDYYYY\n",
    "\n",
    "    Returns:\n",
    "        int: year\n",
    "    \"\"\"\n",
    "    TRANSACTION_DATE = str(TRANSACTION_DATE)\n",
    "    \n",
    "    return int(TRANSACTION_DATE[-4:])\n",
    "\n",
    "# quick test \n",
    "print(f\"testing the find_year function: {find_year(12202019)}\")\n",
    "\n",
    "\n",
    "def find_month(TRANSACTION_DATE):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        TRANSACTION_DATE (str): date in format MMDDYYYY\n",
    "\n",
    "    Returns:\n",
    "        int: month\n",
    "    \"\"\"\n",
    "    TRANSACTION_DATE = str(TRANSACTION_DATE)\n",
    "\n",
    "    if len(TRANSACTION_DATE) == 8:\n",
    "        return int(TRANSACTION_DATE[:2])\n",
    "    else:\n",
    "        return int(TRANSACTION_DATE[:1])\n",
    "    \n",
    "\n",
    "# quick test \n",
    "print(f\"testing the find_month function: {find_month(12202019)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load in the data, we need to truncate the amount of columns we use as well as the states\n",
    "cols_to_keep = [\"REPORTER_DEA_NO\", \"BUYER_STATE\", \"BUYER_ZIP\", \"BUYER_COUNTY\", \"DRUG_CODE\", \"TRANSACTION_CODE\", \"DRUG_NAME\", \"QUANTITY\", \"TRANSACTION_DATE\", \"Product_Name\"]\n",
    "\n",
    "# we know we need Florida, Texas, and Washington\n",
    "states = [\"FL\", \"TX\", \"WA\"]\n",
    "# since we are normalizing based on population, I think we should pick states that are regionally close to our target states\n",
    "# we can change this later as a group, but I have these selected below:\n",
    "\n",
    "# Florida comparison states\n",
    "fl_states = [\"PA\", \"MI\", \"NC\"]\n",
    "\n",
    "# Texas comparison states\n",
    "tx_states = [\"IL\", \"MA\", \"MI\"]\n",
    "\n",
    "# Washington comparison states\n",
    "wa_states = [\"NC\", \"CO\", \"MD\"]\n",
    "\n",
    "# create list of all states to use\n",
    "variable_states = []\n",
    "variable_states.extend(fl_states)\n",
    "variable_states.extend(tx_states)\n",
    "variable_states.extend(wa_states)\n",
    "\n",
    "# append variable states to our original list\n",
    "states.extend(variable_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abzdel\\AppData\\Local\\Temp/ipykernel_48440/2399049506.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(filtered_chunk)\n"
     ]
    }
   ],
   "source": [
    "# now, load in our data as an iterator so we can load in chunks\n",
    "it = pd.read_csv(\"00_source_data/arcos_all_washpost.tsv\", chunksize=500_000, sep='\\t', usecols = cols_to_keep) # may have to change chunksize depending on your computer's memory\n",
    "\n",
    "# init empty dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for chunk in it:\n",
    "    # extract year out of date column\n",
    "    chunk[\"year\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: find_year(x))\n",
    "    chunk[\"month\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: find_month(x))\n",
    "\n",
    "    # ensure we're working in the correct date range\n",
    "    filtered_chunk = chunk[chunk[\"year\"] > 2002]\n",
    "    filtered_chunk = filtered_chunk[filtered_chunk[\"year\"] < 2016]\n",
    "\n",
    "    # filter out the states we want\n",
    "    filtered_chunk = filtered_chunk[filtered_chunk[\"BUYER_STATE\"].isin(states)]\n",
    "\n",
    "    df = df.append(filtered_chunk)\n",
    "    break\n",
    "df\n",
    "\n",
    "df_prescriptions = df.copy() # keep a copy of this df for later filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While doing analysis, we learned that a handful of county values in MA were missing. However, when we looked up the associated zip codes (02401, 02174), we learned that we could fill these values with Plymouth and Middlesex Counties, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REPORTER_DEA_NO</th>\n",
       "      <th>BUYER_STATE</th>\n",
       "      <th>BUYER_ZIP</th>\n",
       "      <th>BUYER_COUNTY</th>\n",
       "      <th>TRANSACTION_CODE</th>\n",
       "      <th>DRUG_CODE</th>\n",
       "      <th>DRUG_NAME</th>\n",
       "      <th>QUANTITY</th>\n",
       "      <th>TRANSACTION_DATE</th>\n",
       "      <th>Product_Name</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4035</th>\n",
       "      <td>PB0020139</td>\n",
       "      <td>MA</td>\n",
       "      <td>2401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5182006</td>\n",
       "      <td>OXYCODO.HCL 5.35MG/TAB</td>\n",
       "      <td>2006</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036</th>\n",
       "      <td>PB0020139</td>\n",
       "      <td>MA</td>\n",
       "      <td>2401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7312006</td>\n",
       "      <td>OXYCODONE HCL/ACETAMINOPHEN 5MG/325M</td>\n",
       "      <td>2006</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141994</th>\n",
       "      <td>PG0149650</td>\n",
       "      <td>MA</td>\n",
       "      <td>2174</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1172006</td>\n",
       "      <td>HYDROCOD.BIT.&amp; APAP,10MG/660MG/TAB</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       REPORTER_DEA_NO BUYER_STATE  BUYER_ZIP BUYER_COUNTY TRANSACTION_CODE  \\\n",
       "4035         PB0020139          MA       2401          NaN                S   \n",
       "4036         PB0020139          MA       2401          NaN                S   \n",
       "141994       PG0149650          MA       2174          NaN                S   \n",
       "\n",
       "        DRUG_CODE    DRUG_NAME  QUANTITY  TRANSACTION_DATE  \\\n",
       "4035         9143    OXYCODONE       2.0           5182006   \n",
       "4036         9143    OXYCODONE      12.0           7312006   \n",
       "141994       9193  HYDROCODONE       2.0           1172006   \n",
       "\n",
       "                                Product_Name  year  month  \n",
       "4035                  OXYCODO.HCL 5.35MG/TAB  2006      5  \n",
       "4036    OXYCODONE HCL/ACETAMINOPHEN 5MG/325M  2006      7  \n",
       "141994    HYDROCOD.BIT.& APAP,10MG/660MG/TAB  2006      1  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick look at the null values (we checked, and these are all the values for which county is null)\n",
    "df_prescriptions[df_prescriptions[\"BUYER_COUNTY\"].isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace values accordingly\n",
    "df_prescriptions.loc[df_prescriptions[\"BUYER_ZIP\"] == 2401, \"BUYER_COUNTY\"] = \"PLYMOUTH\"\n",
    "df_prescriptions.loc[df_prescriptions[\"BUYER_ZIP\"] == 2174, \"BUYER_COUNTY\"] = \"MIDDLESEX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null values again\n",
    "df_prescriptions[df_prescriptions[\"BUYER_COUNTY\"].isnull()]\n",
    "\n",
    "assert len(df_prescriptions[df_prescriptions[\"BUYER_COUNTY\"].isnull()]) == 0, \"still have missing values for counties - double check code above\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up cause of death data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'00_source_data/cause_of_death' # point to correct folder\n",
    "filenames = glob.glob(path + \"/*.txt\") # select all text files in folder\n",
    "\n",
    "df = pd.DataFrame() # empty df - will store data from all txt files\n",
    "\n",
    "for f in filenames:\n",
    "    temp = pd.read_csv(f, index_col=None, header=0, sep='\\t')\n",
    "    # we're getting some extraneous notes at the bottom - let's just drop based on county as these will only be null for these useless notes columns\n",
    "    temp.dropna(subset={'County'}, inplace=True)\n",
    "    \n",
    "    df = pd.concat([df, temp], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to separate county and state\n",
    "\n",
    "def abtract_state(county):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        county (str): county name\n",
    "\n",
    "    Returns:\n",
    "        str: state\n",
    "    \"\"\"\n",
    "    return county.split(\", \")[1]\n",
    "\n",
    "\n",
    "\n",
    "def abstract_county(county):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        county (str): county name\n",
    "\n",
    "    Returns:\n",
    "        str: county\n",
    "    \"\"\"\n",
    "    return county.split(\", \")[0]\n",
    "\n",
    "# apply functions to our df\n",
    "df[\"State\"] = df.apply(lambda x: abtract_state(x[\"County\"]), axis=1)\n",
    "df[\"County\"] = df.apply(lambda x: abstract_county(x[\"County\"]), axis=1)\n",
    "\n",
    "# do not need notes column, let's just drop it here\n",
    "df.drop(columns={\"Notes\"}, inplace=True)\n",
    "\n",
    "df_cause_of_death = df.copy() # keep a copy of this df for later filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's filter our dataframe to be only the states we want\n",
    "df_cause_of_death = df_cause_of_death[df_cause_of_death[\"State\"].isin(states)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding in County Population data\n",
    "\n",
    "[Census county pop. data, 2000-2010](https://www.census.gov/data/tables/time-series/demo/popest/intercensal-2000-2010-counties.html)<br>\n",
    "[Census county pop. data, 2010-2019](https://www.census.gov/data/datasets/time-series/demo/popest/2010s-counties-total.html)<br>\n",
    "For both, just select the appropriate states on the webpage. We will clean and merge as needed in this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guide to cleaning - 2000s data\n",
    "\n",
    "The way the 2000s excel files are formatted, we can clean the data in the following way\n",
    "\n",
    "- load in with header=3\n",
    "- drop null on any of the populations\n",
    "    - notes at the bottom will be removed\n",
    "- drop unnamed 1, 12, and 13\n",
    "    - these contain redundant data about populations from specific dates\n",
    "    - Unnamed 12 is 2010s pop - will be redundant as our next dataset has this as well. Using the newer data\n",
    "- drop first row\n",
    "    - state as a whole\n",
    "- rename Unnamed: 0 to county\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams County</td>\n",
       "      <td>350888.0</td>\n",
       "      <td>359816.0</td>\n",
       "      <td>370753.0</td>\n",
       "      <td>377464.0</td>\n",
       "      <td>384809.0</td>\n",
       "      <td>395146.0</td>\n",
       "      <td>406575.0</td>\n",
       "      <td>415746.0</td>\n",
       "      <td>424913.0</td>\n",
       "      <td>435700.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alamosa County</td>\n",
       "      <td>14954.0</td>\n",
       "      <td>14956.0</td>\n",
       "      <td>15114.0</td>\n",
       "      <td>15067.0</td>\n",
       "      <td>15217.0</td>\n",
       "      <td>15236.0</td>\n",
       "      <td>15196.0</td>\n",
       "      <td>15180.0</td>\n",
       "      <td>15300.0</td>\n",
       "      <td>15289.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arapahoe County</td>\n",
       "      <td>491482.0</td>\n",
       "      <td>502393.0</td>\n",
       "      <td>508936.0</td>\n",
       "      <td>513690.0</td>\n",
       "      <td>518971.0</td>\n",
       "      <td>524466.0</td>\n",
       "      <td>531619.0</td>\n",
       "      <td>542039.0</td>\n",
       "      <td>552461.0</td>\n",
       "      <td>563161.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Archuleta County</td>\n",
       "      <td>10020.0</td>\n",
       "      <td>10454.0</td>\n",
       "      <td>10885.0</td>\n",
       "      <td>11089.0</td>\n",
       "      <td>11266.0</td>\n",
       "      <td>11496.0</td>\n",
       "      <td>11937.0</td>\n",
       "      <td>12262.0</td>\n",
       "      <td>12250.0</td>\n",
       "      <td>12169.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baca County</td>\n",
       "      <td>4501.0</td>\n",
       "      <td>4471.0</td>\n",
       "      <td>4336.0</td>\n",
       "      <td>4117.0</td>\n",
       "      <td>4064.0</td>\n",
       "      <td>3997.0</td>\n",
       "      <td>3933.0</td>\n",
       "      <td>3866.0</td>\n",
       "      <td>3806.0</td>\n",
       "      <td>3767.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             County      2000      2001      2002      2003      2004  \\\n",
       "0      Adams County  350888.0  359816.0  370753.0  377464.0  384809.0   \n",
       "1    Alamosa County   14954.0   14956.0   15114.0   15067.0   15217.0   \n",
       "2   Arapahoe County  491482.0  502393.0  508936.0  513690.0  518971.0   \n",
       "3  Archuleta County   10020.0   10454.0   10885.0   11089.0   11266.0   \n",
       "4       Baca County    4501.0    4471.0    4336.0    4117.0    4064.0   \n",
       "\n",
       "       2005      2006      2007      2008      2009 State  \n",
       "0  395146.0  406575.0  415746.0  424913.0  435700.0    CO  \n",
       "1   15236.0   15196.0   15180.0   15300.0   15289.0    CO  \n",
       "2  524466.0  531619.0  542039.0  552461.0  563161.0    CO  \n",
       "3   11496.0   11937.0   12262.0   12250.0   12169.0    CO  \n",
       "4    3997.0    3933.0    3866.0    3806.0    3767.0    CO  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init emmpty df for our population data\n",
    "pops00 = pd.DataFrame()\n",
    "\n",
    "# end goal - add every excel file in 00_source_data/county_pop/2000s to pops00\n",
    "\n",
    "path = r\"00_source_data/county_pop/2000s/\" # point to correct folder\n",
    "filenames = glob.glob(path + \"*.xls\")\n",
    "\n",
    "for f in filenames:\n",
    "\n",
    "    # read in current file with header = 3\n",
    "    temp = pd.read_excel(f, header = 3)\n",
    "\n",
    "    # regex to pull out state from filename\n",
    "    r = re.search(\"(2000s)(.)(\\w+)\", f)[3]\n",
    "    temp[\"State\"] = r[:2].upper()\n",
    "    \n",
    "    # drop null on any of the years\n",
    "    temp.dropna(subset=[2000], inplace=True)\n",
    "\n",
    "    #drop useless columns\n",
    "    temp.drop(columns={\"Unnamed: 1\", \"Unnamed: 12\", \"Unnamed: 13\"}, inplace=True)\n",
    "\n",
    "    # drop first row\n",
    "    temp = temp.iloc[1:, :]\n",
    "\n",
    "    # rename some cols\n",
    "    temp.rename(columns={\"Unnamed: 0\": \"County\"}, inplace=True)\n",
    "\n",
    "    # remove period at beginning of each county\n",
    "    temp[\"County\"] = temp[\"County\"].apply(lambda x: x[1:])\n",
    "\n",
    "    pops00 = pd.concat([pops00, temp], axis=0, ignore_index=True)\n",
    "\n",
    "# quick peek at the data\n",
    "pops00.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guide to cleaning - 2010s data\n",
    "\n",
    "The way the 2010s excel files are formatted, we can clean the data in the following way\n",
    "\n",
    "- load in with header=3\n",
    "- drop null on any of the populations\n",
    "    - notes at the bottom will be removed\n",
    "- drop census, estimates base\n",
    "- drop first row\n",
    "    - state as a whole\n",
    "- rename Unnamed: 0 to county\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams County</td>\n",
       "      <td>443691.0</td>\n",
       "      <td>452201.0</td>\n",
       "      <td>460558.0</td>\n",
       "      <td>469978.0</td>\n",
       "      <td>479946.0</td>\n",
       "      <td>490443.0</td>\n",
       "      <td>497734.0</td>\n",
       "      <td>503590.0</td>\n",
       "      <td>511354.0</td>\n",
       "      <td>517421.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alamosa County</td>\n",
       "      <td>15515.0</td>\n",
       "      <td>15709.0</td>\n",
       "      <td>15680.0</td>\n",
       "      <td>15787.0</td>\n",
       "      <td>15803.0</td>\n",
       "      <td>15894.0</td>\n",
       "      <td>16053.0</td>\n",
       "      <td>16108.0</td>\n",
       "      <td>16248.0</td>\n",
       "      <td>16233.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arapahoe County</td>\n",
       "      <td>574747.0</td>\n",
       "      <td>585968.0</td>\n",
       "      <td>596500.0</td>\n",
       "      <td>608467.0</td>\n",
       "      <td>619034.0</td>\n",
       "      <td>630984.0</td>\n",
       "      <td>638950.0</td>\n",
       "      <td>644478.0</td>\n",
       "      <td>651797.0</td>\n",
       "      <td>656590.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Archuleta County</td>\n",
       "      <td>12046.0</td>\n",
       "      <td>12021.0</td>\n",
       "      <td>12132.0</td>\n",
       "      <td>12216.0</td>\n",
       "      <td>12231.0</td>\n",
       "      <td>12387.0</td>\n",
       "      <td>12825.0</td>\n",
       "      <td>13295.0</td>\n",
       "      <td>13730.0</td>\n",
       "      <td>14029.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baca County</td>\n",
       "      <td>3807.0</td>\n",
       "      <td>3778.0</td>\n",
       "      <td>3722.0</td>\n",
       "      <td>3656.0</td>\n",
       "      <td>3587.0</td>\n",
       "      <td>3555.0</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>3554.0</td>\n",
       "      <td>3584.0</td>\n",
       "      <td>3581.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             County      2010      2011      2012      2013      2014  \\\n",
       "0      Adams County  443691.0  452201.0  460558.0  469978.0  479946.0   \n",
       "1    Alamosa County   15515.0   15709.0   15680.0   15787.0   15803.0   \n",
       "2   Arapahoe County  574747.0  585968.0  596500.0  608467.0  619034.0   \n",
       "3  Archuleta County   12046.0   12021.0   12132.0   12216.0   12231.0   \n",
       "4       Baca County    3807.0    3778.0    3722.0    3656.0    3587.0   \n",
       "\n",
       "       2015      2016      2017      2018      2019 State  \n",
       "0  490443.0  497734.0  503590.0  511354.0  517421.0    CO  \n",
       "1   15894.0   16053.0   16108.0   16248.0   16233.0    CO  \n",
       "2  630984.0  638950.0  644478.0  651797.0  656590.0    CO  \n",
       "3   12387.0   12825.0   13295.0   13730.0   14029.0    CO  \n",
       "4    3555.0    3530.0    3554.0    3584.0    3581.0    CO  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pops10 = pd.DataFrame()\n",
    "\n",
    "# add every excel file in 00_source_data/county_pop/2000s to pops00\n",
    "\n",
    "path = r\"00_source_data/county_pop/2010s\" # point to correct folder\n",
    "filenames = glob.glob(path + \"/*.xlsx\")\n",
    "\n",
    "for f in filenames:\n",
    "\n",
    "    # read in current file with header = 3\n",
    "    temp = pd.read_excel(f, header = 3)\n",
    "\n",
    "    # regex to pull out state from filename\n",
    "    r = re.search(\"(2010s)(.)(\\w+)\", f)[3]\n",
    "    temp[\"State\"] = r[:2].upper()\n",
    "    \n",
    "    # drop null on any of the years\n",
    "    temp.dropna(subset=[2010], inplace=True)\n",
    "\n",
    "    #drop useless columns\n",
    "    temp.drop(columns={\"Census\", \"Estimates Base\"}, inplace=True)\n",
    "\n",
    "    # drop first row\n",
    "    temp = temp.iloc[1:, :]\n",
    "\n",
    "    # rename some cols\n",
    "    temp.rename(columns={\"Unnamed: 0\": \"County\"}, inplace=True)\n",
    "\n",
    "    # remove period at beginning of each county\n",
    "    temp[\"County\"] = temp[\"County\"].apply(lambda x: x[1:])\n",
    "\n",
    "    # strip state from county\n",
    "    temp[\"County\"] = temp[\"County\"].apply(lambda x: x.split(\", \")[0])\n",
    "\n",
    "    pops10 = pd.concat([pops10, temp], axis=0, ignore_index=True)\n",
    "\n",
    "# quick peek at the data\n",
    "pops10.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt both dfs to get tidy format\n",
    "pops00 = pops00.melt([\"County\", \"State\"])\n",
    "pops10 = pops10.melt([\"County\", \"State\"])\n",
    "\n",
    "# rename columns accordingly\n",
    "pops00.rename(columns={\"variable\": \"Year\", \"value\": \"Population\"}, inplace=True)\n",
    "pops10.rename(columns={\"variable\": \"Year\", \"value\": \"Population\"}, inplace=True)\n",
    "\n",
    "# concatenate the two dfs to get all our population data in one place\n",
    "pops = pd.concat([pops00, pops10], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we have the same number of counties between datasets\n",
    "assert len(pops00[\"County\"].unique()) == len(pops10[\"County\"].unique())\n",
    "\n",
    "# check that we have the same number of counties every year\n",
    "# first, create a df with the number of counties per year\n",
    "pops_county_check = pops.groupby([\"State\", \"Year\"])[\"County\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>State</th>\n",
       "      <th>county_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>CO</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>FL</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>IL</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>MA</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>MD</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year State  county_count\n",
       "0  2000    CO            64\n",
       "1  2000    FL            67\n",
       "2  2000    IL           102\n",
       "3  2000    MA            14\n",
       "4  2000    MD            24"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group the sum of counties by year and state - will help us check if number of counties changes over the years\n",
    "grouped_states = pops_county_check.groupby([\"Year\", \"State\"])[\"County\"].sum().reset_index().rename(columns={\"County\": \"county_count\"})\n",
    "\n",
    "# here's what this looks like\n",
    "# we get a dataframe of states and years, with the number of counties in each state in each year\n",
    "grouped_states.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the above query, we should be able to assert that the number of counties per year is the same\n",
    "# below statement should always equal zero\n",
    "\n",
    "assert (grouped_states.duplicated(subset=[\"Year\", \"State\"]).sum() == 0)\n",
    "#assert (grouped_states10.duplicated(subset=[\"Year\", \"State\"]).sum() == 0)\n",
    "\n",
    "\n",
    "# ensure no duplicate values\n",
    "assert pops.duplicated().sum() == 0\n",
    "\n",
    "# loop to check that every state has the same number of counties every year\n",
    "for state in states:\n",
    "    assert (pops[pops[\"State\"] == state].Year.value_counts().nunique() == 1), f\"error on {state}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trying to integrate fip numbers for a better merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in fips data from external source\n",
    "fips = pd.read_csv(\"https://github.com/ChuckConnell/articles/raw/master/fips2county.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AL']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to get key from value in our abbreviation dictionary\n",
    "# will help us have consistent formatting across dataframes for merging purposes\n",
    "def get_keys_from_value(d, val):\n",
    "    return [k for k, v in d.items() if v == val]\n",
    "\n",
    "\n",
    "keys = get_keys_from_value(abbrev_to_us_state, 'Alabama')\n",
    "keys # quick peek to make sure it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the above to entire fips dataframe\n",
    "fips[\"state_abbrev\"] = fips[\"StateName\"].apply(lambda x: get_keys_from_value(abbrev_to_us_state, x)[0])\n",
    "\n",
    "# filter fips to appropriate states, now that it's in the correct format\n",
    "fips = fips[fips[\"state_abbrev\"].isin(states)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further cleaning of values before merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get rid of the word county in pop df\n",
    "def remove_county(x):\n",
    "\n",
    "    if \"County\" in x:\n",
    "        return x[:-7]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "pops[\"county_test\"] = pops[\"County\"].apply(lambda x: remove_county(x))\n",
    "\n",
    "\n",
    "# fix dona ana and la salle parish\n",
    "pops[\"county_test\"] = pops[\"county_test\"].apply(lambda x: x.replace(\"Doña Ana\", \"Dona Ana\"))\n",
    "fips[\"CountyName\"] = fips[\"CountyName\"].apply(lambda x: x.replace(\"DoÃ±a Ana\", \"Dona Ana\"))\n",
    "\n",
    "\n",
    "#pops[\"county_test\"] = pops[\"county_test\"].apply(lambda x: x.replace(\"La Salle Parish\", \"La Salle\"))\n",
    "\n",
    "\n",
    "# rename county_test where state is texas and county is la salle to La Salle (TX)\n",
    "pops.loc[(pops[\"State\"] == \"TX\") & (pops[\"county_test\"] == \"La Salle\"), \"county_test\"] = \"La Salle County\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change La Salle county name in fips to La Salle County\n",
    "fips.loc[fips[\"CountyName\"] == \"La Salle\", \"CountyName\"] = \"La Salle County\"\n",
    "fips.loc[fips[\"CountyName\"] == \"LaSalle Parish\", \"CountyName\"] = \"La Salle Parish\"\n",
    "pops.loc[pops[\"county_test\"] == \"LaSalle Parish\", \"county_test\"] = \"La Salle Parish\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final merge for population dataset & fip number dataset\n",
    "pops_copy = pops.merge(fips[[\"state_abbrev\", \"CountyFIPS\", \"StateFIPS\", \"CountyName\"]], left_on=[\"county_test\", \"State\"], right_on=[\"CountyName\", \"state_abbrev\"], how=\"outer\", indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should never end up with anything left out of merge\n",
    "assert len(pops_copy[pops_copy[\"_merge\"] != \"both\"]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add fip numbers to df_prescriptions\n",
    "\n",
    "# create copies of both dfs so we have a checkpoint to access our old dfs\n",
    "prescriptions_copy = df_prescriptions.copy()\n",
    "fips_copy = fips.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make buyer_county all lowercase\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.lower())\n",
    "\n",
    "# do the same for fips\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove county and parish from fips_copy\n",
    "\n",
    "def remove_parish(x):\n",
    "\n",
    "    if \"parish\" in x:\n",
    "        return x[:-7]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "# prescription dataset has similar format - match fips to this format\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: remove_county(x))\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: remove_parish(x))\n",
    "\n",
    "def expand_saint(x):\n",
    "\n",
    "    if \"st.\" in x:\n",
    "        return x.replace(\"st.\", \"saint\")\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# fix various other inconsistencies\n",
    "# left only values first\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: expand_saint(x))\n",
    "\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"desoto\", \"de soto\"))\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"desoto\", \"de soto\"))\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"st john the baptist\", \"saint john the baptist\"))\n",
    "\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"dekalb\", \"de kalb\"))\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"dekalb\", \"de kalb\"))\n",
    "\n",
    "# fix right only values\n",
    "\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"desoto\", \"de soto\"))\n",
    "\n",
    "\n",
    "\n",
    "# function to remove apostrophes from county names\n",
    "def remove_apostrophe(x):\n",
    "    \n",
    "    if \"'\" in x:\n",
    "        return x.replace(\"'\", \"\")\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "# apply to fips\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: remove_apostrophe(x))\n",
    "\n",
    "# replace lasalle with la salle in fips copy\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"lasalle\", \"la salle\"))\n",
    "\n",
    "# replace dewitt with de witt in prescriptions copy\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"dewitt\", \"de witt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "prescriptions_fips = prescriptions_copy.merge(fips_copy, left_on=[\"BUYER_COUNTY\", \"BUYER_STATE\"], right_on=[\"CountyName\", \"state_abbrev\"], how=\"outer\", indicator=True)\n",
    "\n",
    "# capitalize year and month columns\n",
    "prescriptions_fips.rename(columns={\"year\": \"Year\", \"month\": \"Month\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing missing values\n",
    "\n",
    "Since we have plenty of values joined with right_only indicator status, we know that some counties in our FIPS dataset is not merging correctly to our prescriptions dataset. Let's take one example of missing data - San Juan County in Washington. When exploring the Washington Post's website on prescription data and selecting for this county individually, we can see that the data does, in fact, exist here. However, we see an exceptionally low rate of pills prescribed (32 pills per person per year, in this case). Upon looking at some other examples, we can see that the counties joining with right_only below are likely missing from the Washington Post data due to having such small numbers.\n",
    "\n",
    "Since any given missing county does not have data available, we need to find a way to impute these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLARK           1663\n",
       "YAKIMA          1419\n",
       "BENTON           966\n",
       "COWLITZ          855\n",
       "SPOKANE          647\n",
       "WALLA WALLA      449\n",
       "WHITMAN          276\n",
       "ASOTIN           260\n",
       "KITTITAS         170\n",
       "STEVENS          135\n",
       "FRANKLIN          97\n",
       "GRANT             97\n",
       "COLUMBIA          82\n",
       "PEND OREILLE      63\n",
       "LINCOLN           30\n",
       "ADAMS             23\n",
       "KLICKITAT         23\n",
       "SKAMANIA          22\n",
       "FERRY             20\n",
       "PACIFIC           17\n",
       "CLALLAM            3\n",
       "PIERCE             2\n",
       "DOUGLAS            2\n",
       "WAHKIAKUM          2\n",
       "MASON              1\n",
       "KING               1\n",
       "Name: BUYER_COUNTY, dtype: int64"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at an example of our missing data\n",
    "wa = df_prescriptions[df_prescriptions[\"BUYER_STATE\"] == 'WA']\n",
    "\n",
    "wa[\"BUYER_COUNTY\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert prescriptions_fips[prescriptions_fips[\"_merge\"] == \"left_only\"].shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not all counties joining back to prescription dataset\n",
    "\n",
    "This could be okay, but I want to do a quick check that there are just not records for these counties. To do this, I'll take a small sample of counties in our FIPS dataset that did NOT merge properly to the prescriptions dataset, and search each one manually in the prescription dataset. I will search various different ways the counties could be transcribed, as well as google the county to ensure there are no secondary names for the same county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REPORTER_DEA_NO</th>\n",
       "      <th>BUYER_STATE</th>\n",
       "      <th>BUYER_ZIP</th>\n",
       "      <th>BUYER_COUNTY</th>\n",
       "      <th>TRANSACTION_CODE</th>\n",
       "      <th>DRUG_CODE</th>\n",
       "      <th>DRUG_NAME</th>\n",
       "      <th>QUANTITY</th>\n",
       "      <th>TRANSACTION_DATE</th>\n",
       "      <th>Product_Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>StateFIPS</th>\n",
       "      <th>CountyFIPS_3</th>\n",
       "      <th>CountyName</th>\n",
       "      <th>StateName</th>\n",
       "      <th>CountyFIPS</th>\n",
       "      <th>StateAbbr</th>\n",
       "      <th>STATE_COUNTY</th>\n",
       "      <th>state_abbrev</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147410</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>costilla</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8023</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | COSTILLA</td>\n",
       "      <td>CO</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147411</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>custer</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8027</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | CUSTER</td>\n",
       "      <td>CO</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147412</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>dolores</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8033</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | DOLORES</td>\n",
       "      <td>CO</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147413</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>53</td>\n",
       "      <td>hinsdale</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8053</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | HINSDALE</td>\n",
       "      <td>CO</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147414</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>57</td>\n",
       "      <td>jackson</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>8057</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO | JACKSON</td>\n",
       "      <td>CO</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147750</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53</td>\n",
       "      <td>55</td>\n",
       "      <td>san juan</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53055</td>\n",
       "      <td>WA</td>\n",
       "      <td>WA | SAN JUAN</td>\n",
       "      <td>WA</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147751</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53</td>\n",
       "      <td>57</td>\n",
       "      <td>skagit</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53057</td>\n",
       "      <td>WA</td>\n",
       "      <td>WA | SKAGIT</td>\n",
       "      <td>WA</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147752</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53</td>\n",
       "      <td>61</td>\n",
       "      <td>snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061</td>\n",
       "      <td>WA</td>\n",
       "      <td>WA | SNOHOMISH</td>\n",
       "      <td>WA</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147753</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53</td>\n",
       "      <td>67</td>\n",
       "      <td>thurston</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53067</td>\n",
       "      <td>WA</td>\n",
       "      <td>WA | THURSTON</td>\n",
       "      <td>WA</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147754</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53</td>\n",
       "      <td>73</td>\n",
       "      <td>whatcom</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53073</td>\n",
       "      <td>WA</td>\n",
       "      <td>WA | WHATCOM</td>\n",
       "      <td>WA</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>345 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       REPORTER_DEA_NO BUYER_STATE  BUYER_ZIP BUYER_COUNTY TRANSACTION_CODE  \\\n",
       "147410             NaN         NaN        NaN          NaN              NaN   \n",
       "147411             NaN         NaN        NaN          NaN              NaN   \n",
       "147412             NaN         NaN        NaN          NaN              NaN   \n",
       "147413             NaN         NaN        NaN          NaN              NaN   \n",
       "147414             NaN         NaN        NaN          NaN              NaN   \n",
       "...                ...         ...        ...          ...              ...   \n",
       "147750             NaN         NaN        NaN          NaN              NaN   \n",
       "147751             NaN         NaN        NaN          NaN              NaN   \n",
       "147752             NaN         NaN        NaN          NaN              NaN   \n",
       "147753             NaN         NaN        NaN          NaN              NaN   \n",
       "147754             NaN         NaN        NaN          NaN              NaN   \n",
       "\n",
       "        DRUG_CODE DRUG_NAME  QUANTITY  TRANSACTION_DATE Product_Name  Year  \\\n",
       "147410        NaN       NaN       NaN               NaN          NaN   NaN   \n",
       "147411        NaN       NaN       NaN               NaN          NaN   NaN   \n",
       "147412        NaN       NaN       NaN               NaN          NaN   NaN   \n",
       "147413        NaN       NaN       NaN               NaN          NaN   NaN   \n",
       "147414        NaN       NaN       NaN               NaN          NaN   NaN   \n",
       "...           ...       ...       ...               ...          ...   ...   \n",
       "147750        NaN       NaN       NaN               NaN          NaN   NaN   \n",
       "147751        NaN       NaN       NaN               NaN          NaN   NaN   \n",
       "147752        NaN       NaN       NaN               NaN          NaN   NaN   \n",
       "147753        NaN       NaN       NaN               NaN          NaN   NaN   \n",
       "147754        NaN       NaN       NaN               NaN          NaN   NaN   \n",
       "\n",
       "        Month  StateFIPS  CountyFIPS_3 CountyName   StateName  CountyFIPS  \\\n",
       "147410    NaN          8            23   costilla    Colorado        8023   \n",
       "147411    NaN          8            27     custer    Colorado        8027   \n",
       "147412    NaN          8            33    dolores    Colorado        8033   \n",
       "147413    NaN          8            53   hinsdale    Colorado        8053   \n",
       "147414    NaN          8            57    jackson    Colorado        8057   \n",
       "...       ...        ...           ...        ...         ...         ...   \n",
       "147750    NaN         53            55   san juan  Washington       53055   \n",
       "147751    NaN         53            57     skagit  Washington       53057   \n",
       "147752    NaN         53            61  snohomish  Washington       53061   \n",
       "147753    NaN         53            67   thurston  Washington       53067   \n",
       "147754    NaN         53            73    whatcom  Washington       53073   \n",
       "\n",
       "       StateAbbr    STATE_COUNTY state_abbrev      _merge  \n",
       "147410        CO   CO | COSTILLA           CO  right_only  \n",
       "147411        CO     CO | CUSTER           CO  right_only  \n",
       "147412        CO    CO | DOLORES           CO  right_only  \n",
       "147413        CO   CO | HINSDALE           CO  right_only  \n",
       "147414        CO    CO | JACKSON           CO  right_only  \n",
       "...          ...             ...          ...         ...  \n",
       "147750        WA   WA | SAN JUAN           WA  right_only  \n",
       "147751        WA     WA | SKAGIT           WA  right_only  \n",
       "147752        WA  WA | SNOHOMISH           WA  right_only  \n",
       "147753        WA   WA | THURSTON           WA  right_only  \n",
       "147754        WA    WA | WHATCOM           WA  right_only  \n",
       "\n",
       "[345 rows x 21 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prescriptions_fips[prescriptions_fips[\"_merge\"] != \"both\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need a new solution to the above - should impute the values somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prescriptions_fips = prescriptions_fips[prescriptions_fips[\"_merge\"] != \"right_only\"]\n",
    "\n",
    "# may have to add explanation for this higher up\n",
    "# re add assertion once value imputer is done\n",
    "#assert len(prescriptions_copy) == len(prescriptions_fips)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding fips to our cause of death data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copies of both dfs\n",
    "\n",
    "cause_of_death_copy = df_cause_of_death.copy()\n",
    "fips_copy = fips.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove county once again\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: remove_county(x))\n",
    "\n",
    "\n",
    "# clean some other miscellaneous values up\n",
    "\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"LaSalle Parish\", \"La Salle Parish\"))\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"DeBaca\", \"De Baca\"))\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"La Salle\", \"La Salle County\"))\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"La Salle County Parish\", \"La Salle Parish\"))\n",
    "\n",
    "\n",
    "\n",
    "# expand mckean to mc kean in fips_copy\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"McKean\", \"Mc Kean\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_of_death_fips = cause_of_death_copy.merge(fips_copy, left_on=[\"County\", \"State\"], right_on=[\"CountyName\", \"state_abbrev\"], how=\"outer\", indicator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not all counties joining back to cause of death dataset\n",
    "\n",
    "If the number of people in a given category (eg. one county/year/cause of death category) is less than 10, those records do not appear in this data. There is also a technicality in the number of total deaths vs. drug deaths (which we are interested in).\n",
    "\n",
    "The example we are given is that if a county has 20 deaths unrelated to drugs and alcohol, and only 7 related to alcohol, only the former figure will be reported. In the next notebook (pick_states.ipynb), we will filter by cause of death. In this notebook, since we still have all causes of death, we will impute for every missing value.\n",
    "\n",
    "To impute this data, we will fill in missing values with **a random integer from 0 to 9**. We thought of drawing from a normal distribution, but this implies negative values could be attained. We could take their absolute values to negate this effect, but then we are no longer drawing from a *true* normal distribution, so we chose to pick random values in our range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace null value with a random integer from 0 to 10 with a normal distribution\n",
    "def value_imputer(x):\n",
    "    if pd.isnull(x):\n",
    "        return random.randint(0, 9)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "cause_of_death_fips[\"Deaths\"] = cause_of_death_fips[\"Deaths\"].apply(lambda x: value_imputer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1\n",
       "2    1\n",
       "6    1\n",
       "5    1\n",
       "8    1\n",
       "7    1\n",
       "Name: Deaths, dtype: int64"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick look at our new imputed data\n",
    "cause_of_death_fips[cause_of_death_fips[\"_merge\"] != \"both\"].Deaths.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Population to final DataFrames\n",
    "\n",
    "For pop_fips, cause_of_death_fips, and prescription_fips. Steps needed:\n",
    "\n",
    "- Create unique ID from county FIPS and state FIPS\n",
    "- Merge population dataset based on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_of_death_fips = cause_of_death_fips[cause_of_death_fips[\"_merge\"] == \"both\"]\n",
    "#pops_copy = pops_copy[cause_of_death_fips[\"_merge\"] == \"both\"]\n",
    "prescriptions_fips = prescriptions_fips[prescriptions_fips[\"_merge\"] == \"both\"]\n",
    "\n",
    "\n",
    "# drop merge columns\n",
    "cause_of_death_fips.drop(columns=[\"_merge\"], inplace=True)\n",
    "prescriptions_fips.drop(columns=[\"_merge\"], inplace=True)\n",
    "pops_copy.drop(columns=[\"_merge\",], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique FIP from county and state fips\n",
    "\n",
    "cause_of_death_fips[\"FIP_unique\"] = cause_of_death_fips[\"CountyFIPS\"].apply(lambda x: str(x)) + cause_of_death_fips[\"StateFIPS\"].apply(lambda x: str(x))\n",
    "prescriptions_fips[\"FIP_unique\"] = prescriptions_fips[\"CountyFIPS\"].apply(lambda x: str(x)) + prescriptions_fips[\"StateFIPS\"].apply(lambda x: str(x))\n",
    "pops_copy[\"FIP_unique\"] = pops_copy[\"CountyFIPS\"].apply(lambda x: str(x)) + pops_copy[\"StateFIPS\"].apply(lambda x: str(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add some sort of assert here. not sure what it should be yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final prescriptions dataset with populations\n",
    "# can safely left join here, because we only need records in the prescriptions dataset\n",
    "prescriptions = prescriptions_fips.merge(pops_copy, on=[\"FIP_unique\", \"Year\"], how=\"left\", indicator=True)\n",
    "\n",
    "assert (prescriptions[\"_merge\"] == \"both\").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one more assert to check length\n",
    "assert len(prescriptions) == len(prescriptions_fips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some useless columns\n",
    "prescriptions.drop(columns=[\"_merge\", \"CountyName_y\", \"StateFIPS_y\", \"CountyFIPS_y\",\"state_abbrev_y\", \"County\", \"CountyFIPS_3\"], inplace=True)\n",
    "\n",
    "# rename x columns\n",
    "prescriptions.rename(columns={\"CountyName_x\": \"CountyName\", \"StateFIPS_x\": \"StateFIPS\", \"CountyFIPS_x\": \"CountyFIPS\", \"state_abbrev_x\": \"state_abbrev\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final cause of death dataset with populations\n",
    "# can safely left join here, because we only need records in the cause of death dataset\n",
    "cause_of_death = cause_of_death_fips.merge(pops_copy, on=[\"FIP_unique\", \"Year\"], how=\"left\", indicator=True)\n",
    "\n",
    "assert cause_of_death_fips.Deaths.isnull().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some useless columns\n",
    "cause_of_death.drop(columns=[\"_merge\", \"CountyName_y\", \"StateFIPS_y\", \"CountyFIPS_y\",\"state_abbrev_y\", \"County_y\", \"CountyFIPS_3\", \"State_y\"], inplace=True)\n",
    "\n",
    "# rename x columns\n",
    "cause_of_death.rename(columns={\"County_x\": \"County\", \"Year_x\": \"Year\", \"State_x\": \"State\", \"StateFIPS_x\": \"StateFIPS\", \"CountyFIPS_x\": \"CountyFIPS\", \"state_abbrev_x\": \"state_abbrev\", \"CountyName_x\": \"CountyName\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asserts to make sure we didn't lose any records from our original datasets\n",
    "\n",
    "assert len(df_cause_of_death) == len(cause_of_death)\n",
    "assert len(df_prescriptions) == len(prescriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export main, unjoined datasets in case we need them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_of_death.to_csv(\"05_cleaned_data/cause_of_death_clean.csv\", index=False)\n",
    "prescriptions.to_csv(\"05_cleaned_data/arcos_all_washpost_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final 3 datasets\n",
    "\n",
    "We should have: (UNSURE IF WE SHOULD EXTEND DATE RANGES, CURRENTLY 3 YEARS BEFORE AND AFTER POLICY IMPLEMENTATION)\n",
    "\n",
    "- Florida and Georgia 2007 - 2013\n",
    "- Texas and Oklahoma 2004 - 2010\n",
    "- Washington and Oregon 2009 - 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug overdose - broken down by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Florida and Georgia\n",
    "\n",
    "prescriptions_fl = prescriptions.copy()\n",
    "prescriptions_tx = prescriptions.copy()\n",
    "prescriptions_wa = prescriptions.copy()\n",
    "\n",
    "prescriptions_fl = prescriptions_fl[(prescriptions_fl[\"BUYER_STATE\"] == \"FL\") | (prescriptions_fl[\"BUYER_STATE\"].isin(fl_states))]\n",
    "prescriptions_tx = prescriptions_tx[(prescriptions_tx[\"BUYER_STATE\"] == \"TX\") | (prescriptions_tx[\"BUYER_STATE\"]).isin(tx_states)]\n",
    "prescriptions_wa = prescriptions_wa[(prescriptions_wa[\"BUYER_STATE\"] == \"WA\") | (prescriptions_wa[\"BUYER_STATE\"]).isin(wa_states)]\n",
    "\n",
    "\n",
    "\n",
    "# filter appropriate years\n",
    "fl_start = 2007\n",
    "fl_end = 2013\n",
    "\n",
    "tx_start = 2004\n",
    "tx_end = 2010\n",
    "\n",
    "wa_start = 2009\n",
    "wa_end = 2015\n",
    "\n",
    "\n",
    "prescriptions_fl = prescriptions_fl[(prescriptions_fl[\"Year\"] >= fl_start) & (prescriptions_fl[\"Year\"] <= fl_end)]\n",
    "prescriptions_tx = prescriptions_tx[(prescriptions_tx[\"Year\"] >= tx_start) & (prescriptions_tx[\"Year\"] <= tx_end)]\n",
    "prescriptions_wa = prescriptions_wa[(prescriptions_wa[\"Year\"] >= wa_start) & (prescriptions_wa[\"Year\"] <= wa_end)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cause of death - broken down by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_fl = cause_of_death.copy()\n",
    "deaths_tx = cause_of_death.copy()\n",
    "deaths_wa = cause_of_death.copy()\n",
    "\n",
    "deaths_fl = deaths_fl[(deaths_fl[\"StateName\"] == \"Florida\") | (deaths_fl[\"State\"].isin(fl_states))]\n",
    "deaths_tx = deaths_tx[(deaths_tx[\"StateName\"] == \"Texas\") | (deaths_tx[\"State\"].isin(tx_states))]\n",
    "deaths_wa = deaths_wa[(deaths_wa[\"StateName\"] == \"Washington\") | (deaths_wa[\"State\"].isin(wa_states))]\n",
    "\n",
    "deaths_fl = deaths_fl[(deaths_fl[\"Year\"] >= fl_start) & (deaths_fl[\"Year\"] <= fl_end)]\n",
    "deaths_tx = deaths_tx[(deaths_tx[\"Year\"] >= tx_start) & (deaths_tx[\"Year\"] <= tx_end)]  \n",
    "deaths_wa = deaths_wa[(deaths_wa[\"Year\"] >= wa_start) & (deaths_wa[\"Year\"] <= wa_end)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export all to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "prescriptions_fl.to_csv(\"05_cleaned_data/prescriptions_fl.csv\", index=False)\n",
    "prescriptions_tx.to_csv(\"05_cleaned_data/prescriptions_tx.csv\", index=False)\n",
    "prescriptions_wa.to_csv(\"05_cleaned_data/prescriptions_wa.csv\", index=False)\n",
    "\n",
    "deaths_fl.to_csv(\"05_cleaned_data/deaths_fl.csv\", index=False)\n",
    "deaths_tx.to_csv(\"05_cleaned_data/deaths_tx.csv\", index=False)\n",
    "deaths_wa.to_csv(\"05_cleaned_data/deaths_wa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for the group\n",
    "\n",
    "- may need to filter out a couple more columns - haven't done this yet as I don't want to accidentally delete something we need\n",
    "- overdose data is only broken down by year unless i messed something up - overdose analysis will have to be less granular"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49aa7209ac0e1a36a89cb04290394fd089cb5ce56cb44c9d4652c0180c6152a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
